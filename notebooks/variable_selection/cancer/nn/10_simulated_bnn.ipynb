{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0C0j-04g7FWS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\r\n",
      "Collecting jaxlib[cuda112]==0.3.15\r\n",
      "  Downloading https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.15%2Bcuda11.cudnn82-cp39-none-manylinux2014_x86_64.whl (162.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m162.7/162.7 MB\u001B[0m \u001B[31m12.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25h\u001B[33mWARNING: jaxlib 0.3.15+cuda11.cudnn82 does not provide the extra 'cuda112'\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from jaxlib[cuda112]==0.3.15) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.9/dist-packages (from jaxlib[cuda112]==0.3.15) (1.23.1)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jaxlib[cuda112]==0.3.15) (1.8.1)\r\n",
      "Installing collected packages: jaxlib\r\n",
      "  Attempting uninstall: jaxlib\r\n",
      "    Found existing installation: jaxlib 0.3.8+cuda11.cudnn82\r\n",
      "    Uninstalling jaxlib-0.3.8+cuda11.cudnn82:\r\n",
      "      Successfully uninstalled jaxlib-0.3.8+cuda11.cudnn82\r\n",
      "Successfully installed jaxlib-0.3.15+cuda11.cudnn82\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mLooking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\r\n",
      "Collecting jax[cuda112]==0.3.17\r\n",
      "  Downloading jax-0.3.17.tar.gz (1.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m66.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[33mWARNING: jax 0.3.17 does not provide the extra 'cuda112'\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (1.23.1)\r\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (3.3.0)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (1.8.1)\r\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (4.3.0)\r\n",
      "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (0.6.0)\r\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax[cuda112]==0.3.17) (3.8.1)\r\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax[cuda112]==0.3.17) (5.8.0)\r\n",
      "Building wheels for collected packages: jax\r\n",
      "  Building wheel for jax (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for jax: filename=jax-0.3.17-py3-none-any.whl size=1217849 sha256=91e186b43739d9ed07c6336d0041933c19eca32ac1dcca375b2cd4de5611ad2f\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/36/cd/88/2d90379f7549c27d5654e893f74210f30f0c645c23a71e6f56\r\n",
      "Successfully built jax\r\n",
      "Installing collected packages: jax\r\n",
      "  Attempting uninstall: jax\r\n",
      "    Found existing installation: jax 0.3.14\r\n",
      "    Uninstalling jax-0.3.14:\r\n",
      "      Successfully uninstalled jax-0.3.14\r\n",
      "Successfully installed jax-0.3.17\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting optax\r\n",
      "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m154.9/154.9 kB\u001B[0m \u001B[31m24.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.9/dist-packages (from optax) (0.3.17)\r\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.9/dist-packages (from optax) (4.3.0)\r\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.9/dist-packages (from optax) (0.3.15+cuda11.cudnn82)\r\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from optax) (1.1.0)\r\n",
      "Collecting chex>=0.1.5\r\n",
      "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m85.3/85.3 kB\u001B[0m \u001B[31m22.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from optax) (1.23.1)\r\n",
      "Collecting toolz>=0.9.0\r\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m55.8/55.8 kB\u001B[0m \u001B[31m15.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting dm-tree>=0.1.5\r\n",
      "  Downloading dm_tree-0.1.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m153.0/153.0 kB\u001B[0m \u001B[31m38.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: etils[epath] in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (0.6.0)\r\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (3.3.0)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (1.8.1)\r\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax>=0.1.55->optax) (3.8.1)\r\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax>=0.1.55->optax) (5.8.0)\r\n",
      "Installing collected packages: dm-tree, toolz, chex, optax\r\n",
      "Successfully installed chex-0.1.5 dm-tree-0.1.8 optax-0.1.4 toolz-0.12.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting optuna\r\n",
      "  Downloading optuna-3.0.5-py3-none-any.whl (348 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m348.5/348.5 kB\u001B[0m \u001B[31m14.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting cmaes>=0.8.2\r\n",
      "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\r\n",
      "Collecting colorlog\r\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from optuna) (4.64.0)\r\n",
      "Collecting cliff\r\n",
      "  Downloading cliff-4.1.0-py3-none-any.whl (81 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m81.0/81.0 kB\u001B[0m \u001B[31m25.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (1.8.1)\r\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (1.4.39)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (21.3)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from optuna) (1.23.1)\r\n",
      "Requirement already satisfied: importlib-metadata<5.0.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (4.12.0)\r\n",
      "Collecting alembic>=1.5.0\r\n",
      "  Downloading alembic-1.9.1-py3-none-any.whl (210 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m210.4/210.4 kB\u001B[0m \u001B[31m48.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from optuna) (5.4.1)\r\n",
      "Collecting Mako\r\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.7/78.7 kB\u001B[0m \u001B[31m25.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata<5.0.0->optuna) (3.8.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->optuna) (3.0.9)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy>=1.3.0->optuna) (1.1.2)\r\n",
      "Collecting PrettyTable>=0.7.2\r\n",
      "  Downloading prettytable-3.6.0-py3-none-any.whl (27 kB)\r\n",
      "Collecting cmd2>=1.0.0\r\n",
      "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m147.1/147.1 kB\u001B[0m \u001B[31m38.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting autopage>=0.4.0\r\n",
      "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\r\n",
      "Collecting stevedore>=2.0.1\r\n",
      "  Downloading stevedore-4.1.1-py3-none-any.whl (50 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m50.0/50.0 kB\u001B[0m \u001B[31m15.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting pyperclip>=1.6\r\n",
      "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\r\n",
      "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.9/dist-packages (from cmd2>=1.0.0->cliff->optuna) (18.2.0)\r\n",
      "Collecting pbr!=2.1.0,>=2.0.0\r\n",
      "  Downloading pbr-5.11.0-py2.py3-none-any.whl (112 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m112.6/112.6 kB\u001B[0m \u001B[31m31.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\r\n",
      "Building wheels for collected packages: pyperclip\r\n",
      "  Building wheel for pyperclip (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11125 sha256=379166a317546ec45abcf36d961bfe9205a6358ad93fdb61218c64bbe3d26b08\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0c/09/9e/49e21a6840ef7955b06d47394afef0058f0378c0914e48b8b8\r\n",
      "Successfully built pyperclip\r\n",
      "Installing collected packages: pyperclip, PrettyTable, pbr, Mako, colorlog, cmd2, cmaes, autopage, stevedore, alembic, cliff, optuna\r\n",
      "Successfully installed Mako-1.2.4 PrettyTable-3.6.0 alembic-1.9.1 autopage-0.5.1 cliff-4.1.0 cmaes-0.9.1 cmd2-2.4.2 colorlog-6.7.0 optuna-3.0.5 pbr-5.11.0 pyperclip-1.8.2 stevedore-4.1.1\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting dm-haiku\r\n",
      "  Downloading dm_haiku-0.0.9-py3-none-any.whl (352 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m352.1/352.1 kB\u001B[0m \u001B[31m31.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (1.23.1)\r\n",
      "Collecting tabulate>=0.8.9\r\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\r\n",
      "Collecting jmp>=0.0.2\r\n",
      "  Downloading jmp-0.0.2-py3-none-any.whl (16 kB)\r\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (1.1.0)\r\n",
      "Installing collected packages: tabulate, jmp, dm-haiku\r\n",
      "Successfully installed dm-haiku-0.0.9 jmp-0.0.2 tabulate-0.9.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting tensorflow-probability==0.17\r\n",
      "  Downloading tensorflow_probability-0.17.0-py2.py3-none-any.whl (6.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.5/6.5 MB\u001B[0m \u001B[31m69.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m0:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (0.1.8)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (1.23.1)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (1.1.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (2.1.0)\r\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow-probability==0.17) (1.14.0)\r\n",
      "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (0.4.0)\r\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (5.1.1)\r\n",
      "Installing collected packages: tensorflow-probability\r\n",
      "Successfully installed tensorflow-probability-0.17.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U jaxlib[cuda112]==0.3.15 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install -U jax[cuda112]==0.3.17 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install optax\n",
    "!pip install optuna\n",
    "!pip install dm-haiku\n",
    "!pip install tensorflow-probability==0.17"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"False\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "SERVER = 1\n",
    "import jax\n",
    "import jax.random as random\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "tfd = tfp.distributions\n",
    "from tqdm import tqdm\n",
    "from nn_util import *\n",
    "from optim_util import *"
   ],
   "metadata": {
    "id": "PjXBcCX47Q3Y",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "if SERVER:\n",
    "    data_dir = \".\"\n",
    "else:\n",
    "    data_dir = \"/home/xabush/code/snet/moses-incons-pen-xp/data\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "N = 600\n",
    "P = 500\n",
    "\n",
    "def func(key, x):\n",
    "    _, subkey = random.split(key, 2)\n",
    "    noise = random.normal(subkey, shape=(N,))\n",
    "    t1 = jnp.max(x[:,[0, 1]], axis=1)\n",
    "    t2 = jnp.max(x[:,[2, 3, 4]], axis=1)\n",
    "    y = (((10*jnp.sin(t1) + (t2**3)) / (1 + (x[:,0] + x[:,4])**2)) +\n",
    "         jnp.sin(0.5*x[:,2])*(1 + jnp.exp(x[:,3] - 0.5*x[:,2])) + x[:,2]**2 + 2*jnp.sin(x[:,3]) + 2*x[:,4]) + noise\n",
    "\n",
    "    return y\n",
    "\n",
    "def generate_dataset_v1(seed):\n",
    "    rng_key = random.PRNGKey(seed)\n",
    "    key1, key2 = random.split(rng_key, 2)\n",
    "    e = random.normal(key1, shape=(N,))\n",
    "    z = random.normal(key2, shape=(N, P))\n",
    "    dataset = jax.vmap(lambda a, b: 0.5*(a + b), in_axes=(None, 1), out_axes=1)(e, z)\n",
    "    # dataset = 0.5*(e + z)\n",
    "    output = func(key1, dataset)\n",
    "\n",
    "    return dataset, output"
   ],
   "metadata": {
    "id": "uLlTs_w97hxI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f845cb24-4e50-45c8-c88d-d8ebc29438f3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title\n",
    "class BayesNN():\n",
    "    def __init__(self, sgd_optim, sgld_optim, temperature, sigma, data_size, hidden_sizes, act_fn=jax.nn.relu):\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.act_fn = act_fn\n",
    "        self.sgd_optim = sgd_optim\n",
    "        self.sgld_optim = sgld_optim\n",
    "        self.optimiser = sgd_optim\n",
    "        self._forward = hk.without_apply_rng(hk.transform(self._forward_fn))\n",
    "        self.loss = jax.jit(self.loss)\n",
    "        self.update = jax.jit(self.update)\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.sigma = sigma\n",
    "        self.data_size = data_size\n",
    "        self.add_noise = False\n",
    "\n",
    "        # weight_decay = self.sigma*self.temperature\n",
    "        # self.weight_prior = tfd.Normal(0, self.sigma)\n",
    "        self.weight_prior = tfd.StudentT(df=2, loc=0, scale=self.sigma)\n",
    "        # self.weight_prior = tfd.Laplace(0, self.sigma)\n",
    "\n",
    "    def init(self, rng, x):\n",
    "        params = self._forward.init(rng, x)\n",
    "        opt_state = self.optimiser.init(params)\n",
    "        return params, opt_state\n",
    "\n",
    "    def apply(self, params, x):\n",
    "        return self._forward.apply(params, x).ravel()\n",
    "\n",
    "\n",
    "    def update(self, key, params, opt_state, x, y):\n",
    "        if self.add_noise:\n",
    "            self.optimiser = self.sgld_optim\n",
    "        else:\n",
    "            self.optimiser = self.sgd_optim\n",
    "        grads = jax.grad(self.loss)(params, x, y)\n",
    "        updates, opt_state = self.optimiser.update(key, grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state\n",
    "\n",
    "    def _forward_fn(self, x):\n",
    "        init_fn = hk.initializers.VarianceScaling()\n",
    "        for hd in self.hidden_sizes:\n",
    "            x = hk.Linear(hd, w_init=init_fn, b_init=init_fn)(x)\n",
    "            x = self.act_fn(x)\n",
    "\n",
    "        x = hk.Linear(1)(x)\n",
    "        return x\n",
    "\n",
    "    def log_prior(self, params):\n",
    "        \"\"\"Computes the Gaussian prior log-density.\"\"\"\n",
    "        logprob_tree = jax.tree_util.tree_leaves(jax.tree_util.tree_map(lambda x: jnp.sum(self.weight_prior.log_prob(x.reshape(-1))/self.temperature),\n",
    "                                                                        params))\n",
    "\n",
    "        return sum(logprob_tree)\n",
    "\n",
    "    def log_likelihood(self, params, x, y):\n",
    "        preds = self.apply(params, x).ravel()\n",
    "        log_prob = jnp.sum(tfd.Normal(preds, self.temperature).log_prob(y))\n",
    "        batch_size = x.shape[0]\n",
    "        log_prob = (self.data_size / batch_size)*log_prob\n",
    "        return log_prob\n",
    "\n",
    "    def loss(self, params, x, y):\n",
    "        logprob_prior = self.log_prior(params)\n",
    "        logprob_likelihood = self.log_likelihood(params, x, y)\n",
    "        return logprob_likelihood + logprob_prior"
   ],
   "metadata": {
    "cellView": "form",
    "id": "7W_4blor9txY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title\n",
    "from sklearn.metrics import r2_score\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def init_bnn_model(seed, train_loader, epochs, lr_0, num_cycles, temp, sigma, hidden_sizes, act_fn):\n",
    "    torch.manual_seed(seed)\n",
    "    num_batches = len(train_loader)\n",
    "    data_size = X.shape[0]\n",
    "    total_steps = num_batches*epochs\n",
    "    step_size_fn = make_cyclical_lr_fn(lr_0, total_steps, num_cycles)\n",
    "    sgd_optim = sgd_gradient_update(step_size_fn, momentum_decay=0, preconditioner=get_rmsprop_preconditioner())\n",
    "    sgld_optim = sgld_gradient_update(step_size_fn, momentum_decay=0, preconditioner=get_rmsprop_preconditioner())\n",
    "\n",
    "    model = BayesNN(sgd_optim, sgld_optim,\n",
    "                      temp, sigma, data_size, hidden_sizes, act_fn)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_bnn_model(seed, train_loader, epochs, num_cycles, beta, lr_0,\n",
    "                       hidden_sizes, temp, sigma, act_fn=jax.nn.relu):\n",
    "\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    model = init_bnn_model(seed, train_loader, epochs, lr_0, num_cycles, temp, sigma, hidden_sizes, act_fn)\n",
    "\n",
    "    cycle_len = epochs // num_cycles\n",
    "    num_batches = len(train_loader)\n",
    "    M = (epochs*num_batches) // num_cycles\n",
    "    init_params, init_opt_state = model.init(rng_key, next(iter(train_loader))[0])\n",
    "\n",
    "\n",
    "    states = []\n",
    "    params, opt_state = init_params, init_opt_state\n",
    "    step = 0\n",
    "    key = rng_key\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            _, key = jax.random.split(key, 2)\n",
    "            rk = (step % M) / M\n",
    "            params, opt_state = model.update(key, params, opt_state, batch_x, batch_y)\n",
    "            if rk > beta:\n",
    "                model.add_noise = True\n",
    "                states.append(params)\n",
    "            else:\n",
    "                model.add_noise = False\n",
    "            step += 1\n",
    "\n",
    "    return model, states\n",
    "\n",
    "def eval_bnn_model(model, X, y, params):\n",
    "\n",
    "    if isinstance(params, list):\n",
    "        y_preds = np.zeros((len(params), len(y)))\n",
    "        for i, param in enumerate(params):\n",
    "            preds = model.apply(param, X).ravel()\n",
    "            y_preds[i] = preds\n",
    "\n",
    "        y_preds = np.mean(y_preds, axis=0)\n",
    "        rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "    else:\n",
    "        y_preds = model.apply(params, X).ravel()\n",
    "        rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "\n",
    "    return rmse\n",
    "\n",
    "def score_bnn_model(model, X, y, params):\n",
    "    if isinstance(params, list):\n",
    "        y_preds = np.zeros((len(params), len(y)))\n",
    "        for i, param in enumerate(params):\n",
    "            preds = model.apply(param, X).ravel()\n",
    "            # preds_mean = preds[::2]\n",
    "            y_preds[i] = preds\n",
    "\n",
    "        y_preds = np.mean(y_preds, axis=0)\n",
    "        rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "        if np.isfinite(y_preds).all():\n",
    "            r2 = r2_score(y, y_preds)\n",
    "        else:\n",
    "            r2 = np.nan\n",
    "    else:\n",
    "        y_preds = model.apply(params, X)\n",
    "        preds_mean = y_preds[::2]\n",
    "        rmse = jnp.sqrt(jnp.mean((y - preds_mean)**2))\n",
    "        if np.isfinite(y_preds).all():\n",
    "            r2 = r2_score(y, y_preds)\n",
    "        else:\n",
    "            r2 = np.nan\n",
    "\n",
    "    return rmse, r2\n",
    "\n",
    "def eval_per_model_score(model, X, y, params):\n",
    "    scores = []\n",
    "\n",
    "    for param in params:\n",
    "        preds = model.apply(param, X).ravel()\n",
    "        # preds_mean = preds[::2]\n",
    "        rmse = jnp.sqrt(jnp.mean((y - preds)**2))\n",
    "        scores.append(rmse)\n",
    "\n",
    "\n",
    "\n",
    "    return np.array(scores)"
   ],
   "metadata": {
    "cellView": "form",
    "id": "tqM0tdtx-NBP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "dataset, output = generate_dataset_v1(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "X_df, y_df = pd.DataFrame(dataset[0]), pd.Series(output[0])\n",
    "X_train_outer_df, X_test_df, y_train_outer_df, y_test_df = train_test_split(X_df, y_df, shuffle=False, test_size=0.2)\n",
    "X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X_train_outer_df, y_train_outer_df, test_size=0.2, shuffle=False)"
   ],
   "metadata": {
    "id": "81A2SzwK-Sbo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [500, 1]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      3\u001B[0m X_df, y_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(dataset[\u001B[38;5;241m0\u001B[39m]), pd\u001B[38;5;241m.\u001B[39mSeries(output[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m----> 4\u001B[0m X_train_outer_df, X_test_df, y_train_outer_df, y_test_df \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m X_train_df, X_val_df, y_train_df, y_val_df \u001B[38;5;241m=\u001B[39m train_test_split(X_train_outer_df, y_train_outer_df, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py:2430\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[1;32m   2427\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_arrays \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   2428\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAt least one array required as input\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 2430\u001B[0m arrays \u001B[38;5;241m=\u001B[39m \u001B[43mindexable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43marrays\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2432\u001B[0m n_samples \u001B[38;5;241m=\u001B[39m _num_samples(arrays[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m   2433\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m _validate_shuffle_split(\n\u001B[1;32m   2434\u001B[0m     n_samples, test_size, train_size, default_test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.25\u001B[39m\n\u001B[1;32m   2435\u001B[0m )\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:433\u001B[0m, in \u001B[0;36mindexable\u001B[0;34m(*iterables)\u001B[0m\n\u001B[1;32m    414\u001B[0m \u001B[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001B[39;00m\n\u001B[1;32m    415\u001B[0m \n\u001B[1;32m    416\u001B[0m \u001B[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    429\u001B[0m \u001B[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001B[39;00m\n\u001B[1;32m    430\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    432\u001B[0m result \u001B[38;5;241m=\u001B[39m [_make_indexable(X) \u001B[38;5;28;01mfor\u001B[39;00m X \u001B[38;5;129;01min\u001B[39;00m iterables]\n\u001B[0;32m--> 433\u001B[0m \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:387\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[0;34m(*arrays)\u001B[0m\n\u001B[1;32m    385\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 387\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    388\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    389\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[1;32m    390\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [500, 1]"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_indices, val_indices = X_train_df.index.to_list() ,X_val_df.index.to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = X_train_df.to_numpy(), X_val_df.to_numpy(), X_test_df.to_numpy()\n",
    "y_train, y_val, y_test = y_train_df.to_numpy(), y_val_df.to_numpy(), y_test_df.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "J = np.zeros((P, P))\n",
    "J.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YopdN3ToA2MI",
    "outputId": "75a812f5-96ae-4e82-e4b3-9f1b1a22134c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title\n",
    "class BgBayesNN():\n",
    "    def __init__(self, sgd_optim, sgld_optim, disc_sgd_optim, disc_sgld_optim, \n",
    "                        temperature, sigma, data_size, hidden_sizes, \n",
    "                        J, eta, mu,\n",
    "                        act_fn=jax.nn.relu):\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.act_fn = act_fn\n",
    "        self.sgd_optim = sgd_optim\n",
    "        self.sgld_optim = sgld_optim\n",
    "        self.optimiser = sgd_optim\n",
    "\n",
    "        self.disc_optimiser = disc_sgd_optim\n",
    "        self.disc_sgd_optim = disc_sgd_optim\n",
    "        self.disc_sgld_optim = disc_sgld_optim\n",
    "\n",
    "        self._forward = hk.transform(self._forward_fn)\n",
    "        self.loss = jax.jit(self.loss)\n",
    "        self.update = jax.jit(self.update)\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.sigma = sigma\n",
    "        self.data_size = data_size\n",
    "        self.add_noise = False\n",
    "        self.J = J\n",
    "        self.eta = eta\n",
    "        self.mu = mu\n",
    "\n",
    "        # weight_decay = self.sigma*self.temperature\n",
    "        # self.weight_prior = tfd.Normal(0, self.sigma)\n",
    "        self.weight_prior = tfd.StudentT(df=2, loc=0, scale=self.sigma)\n",
    "        # self.weight_prior = tfd.Horseshoe(scale=self.sigma)\n",
    "        # self.weight_prior = tfd.Laplace(0, self.sigma)\n",
    "\n",
    "    def init(self, rng, x):\n",
    "        gamma = tfd.Bernoulli(0.5*jnp.ones(x.shape[-1])).sample(seed=rng)*1.\n",
    "        params = self._forward.init(rng, x, gamma)\n",
    "        opt_state = self.optimiser.init(params)\n",
    "        disc_opt_state = self.disc_optimiser.init(gamma)\n",
    "        return params, gamma, opt_state, disc_opt_state\n",
    "\n",
    "    def apply(self, params, gamma, x):\n",
    "        return self._forward.apply(params, None, x, gamma).ravel()\n",
    "\n",
    "    \n",
    "    def loss(self, params, gamma, x, y):\n",
    "        logprob_prior = self.log_prior(params)\n",
    "        logprob_likelihood = self.log_likelihood(params, gamma, x, y)\n",
    "        return logprob_likelihood + logprob_prior\n",
    "\n",
    "    def update(self, key, params, gamma, opt_state, disc_opt_state, x, y):\n",
    "        if self.add_noise:\n",
    "            self.optimiser = self.sgld_optim\n",
    "            self.disc_optimiser = self.disc_sgld_optim\n",
    "        else:\n",
    "            self.optimiser = self.sgd_optim\n",
    "            self.disc_optimiser = self.disc_sgd_optim\n",
    "\n",
    "        contin_loss = lambda p: self.log_prior(params) + self.log_likelihood(p, gamma, x, y)\n",
    "\n",
    "        grads = jax.grad(contin_loss)(params)\n",
    "        updates, opt_state = self.optimiser.update(key, grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "\n",
    "        disc_loss = lambda g: self.ising_prior(g) + self.log_likelihood(params, g, x, y)\n",
    "        disc_logprob, disc_grads = jax.value_and_grad(disc_loss)(gamma)\n",
    "        gamma, disc_opt_state = self.disc_optimiser.update(key, gamma, disc_grads, disc_opt_state)\n",
    "        return params, gamma, opt_state, disc_opt_state\n",
    "\n",
    "    def _forward_fn(self, x, gamma):\n",
    "        x = x @ jnp.diag(gamma)\n",
    "        init_fn = hk.initializers.VarianceScaling()\n",
    "        for hd in self.hidden_sizes:\n",
    "            x = hk.Linear(hd, w_init=init_fn)(x)\n",
    "            x = self.act_fn(x)\n",
    "\n",
    "        x = hk.Linear(1)(x)\n",
    "        return x\n",
    "\n",
    "    def log_prior(self, params):\n",
    "        \"\"\"Computes the Gaussian prior log-density.\"\"\"\n",
    "        logprob_tree = jax.tree_util.tree_leaves(jax.tree_util.tree_map(lambda x: jnp.sum(self.weight_prior.log_prob(x.reshape(-1))/self.temperature), \n",
    "                                                                            params))\n",
    "        \n",
    "        return sum(logprob_tree)\n",
    "\n",
    "    def log_likelihood(self, params, gamma, x, y):\n",
    "        preds = self.apply(params, gamma, x).ravel()\n",
    "        # preds_mean, preds_std = preds[::2], preds[1::2]\n",
    "        # print(preds.shape)\n",
    "        # print(preds_mean.shape)\n",
    "        # print(preds_std.shape)\n",
    "        # preds_std = jax.nn.softplus(preds_std.squeeze())\n",
    "        # preds_mean = preds_mean.squeeze()\n",
    "        # preds_std = (preds_std**2)*self.temperature\n",
    "        log_prob = jnp.sum(tfd.Normal(preds, self.temperature).log_prob(y))\n",
    "        # log_prob = jnp.sum(tfd.MultivariateNormalDiag(preds_mean, preds_std).log_prob(y))\n",
    "        batch_size = x.shape[0]\n",
    "        log_prob = (self.data_size / batch_size)*log_prob\n",
    "        return log_prob\n",
    "\n",
    "    def ising_prior(self, gamma):\n",
    "        \"\"\"Log probability of the Ising model - prior over the discrete variables\"\"\"\n",
    "        return  -self.mu*jnp.sum(gamma) / self.temperature\n",
    "        # x = (2 * gamma) - 1\n",
    "        # xg = x @ self.J\n",
    "        # xgx = (xg * x).sum(-1)\n",
    "        # return (0.5*self.eta*xgx + self.mu*jnp.sum(x)) / self.temperature"
   ],
   "metadata": {
    "cellView": "form",
    "id": "m86q3ICWBBna",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title\n",
    "def init_bg_bnn_model(seed, train_loader, epochs, lr_0, disc_lr_0, num_cycles, temp, sigma, hidden_sizes, J, eta, mu, act_fn):\n",
    "    torch.manual_seed(seed)\n",
    "    num_batches = len(train_loader)\n",
    "    data_size = X.shape[0]\n",
    "    total_steps = num_batches*epochs\n",
    "    step_size_fn = make_cyclical_lr_fn(lr_0, total_steps, num_cycles)\n",
    "    # disc_step_size_fn = make_cyclical_lr_fn(disc_lr_0, total_steps, num_cycles)\n",
    "    disc_step_size_fn = lambda count: disc_lr_0\n",
    "    sgd_optim = sgd_gradient_update(step_size_fn, momentum_decay=0, preconditioner=get_rmsprop_preconditioner())\n",
    "    sgld_optim = sgld_gradient_update(step_size_fn, momentum_decay=0, preconditioner=get_rmsprop_preconditioner())\n",
    "    disc_sgd_optim = disc_sgld_gradient_update(disc_step_size_fn, momentum_decay=0, preconditioner=get_identity_preconditioner())\n",
    "    disc_sgld_optim = disc_sgld_gradient_update(disc_step_size_fn, momentum_decay=0, preconditioner=get_identity_preconditioner())\n",
    "\n",
    "    model = BgBayesNN(sgd_optim, sgld_optim, disc_sgd_optim, disc_sgld_optim,\n",
    "                      temp, sigma, data_size, hidden_sizes,\n",
    "                      J, eta, mu, act_fn)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_bg_bnn_model(seed, train_loader, epochs, num_cycles, beta, lr_0, disc_lr_0,\n",
    "                    hidden_sizes, temp, sigma, eta, mu, J, act_fn=jax.nn.relu):\n",
    "\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    model = init_bg_bnn_model(seed, train_loader, epochs, lr_0, disc_lr_0, num_cycles, temp, sigma, hidden_sizes, J, eta, mu, act_fn)\n",
    "\n",
    "    cycle_len = epochs // num_cycles\n",
    "    n_models_per_cycle = 3\n",
    "    num_batches = len(train_loader)\n",
    "    M = (epochs*num_batches) // num_cycles\n",
    "    init_params, init_gamma, init_opt_state, init_disc_opt_state = model.init(rng_key, next(iter(train_loader))[0])\n",
    "\n",
    "\n",
    "    states = []\n",
    "    disc_states = []\n",
    "    val_losses = []\n",
    "    params, gamma, opt_state, disc_opt_state = init_params, init_gamma, init_opt_state, init_disc_opt_state\n",
    "    step = 0\n",
    "    key = rng_key\n",
    "    for epoch in range(epochs):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            _, key = jax.random.split(key, 2)\n",
    "            rk = (step % M) / M\n",
    "            params, gamma, opt_state, disc_opt_state = model.update(key, params, gamma, opt_state, disc_opt_state, batch_x, batch_y)\n",
    "            if rk > beta:\n",
    "                states.append(params)\n",
    "                disc_states.append(gamma)\n",
    "                model.add_noise = True\n",
    "            else:\n",
    "                model.add_noise = False\n",
    "            step += 1\n",
    "        # if (epoch % cycle_len) + 1 > (cycle_len - n_models_per_cycle):\n",
    "        #     # print(epoch)\n",
    "        #     states.append(params)\n",
    "        #     disc_states.append(gamma)\n",
    "        # val_loss = eval_bg_bnn_model(model, X_val, y_val, params, gamma)\n",
    "        # val_losses.append(val_loss)\n",
    "\n",
    "    return model, states, disc_states\n",
    "\n",
    "def eval_bg_bnn_model(model, X, y, params, gammas):\n",
    "\n",
    "    if isinstance(params, list):\n",
    "        y_preds = np.zeros((len(params), len(y)))\n",
    "        for i, (param, gamma) in enumerate(zip(params, gammas)):\n",
    "            preds = model.apply(param, gamma, X).ravel()\n",
    "            y_preds[i] = preds\n",
    "\n",
    "        y_preds = np.mean(y_preds, axis=0)\n",
    "        rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "    else:\n",
    "        y_preds = model.apply(params, gammas, X).ravel()\n",
    "        rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "\n",
    "    return rmse\n",
    "\n",
    "def score_bg_bnn_model(model, X, y, params, gammas):\n",
    "    if isinstance(params, list):\n",
    "        y_preds = np.zeros((len(params), len(y)))\n",
    "        for i, (param, gamma) in enumerate(zip(params, gammas)):\n",
    "            preds = model.apply(param, gamma, X).ravel()\n",
    "            # preds_mean = preds[::2]\n",
    "            y_preds[i] = preds\n",
    "\n",
    "        y_preds = np.mean(y_preds, axis=0)\n",
    "        rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "        if np.isfinite(y_preds).all():\n",
    "            r2 = r2_score(y, y_preds)\n",
    "        else:\n",
    "            r2 = np.nan\n",
    "    else:\n",
    "        y_preds = model.apply(params, gammas, X)\n",
    "        preds_mean = y_preds[::2]\n",
    "        rmse = jnp.sqrt(jnp.mean((y - preds_mean)**2))\n",
    "        if np.isfinite(y_preds).all():\n",
    "            r2 = r2_score(y, y_preds)\n",
    "        else:\n",
    "            r2 = np.nan\n",
    "\n",
    "    return rmse, r2\n",
    "\n",
    "def eval_per_model_score_bg(model, X, y, params, gammas):\n",
    "    scores = []\n",
    "\n",
    "    for param, gamma in zip(params, gammas):\n",
    "        preds = model.apply(param, gamma, X).ravel()\n",
    "        # preds_mean = preds[::2]\n",
    "        rmse = jnp.sqrt(jnp.mean((y - preds)**2))\n",
    "        scores.append(rmse)\n",
    "\n",
    "\n",
    "\n",
    "    return np.array(scores)"
   ],
   "metadata": {
    "cellView": "form",
    "id": "xsAzls24BIPb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "\n",
    "seed = 0\n",
    "epochs = 100\n",
    "num_cycles = 10\n",
    "batch_size = 80\n",
    "lr_0 = 1e-3\n",
    "hidden_sizes = [6]\n",
    "act_fn = jax.nn.swish\n",
    "\n",
    "def objective(trial, seed, x_train, x_val, y_train, y_val, J, epochs, num_cycles,\n",
    "              batch_size, hidden_sizes, lr_0, act_fn):\n",
    "\n",
    "    disc_lr = trial.suggest_float(\"disc_lr\", 0.1, 0.9)\n",
    "    temp = trial.suggest_categorical(\"temp\", [1e-3, 1e-2, 1e-1, 0.5, 1.])\n",
    "    beta = trial.suggest_float(\"beta\", 0.7, 0.9)\n",
    "    eta = 1.0\n",
    "    mu = trial.suggest_float(\"mu\", 1.0, 1e2, log=True)\n",
    "    sigma = 1.0\n",
    "    torch.manual_seed(seed)\n",
    "    data_loader = NumpyLoader(NumpyData(x_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    bg_bnn_model, states, disc_states = train_bg_bnn_model(seed, data_loader, epochs, num_cycles, beta, lr_0, disc_lr,\n",
    "                                                           hidden_sizes, temp, sigma, eta, mu, J, act_fn)\n",
    "\n",
    "    rmse = eval_bg_bnn_model(bg_bnn_model, x_val, y_val, states, disc_states)\n",
    "    return rmse"
   ],
   "metadata": {
    "id": "mkycladN5pD1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sampler = optuna.samplers.TPESampler(seed=seed)\n",
    "study = optuna.create_study(sampler=sampler)\n",
    "study.optimize(lambda trial: objective(trial, seed, X_train, X_val, y_train, y_val, J, epochs, num_cycles, batch_size, hidden_sizes, lr_0, act_fn), timeout=60)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-XVI23v53Yt",
    "outputId": "414c3548-931e-4c5b-fcf4-cddf65c2cd48",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'disc_lr': 0.7695758245603153, 'temp': 0.1, 'beta': 0.8435518997739255, 'mu': 4.555786039977584}\n"
     ]
    }
   ],
   "source": [
    "bnn_config = study.best_params\n",
    "print(bnn_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from tree_utils import tree_stack\n",
    "\n",
    "def false_selection_rate(ft_idxs):\n",
    "    causal_fts = {0, 1, 2, 3, 4}\n",
    "    sel_fts = set(ft_idxs)\n",
    "    diff = sel_fts.difference(causal_fts)\n",
    "\n",
    "    return (len(diff)/len(ft_idxs))\n",
    "\n",
    "def negative_selection_rate(ft_idxs):\n",
    "    causal_fts = {0, 1, 2, 3, 4}\n",
    "    sel_fts = set(ft_idxs)\n",
    "    diff = causal_fts.difference(sel_fts)\n",
    "\n",
    "    return (len(diff)/len(causal_fts))\n",
    "\n",
    "\n",
    "def evaluate_bnn_bg_models(model, X, y, params, gammas):\n",
    "    eval_fn = lambda p, g: model.apply(p, g, X).ravel()\n",
    "    preds = jax.vmap(eval_fn)(params, gammas)\n",
    "    preds = preds.reshape(-1, preds.shape[-1])\n",
    "    losses = jax.vmap(lambda x, z: jnp.sqrt(jnp.mean((x - z)**2)), in_axes=(0, None))(preds, y)\n",
    "    # mean_loss = jnp.sqrt(jnp.mean(losses, axis=-1))\n",
    "    return jnp.mean(losses)\n",
    "\n",
    "\n",
    "def get_feats_dropout_loss(model, params, gammas, X, y):\n",
    "    var_loss_dict = {\"feats_idx\": [], \"num_models\": [] , \"loss_on\": [], \"loss_off\": [], \"loss_diff\": []}\n",
    "\n",
    "    disc_states = tree_stack(gammas)\n",
    "    contin_states = tree_stack(params)\n",
    "\n",
    "\n",
    "    # print(disc_states.shape)\n",
    "    eval_fn = jax.jit(lambda X, y, params, gammas: evaluate_bnn_bg_models(model, X, y, params, gammas))\n",
    "    p = X.shape[1]\n",
    "\n",
    "    for idx in range(p):\n",
    "        idx_on = np.argwhere(disc_states[:,idx] == 1.).ravel()\n",
    "        loss_on, loss_off = 0., 0.\n",
    "        if idx_on.size == 0: ## irrelevant feature\n",
    "            loss_diff = 1e9\n",
    "        else:\n",
    "            disc_states_on = disc_states[idx_on]\n",
    "            params_on = jax.tree_util.tree_map(lambda x: x[idx_on], contin_states)\n",
    "            loss_on = eval_fn(X, y, params_on, disc_states_on)\n",
    "\n",
    "            # Turn-off the variable, and see how the loss changes\n",
    "            disc_states_off = disc_states_on.at[:,idx].set(0)\n",
    "            loss_off = eval_fn(X, y, params_on, disc_states_off)\n",
    "\n",
    "            # loss_diff = (loss_on - loss_off) * (len(idx_on) / num_models)\n",
    "            loss_diff = (loss_on - loss_off)\n",
    "\n",
    "\n",
    "        var_loss_dict[\"feats_idx\"].append(idx)\n",
    "        var_loss_dict[\"num_models\"].append(idx_on.size)\n",
    "        var_loss_dict[\"loss_on\"].append(loss_on)\n",
    "        var_loss_dict[\"loss_off\"].append(loss_off)\n",
    "        var_loss_dict[\"loss_diff\"].append(loss_diff)\n",
    "\n",
    "\n",
    "    var_loss_df = pd.DataFrame(var_loss_dict).sort_values(by=\"loss_diff\")\n",
    "\n",
    "    return var_loss_df\n",
    "\n",
    "def train_rf_model(seed, X, y, train_idxs, val_idxs):\n",
    "\n",
    "    # cv = KFold(n_splits=5)\n",
    "    cv = [(train_idxs, val_idxs) for _ in range(5)]\n",
    "    param_grid = {\n",
    "        'max_depth': [80, 100, 120],\n",
    "        'max_features': [2, 3],\n",
    "        'min_samples_leaf': [3, 4, 5],\n",
    "        'min_samples_split': [8, 10, 12],\n",
    "        'n_estimators': [100, 500, 1000]\n",
    "    }\n",
    "\n",
    "    rf_reg = RandomForestRegressor(random_state=seed, max_samples=1.0)\n",
    "    grid_cv = GridSearchCV(estimator = rf_reg, param_grid = param_grid,\n",
    "                           cv = cv, n_jobs = -1, verbose = 0, scoring=\"neg_root_mean_squared_error\").fit(X, y)\n",
    "\n",
    "    rf_reg = RandomForestRegressor(random_state=seed, max_samples=1.0, **grid_cv.best_params_)\n",
    "    rf_reg.fit(X, y)\n",
    "\n",
    "    return rf_reg\n",
    "\n",
    "def eval_rf_model(model, X, y):\n",
    "    y_preds = model.predict(X)\n",
    "    rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "    r2 = r2_score(y, y_preds)\n",
    "    return rmse, r2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "num_cycles = 10\n",
    "batch_size = 80\n",
    "lr_0 = 1e-3\n",
    "sigma = 1.0\n",
    "eta = 1.0\n",
    "disc_lr_0 = bnn_config[\"disc_lr\"]\n",
    "mu = bnn_config[\"mu\"]\n",
    "temp = bnn_config[\"temp\"]\n",
    "beta = bnn_config[\"beta\"]\n",
    "\n",
    "data_loader = NumpyLoader(NumpyData(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "bg_bnn_model, bnn_bg_states, bg_disc_states = train_bg_bnn_model(seed, data_loader, epochs, num_cycles, beta, lr_0, disc_lr_0,\n",
    "                                                                     hidden_sizes, temp, sigma, eta, mu, J, act_fn)\n",
    "\n",
    "dropout_loss = get_feats_dropout_loss(bg_bnn_model, bnn_bg_states, bg_disc_states, X_val, y_val)\n",
    "\n",
    "print(false_selection_rate(dropout_loss[\"feats_idx\"][:5].to_list()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "150"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bg_disc_states)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "     feats_idx  num_models    loss_on   loss_off     loss_diff\n1            1         126  3.1637871   3.233195   -0.06940794\n3            3         139  3.3348029  3.3934684  -0.058665514\n2            2         116  3.0309684   3.070101  -0.039132595\n4            4          98   2.907516  2.9180067  -0.010490656\n0            0          99  2.9239378  2.9320858  -0.008147955\n..         ...         ...        ...        ...           ...\n339        339          94  2.8699884  2.8593206   0.010667801\n469        469         100  2.8946939  2.8834436  0.0112502575\n321        321          97  2.9019895  2.8899856   0.012003899\n397        397          93    2.83549  2.8224888   0.013001204\n220        220          97  2.8501527  2.8284853    0.02166748\n\n[500 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feats_idx</th>\n      <th>num_models</th>\n      <th>loss_on</th>\n      <th>loss_off</th>\n      <th>loss_diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>126</td>\n      <td>3.1637871</td>\n      <td>3.233195</td>\n      <td>-0.06940794</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>139</td>\n      <td>3.3348029</td>\n      <td>3.3934684</td>\n      <td>-0.058665514</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>116</td>\n      <td>3.0309684</td>\n      <td>3.070101</td>\n      <td>-0.039132595</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>98</td>\n      <td>2.907516</td>\n      <td>2.9180067</td>\n      <td>-0.010490656</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>99</td>\n      <td>2.9239378</td>\n      <td>2.9320858</td>\n      <td>-0.008147955</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>339</th>\n      <td>339</td>\n      <td>94</td>\n      <td>2.8699884</td>\n      <td>2.8593206</td>\n      <td>0.010667801</td>\n    </tr>\n    <tr>\n      <th>469</th>\n      <td>469</td>\n      <td>100</td>\n      <td>2.8946939</td>\n      <td>2.8834436</td>\n      <td>0.0112502575</td>\n    </tr>\n    <tr>\n      <th>321</th>\n      <td>321</td>\n      <td>97</td>\n      <td>2.9019895</td>\n      <td>2.8899856</td>\n      <td>0.012003899</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>397</td>\n      <td>93</td>\n      <td>2.83549</td>\n      <td>2.8224888</td>\n      <td>0.013001204</td>\n    </tr>\n    <tr>\n      <th>220</th>\n      <td>220</td>\n      <td>97</td>\n      <td>2.8501527</td>\n      <td>2.8284853</td>\n      <td>0.02166748</td>\n    </tr>\n  </tbody>\n</table>\n<p>500 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 3.1551499366760254, r2_score: 0.5481285440288763\n"
     ]
    }
   ],
   "source": [
    "bnn_bg_rmse_test, bnn_bg_r2_test = score_bg_bnn_model(bg_bnn_model, X_test, y_test, bnn_bg_states, bg_disc_states)\n",
    "print(f\"Test RMSE: {bnn_bg_rmse_test}, r2_score: {bnn_bg_r2_test}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "save_dir = f\"{data_dir}/exp_data_5/synthetic/exp1_mao\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "epochs = 200\n",
    "hpo_epochs = 100\n",
    "num_cycles = 10\n",
    "batch_size = 80\n",
    "lr_0 = 1e-3\n",
    "hidden_sizes = [6]\n",
    "sigma = 1.0\n",
    "eta = 1.0\n",
    "act_fn = jax.nn.swish\n",
    "k = dataset.shape[0]\n",
    "\n",
    "dropout_loss_lst = []\n",
    "bnn_fdr = []\n",
    "rf_fdr = []\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "res_ft_dict = {\"model\": [], \"rmse\": [], \"fdr\": []}\n",
    "\n",
    "seeds = [422,261,968,282,739,573,220,413,745,775]\n",
    "s = 5 # change this? See Wojats et.al 2020\n",
    "\n",
    "for seed in tqdm(seeds):\n",
    "    dataset, output = generate_dataset_v1(seed)\n",
    "    X_df, y_df = pd.DataFrame(dataset), pd.Series(output)\n",
    "    X_train_outer_df, X_test_df, y_train_outer_df, y_test_df = train_test_split(X_df, y_df, shuffle=False, test_size=0.2)\n",
    "    X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X_train_outer_df, y_train_outer_df, test_size=0.2, shuffle=False)\n",
    "    train_indices, val_indices = X_train_df.index.to_list(), X_val_df.index.to_list()\n",
    "    X_train, X_val, X_test = X_train_df.to_numpy(), X_val_df.to_numpy(), X_test_df.to_numpy()\n",
    "    y_train, y_val, y_test = y_train_df.to_numpy(), y_val_df.to_numpy(), y_test_df.to_numpy()\n",
    "    data_loader = NumpyLoader(NumpyData(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "    study = optuna.create_study(sampler=sampler)\n",
    "    study.optimize(lambda trial: objective(trial, seed, X_train, X_val, y_train, y_val, J, hpo_epochs, num_cycles, batch_size, hidden_sizes, lr_0, act_fn), timeout=100)\n",
    "    bnn_config = study.best_params\n",
    "    disc_lr_0 = bnn_config[\"disc_lr\"]\n",
    "    mu = bnn_config[\"mu\"]\n",
    "    temp = bnn_config[\"temp\"]\n",
    "    beta = bnn_config[\"beta\"]\n",
    "    bg_bnn_model, bnn_bg_states, bg_disc_states = train_bg_bnn_model(seed, data_loader, epochs, num_cycles, beta, lr_0, disc_lr_0,\n",
    "                                                                 hidden_sizes, temp, sigma, eta, mu, J, act_fn)\n",
    "    bnn_bg_rmse_test, _ = score_bg_bnn_model(bg_bnn_model, X_test, y_test, bnn_bg_states, bg_disc_states)\n",
    "    dropout_loss = get_feats_dropout_loss(bg_bnn_model, bnn_bg_states, bg_disc_states, X_val, y_val)\n",
    "    dropout_loss_lst.append(dropout_loss)\n",
    "    bnn_fdr = false_selection_rate(dropout_loss[\"feats_idx\"][:s].to_list())\n",
    "\n",
    "    rf_model = train_rf_model(seed, X_train_outer_df, y_train_outer_df, train_indices, val_indices)\n",
    "    rf_rmse_test, _ = eval_rf_model(rf_model, X_test, y_test)\n",
    "    rf_fdr = false_selection_rate(np.argsort(rf_model.feature_importances_)[::-1][:s])\n",
    "\n",
    "    res_ft_dict[\"model\"].append(\"BNN\")\n",
    "    res_ft_dict[\"rmse\"].append(bnn_bg_rmse_test)\n",
    "    res_ft_dict[\"fdr\"].append(bnn_fdr)\n",
    "\n",
    "    res_ft_dict[\"model\"].append(\"RF\")\n",
    "    res_ft_dict[\"rmse\"].append(rf_rmse_test)\n",
    "    res_ft_dict[\"fdr\"].append(rf_fdr)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9dvn0TYBQE9",
    "outputId": "0e37bdfc-476c-4db0-cffe-ddb976411019",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 153,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [38:35<00:00, 231.59s/it]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "   model      rmse  fdr\n0    BNN  3.224490  0.2\n1     RF  2.693497  0.8\n2    BNN  3.288866  0.4\n3     RF  2.935442  0.8\n4    BNN  3.332286  0.0\n5     RF  3.190772  0.8\n6    BNN  3.068228  0.2\n7     RF  2.737681  1.0\n8    BNN  3.095022  0.2\n9     RF  2.853587  0.4\n10   BNN  3.239062  0.4\n11    RF  2.910877  0.8\n12   BNN  2.507105  0.6\n13    RF  2.541606  1.0\n14   BNN  2.844461  0.4\n15    RF  2.630267  0.4\n16   BNN  3.221443  0.2\n17    RF  2.900068  1.0\n18   BNN  3.252140  0.4\n19    RF  2.794371  0.6",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>rmse</th>\n      <th>fdr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BNN</td>\n      <td>3.224490</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>RF</td>\n      <td>2.693497</td>\n      <td>0.8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>BNN</td>\n      <td>3.288866</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>RF</td>\n      <td>2.935442</td>\n      <td>0.8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BNN</td>\n      <td>3.332286</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>RF</td>\n      <td>3.190772</td>\n      <td>0.8</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>BNN</td>\n      <td>3.068228</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>RF</td>\n      <td>2.737681</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>BNN</td>\n      <td>3.095022</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>RF</td>\n      <td>2.853587</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>BNN</td>\n      <td>3.239062</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>RF</td>\n      <td>2.910877</td>\n      <td>0.8</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>BNN</td>\n      <td>2.507105</td>\n      <td>0.6</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>RF</td>\n      <td>2.541606</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>BNN</td>\n      <td>2.844461</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>RF</td>\n      <td>2.630267</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>BNN</td>\n      <td>3.221443</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>RF</td>\n      <td>2.900068</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>BNN</td>\n      <td>3.252140</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>RF</td>\n      <td>2.794371</td>\n      <td>0.6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df = pd.read_csv(f\"{save_dir}/res_df_hn_6.csv\")\n",
    "res_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "model\nBNN    3.107310\nRF     2.818817\nName: rmse, dtype: float64"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.groupby(['model'])[\"rmse\"].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "model\nBNN    0.30\nRF     0.76\nName: fdr, dtype: float64"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.groupby(['model'])[\"fdr\"].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [39:38<00:00, 237.81s/it]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 200\n",
    "hpo_epochs = 100\n",
    "num_cycles = 10\n",
    "batch_size = 80\n",
    "lr_0 = 1e-3\n",
    "hidden_sizes = [100]\n",
    "sigma = 1.0\n",
    "eta = 1.0\n",
    "act_fn = jax.nn.swish\n",
    "k = dataset.shape[0]\n",
    "\n",
    "dropout_loss_lst = []\n",
    "bnn_fdr = []\n",
    "rf_fdr = []\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "res_ft_dict_2 = {\"model\": [], \"rmse\": [], \"fdr\": []}\n",
    "\n",
    "seeds = [422,261,968,282,739,573,220,413,745,775]\n",
    "s = 5 # change this? See Wojats et.al 2020\n",
    "\n",
    "for seed in tqdm(seeds):\n",
    "    dataset, output = generate_dataset_v1(seed)\n",
    "    X_df, y_df = pd.DataFrame(dataset), pd.Series(output)\n",
    "    X_train_outer_df, X_test_df, y_train_outer_df, y_test_df = train_test_split(X_df, y_df, shuffle=False, test_size=0.2)\n",
    "    X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X_train_outer_df, y_train_outer_df, test_size=0.2, shuffle=False)\n",
    "    train_indices, val_indices = X_train_df.index.to_list(), X_val_df.index.to_list()\n",
    "    X_train, X_val, X_test = X_train_df.to_numpy(), X_val_df.to_numpy(), X_test_df.to_numpy()\n",
    "    y_train, y_val, y_test = y_train_df.to_numpy(), y_val_df.to_numpy(), y_test_df.to_numpy()\n",
    "    data_loader = NumpyLoader(NumpyData(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "    study = optuna.create_study(sampler=sampler)\n",
    "    study.optimize(lambda trial: objective(trial, seed, X_train, X_val, y_train, y_val, J, hpo_epochs, num_cycles, batch_size, hidden_sizes, lr_0, act_fn), timeout=100)\n",
    "    bnn_config = study.best_params\n",
    "    disc_lr_0 = bnn_config[\"disc_lr\"]\n",
    "    mu = bnn_config[\"mu\"]\n",
    "    temp = bnn_config[\"temp\"]\n",
    "    beta = bnn_config[\"beta\"]\n",
    "\n",
    "    pickle.dump(bnn_config, open(f\"{save_dir}/bnn_config_s_{seed}.pkl\", \"wb\"))\n",
    "\n",
    "    bg_bnn_model, bnn_bg_states, bg_disc_states = train_bg_bnn_model(seed, data_loader, epochs, num_cycles, beta, lr_0, disc_lr_0,\n",
    "                                                                     hidden_sizes, temp, sigma, eta, mu, J, act_fn)\n",
    "    bnn_bg_rmse_test, _ = score_bg_bnn_model(bg_bnn_model, X_test, y_test, bnn_bg_states, bg_disc_states)\n",
    "    dropout_loss = get_feats_dropout_loss(bg_bnn_model, bnn_bg_states, bg_disc_states, X_val, y_val)\n",
    "    dropout_loss_lst.append(dropout_loss)\n",
    "    bnn_fdr = false_selection_rate(dropout_loss[\"feats_idx\"][:s].to_list())\n",
    "\n",
    "    rf_model = train_rf_model(seed, X_train_outer_df, y_train_outer_df, train_indices, val_indices)\n",
    "    rf_rmse_test, _ = eval_rf_model(rf_model, X_test, y_test)\n",
    "    rf_fdr = false_selection_rate(np.argsort(rf_model.feature_importances_)[::-1][:s])\n",
    "\n",
    "    pickle.dump(rf_model, open(f\"{save_dir}/rf_model_{seed}.pkl\", \"wb\"))\n",
    "\n",
    "    res_ft_dict_2[\"model\"].append(\"BNN\")\n",
    "    res_ft_dict_2[\"rmse\"].append(bnn_bg_rmse_test)\n",
    "    res_ft_dict_2[\"fdr\"].append(bnn_fdr)\n",
    "\n",
    "    res_ft_dict_2[\"model\"].append(\"RF\")\n",
    "    res_ft_dict_2[\"rmse\"].append(rf_rmse_test)\n",
    "    res_ft_dict_2[\"fdr\"].append(rf_fdr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "   model      rmse  fdr\n0    BNN  2.701590  0.2\n1     RF  2.693497  0.8\n2    BNN  2.766211  0.2\n3     RF  2.935442  0.8\n4    BNN  3.048509  0.0\n5     RF  3.190772  0.8\n6    BNN  2.827833  0.0\n7     RF  2.737681  1.0\n8    BNN  2.594652  0.2\n9     RF  2.853587  0.4\n10   BNN  2.683004  0.2\n11    RF  2.910877  0.8\n12   BNN  2.499600  0.6\n13    RF  2.541606  1.0\n14   BNN  2.790678  0.4\n15    RF  2.630267  0.4\n16   BNN  3.081494  0.0\n17    RF  2.900068  1.0\n18   BNN  2.759740  0.2\n19    RF  2.794371  0.6",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>rmse</th>\n      <th>fdr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BNN</td>\n      <td>2.701590</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>RF</td>\n      <td>2.693497</td>\n      <td>0.8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>BNN</td>\n      <td>2.766211</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>RF</td>\n      <td>2.935442</td>\n      <td>0.8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BNN</td>\n      <td>3.048509</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>RF</td>\n      <td>3.190772</td>\n      <td>0.8</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>BNN</td>\n      <td>2.827833</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>RF</td>\n      <td>2.737681</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>BNN</td>\n      <td>2.594652</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>RF</td>\n      <td>2.853587</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>BNN</td>\n      <td>2.683004</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>RF</td>\n      <td>2.910877</td>\n      <td>0.8</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>BNN</td>\n      <td>2.499600</td>\n      <td>0.6</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>RF</td>\n      <td>2.541606</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>BNN</td>\n      <td>2.790678</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>RF</td>\n      <td>2.630267</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>BNN</td>\n      <td>3.081494</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>RF</td>\n      <td>2.900068</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>BNN</td>\n      <td>2.759740</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>RF</td>\n      <td>2.794371</td>\n      <td>0.6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df_2 = pd.read_csv(f\"{save_dir}/res_df_hn_100.csv\")\n",
    "res_df_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "model\nBNN    2.775331\nRF     2.818817\nName: rmse, dtype: float64"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df_2.groupby(['model'])[\"rmse\"].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "model\nBNN    0.20\nRF     0.76\nName: fdr, dtype: float64"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df_2.groupby(['model'])[\"fdr\"].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Wojtas et.al 2020"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}