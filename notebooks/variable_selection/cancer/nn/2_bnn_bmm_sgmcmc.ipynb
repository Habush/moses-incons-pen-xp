{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
      "Requirement already satisfied: jax[cuda112] in /usr/local/lib/python3.9/dist-packages (0.3.14)\n",
      "Collecting jax[cuda112]\n",
      "  Downloading jax-0.3.24.tar.gz (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[33mWARNING: jax 0.3.24 does not provide the extra 'cuda112'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]) (1.23.1)\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]) (1.8.1)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]) (4.3.0)\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jax: filename=jax-0.3.24-py3-none-any.whl size=1297412 sha256=0826405ff2ae02cef01cf14ab26ca2f785ae38c2c8ea1d09400134c948e19192\n",
      "  Stored in directory: /root/.cache/pip/wheels/13/d4/90/1904bbdf0ee77093daa3bf6a4939e5ddc266402262423e29d1\n",
      "Successfully built jax\n",
      "Installing collected packages: jax\n",
      "  Attempting uninstall: jax\n",
      "    Found existing installation: jax 0.3.14\n",
      "    Uninstalling jax-0.3.14:\n",
      "      Successfully uninstalled jax-0.3.14\n",
      "Successfully installed jax-0.3.24\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
      "Requirement already satisfied: jaxlib[cuda112] in /usr/local/lib/python3.9/dist-packages (0.3.8+cuda11.cudnn82)\n",
      "Collecting jaxlib[cuda112]\n",
      "  Downloading https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.24%2Bcuda11.cudnn82-cp39-cp39-manylinux2014_x86_64.whl (153.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: jaxlib 0.3.24+cuda11.cudnn82 does not provide the extra 'cuda112'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from jaxlib[cuda112]) (1.23.1)\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jaxlib[cuda112]) (1.8.1)\n",
      "Installing collected packages: jaxlib\n",
      "  Attempting uninstall: jaxlib\n",
      "    Found existing installation: jaxlib 0.3.8+cuda11.cudnn82\n",
      "    Uninstalling jaxlib-0.3.8+cuda11.cudnn82:\n",
      "      Successfully uninstalled jaxlib-0.3.8+cuda11.cudnn82\n",
      "Successfully installed jaxlib-0.3.24+cuda11.cudnn82\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting optax\n",
      "  Downloading optax-0.1.3-py3-none-any.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.1/145.1 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.9/dist-packages (from optax) (4.3.0)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.9/dist-packages (from optax) (0.3.24+cuda11.cudnn82)\n",
      "Collecting chex>=0.0.4\n",
      "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from optax) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from optax) (1.23.1)\n",
      "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.9/dist-packages (from optax) (0.3.24)\n",
      "Collecting toolz>=0.9.0\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dm-tree>=0.1.5\n",
      "  Downloading dm_tree-0.1.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (142 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (1.8.1)\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
      "Installing collected packages: dm-tree, toolz, chex, optax\n",
      "Successfully installed chex-0.1.5 dm-tree-0.1.7 optax-0.1.3 toolz-0.12.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting dm-haiku\n",
      "  Downloading dm_haiku-0.0.8-py3-none-any.whl (350 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m350.2/350.2 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jmp>=0.0.2\n",
      "  Downloading jmp-0.0.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (1.23.1)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (1.1.0)\n",
      "Installing collected packages: tabulate, jmp, dm-haiku\n",
      "Successfully installed dm-haiku-0.0.8 jmp-0.0.2 tabulate-0.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tensorflow-probability==0.17\n",
      "  Downloading tensorflow_probability-0.17.0-py2.py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (1.23.1)\n",
      "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (0.4.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (5.1.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (1.1.0)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (0.1.7)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow-probability==0.17) (1.14.0)\n",
      "Installing collected packages: tensorflow-probability\n",
      "Successfully installed tensorflow-probability-0.17.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting lark==1.1.2\n",
      "  Downloading lark-1.1.2-py2.py3-none-any.whl (104 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.7/104.7 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lark\n",
      "Successfully installed lark-1.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/blackjax-devs/blackjax.git\n",
      "  Cloning https://github.com/blackjax-devs/blackjax.git to /tmp/pip-req-build-o6ob54og\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/blackjax-devs/blackjax.git /tmp/pip-req-build-o6ob54og\n",
      "  Resolved https://github.com/blackjax-devs/blackjax.git to commit becd2d20b987cb496f873d84fa4f6265f17d4483\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fastprogress>=0.2.0\n",
      "  Downloading fastprogress-1.0.3-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: jax>=0.3.13 in /usr/local/lib/python3.9/dist-packages (from blackjax==0.9.6+47.gbecd2d2) (0.3.24)\n",
      "Requirement already satisfied: jaxlib>=0.3.10 in /usr/local/lib/python3.9/dist-packages (from blackjax==0.9.6+47.gbecd2d2) (0.3.24+cuda11.cudnn82)\n",
      "Collecting jaxopt>=0.5.5\n",
      "  Downloading jaxopt-0.5.5-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+47.gbecd2d2) (1.23.1)\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+47.gbecd2d2) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+47.gbecd2d2) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+47.gbecd2d2) (4.3.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.1 in /usr/local/lib/python3.9/dist-packages (from jaxopt>=0.5.5->blackjax==0.9.6+47.gbecd2d2) (3.5.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from jaxopt>=0.5.5->blackjax==0.9.6+47.gbecd2d2) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+47.gbecd2d2) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+47.gbecd2d2) (9.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+47.gbecd2d2) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+47.gbecd2d2) (4.34.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+47.gbecd2d2) (1.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+47.gbecd2d2) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+47.gbecd2d2) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+47.gbecd2d2) (1.14.0)\n",
      "Building wheels for collected packages: blackjax\n",
      "  Building wheel for blackjax (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for blackjax: filename=blackjax-0.9.6+47.gbecd2d2-py3-none-any.whl size=114162 sha256=0052c8c53637aa02b2619daacbfb97a4ce963bae34756cd8b2374f9c7d64bb4e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-oysdcekx/wheels/e6/1e/f6/a6e0408a4e374b9cdb789b1769716b4ed61eef520a2dd702b1\n",
      "Successfully built blackjax\n",
      "Installing collected packages: fastprogress, jaxopt, blackjax\n",
      "Successfully installed blackjax-0.9.6+47.gbecd2d2 fastprogress-1.0.3 jaxopt-0.5.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:2 https://deb.nodesource.com/node_16.x focal InRelease [4583 B]            \u001b[0m\u001b[33m\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]                \u001b[0m\u001b[33m\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]        \u001b[33m\n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [27.5 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]      \u001b[0m\u001b[33m\n",
      "Get:7 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [931 kB]\n",
      "Get:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease [18.1 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2269 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1661 kB]3m\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]0m\u001b[33m\u001b[33m\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]    \u001b[0m\u001b[33m\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]m\u001b[33m\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB][0m\u001b[33m\u001b[33m\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [1778 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2738 kB]\u001b[33m\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [30.2 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1229 kB]m\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]m\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [27.5 kB]\n",
      "Get:21 https://deb.nodesource.com/node_16.x focal/main amd64 Packages [774 B]  \u001b[0m\u001b[33m\n",
      "Get:22 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 Packages [29.4 kB]\n",
      "Fetched 24.2 MB in 1s (18.2 MB/s)[33m                        \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "100 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  fonts-liberation libann0 libcdt5 libcgraph6 libgts-0.7-5 libgts-bin libgvc6\n",
      "  libgvpr2 liblab-gamut1 libpathplan4\n",
      "Suggested packages:\n",
      "  gsfonts graphviz-doc\n",
      "The following NEW packages will be installed:\n",
      "  fonts-liberation graphviz libann0 libcdt5 libcgraph6 libgts-0.7-5 libgts-bin\n",
      "  libgvc6 libgvpr2 liblab-gamut1 libpathplan4\n",
      "0 upgraded, 11 newly installed, 0 to remove and 100 not upgraded.\n",
      "Need to get 2701 kB of archives.\n",
      "After this operation, 11.3 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 fonts-liberation all 1:1.07.4-11 [822 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libann0 amd64 1.1.2+doc-7build1 [26.0 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 libcdt5 amd64 2.42.2-3build2 [18.7 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 libcgraph6 amd64 2.42.2-3build2 [41.3 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgts-0.7-5 amd64 0.7.6+darcs121130-4 [150 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/universe amd64 libpathplan4 amd64 2.42.2-3build2 [21.9 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgvc6 amd64 2.42.2-3build2 [647 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgvpr2 amd64 2.42.2-3build2 [167 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/universe amd64 liblab-gamut1 amd64 2.42.2-3build2 [177 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/universe amd64 graphviz amd64 2.42.2-3build2 [590 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgts-bin amd64 0.7.6+darcs121130-4 [41.3 kB]\n",
      "Fetched 2701 kB in 1s (2965 kB/s)      \u001b[0m\u001b[33m\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package fonts-liberation.\n",
      "(Reading database ... 78556 files and directories currently installed.)\n",
      "Preparing to unpack .../00-fonts-liberation_1%3a1.07.4-11_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking fonts-liberation (1:1.07.4-11) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  4%]\u001b[49m\u001b[39m [##........................................................] \u001b8Selecting previously unselected package libann0.\n",
      "Preparing to unpack .../01-libann0_1.1.2+doc-7build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  7%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libann0 (1.1.2+doc-7build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  9%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Selecting previously unselected package libcdt5:amd64.\n",
      "Preparing to unpack .../02-libcdt5_2.42.2-3build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Unpacking libcdt5:amd64 (2.42.2-3build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 13%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Selecting previously unselected package libcgraph6:amd64.\n",
      "Preparing to unpack .../03-libcgraph6_2.42.2-3build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 16%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking libcgraph6:amd64 (2.42.2-3build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Selecting previously unselected package libgts-0.7-5:amd64.\n",
      "Preparing to unpack .../04-libgts-0.7-5_0.7.6+darcs121130-4_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking libgts-0.7-5:amd64 (0.7.6+darcs121130-4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 22%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package libpathplan4:amd64.\n",
      "Preparing to unpack .../05-libpathplan4_2.42.2-3build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [##############............................................] \u001b8Unpacking libpathplan4:amd64 (2.42.2-3build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 27%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Selecting previously unselected package libgvc6.\n",
      "Preparing to unpack .../06-libgvc6_2.42.2-3build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 29%]\u001b[49m\u001b[39m [################..........................................] \u001b8Unpacking libgvc6 (2.42.2-3build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [##################........................................] \u001b8Selecting previously unselected package libgvpr2:amd64.\n",
      "Preparing to unpack .../07-libgvpr2_2.42.2-3build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Unpacking libgvpr2:amd64 (2.42.2-3build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 36%]\u001b[49m\u001b[39m [####################......................................] \u001b8Selecting previously unselected package liblab-gamut1:amd64.\n",
      "Preparing to unpack .../08-liblab-gamut1_2.42.2-3build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Unpacking liblab-gamut1:amd64 (2.42.2-3build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Selecting previously unselected package graphviz.\n",
      "Preparing to unpack .../09-graphviz_2.42.2-3build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [########################..................................] \u001b8Unpacking graphviz (2.42.2-3build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Selecting previously unselected package libgts-bin.\n",
      "Preparing to unpack .../10-libgts-bin_0.7.6+darcs121130-4_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Unpacking libgts-bin (0.7.6+darcs121130-4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 49%]\u001b[49m\u001b[39m [############################..............................] \u001b8Setting up liblab-gamut1:amd64 (2.42.2-3build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 51%]\u001b[49m\u001b[39m [#############################.............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 53%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up libgts-0.7-5:amd64 (0.7.6+darcs121130-4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 58%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Setting up libpathplan4:amd64 (2.42.2-3build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up libann0 (1.1.2+doc-7build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 64%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up fonts-liberation (1:1.07.4-11) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 69%]\u001b[49m\u001b[39m [#######################################...................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Setting up libcdt5:amd64 (2.42.2-3build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 73%]\u001b[49m\u001b[39m [##########################################................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [###########################################...............] \u001b8Setting up libcgraph6:amd64 (2.42.2-3build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 78%]\u001b[49m\u001b[39m [#############################################.............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Setting up libgts-bin (0.7.6+darcs121130-4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 84%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libgvc6 (2.42.2-3build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 87%]\u001b[49m\u001b[39m [##################################################........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Setting up libgvpr2:amd64 (2.42.2-3build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 91%]\u001b[49m\u001b[39m [####################################################......] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [######################################################....] \u001b8Setting up graphviz (2.42.2-3build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 96%]\u001b[49m\u001b[39m [#######################################################...] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [########################################################..] \u001b8Processing triggers for libc-bin (2.31-0ubuntu9.7) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "Processing triggers for fontconfig (2.13.1-2ubuntu3) ...\n",
      "\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  swig4.0\n",
      "Suggested packages:\n",
      "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
      "The following NEW packages will be installed:\n",
      "  swig swig4.0\n",
      "0 upgraded, 2 newly installed, 0 to remove and 100 not upgraded.\n",
      "Need to get 1086 kB of archives.\n",
      "After this operation, 5413 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig4.0 amd64 4.0.1-5build1 [1081 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig all 4.0.1-5build1 [5528 B]\n",
      "Fetched 1086 kB in 1s (1276 kB/s)\n",
      "Selecting previously unselected package swig4.0.\n",
      "(Reading database ... 78769 files and directories currently installed.)\n",
      "Preparing to unpack .../swig4.0_4.0.1-5build1_amd64.deb ...\n",
      "Unpacking swig4.0 (4.0.1-5build1) ...\n",
      "Selecting previously unselected package swig.\n",
      "Preparing to unpack .../swig_4.0.1-5build1_all.deb ...\n",
      "Unpacking swig (4.0.1-5build1) ...\n",
      "Setting up swig4.0 (4.0.1-5build1) ...\n",
      "Setting up swig (4.0.1-5build1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "Collecting smac\n",
      "  Downloading smac-1.4.0.tar.gz (202 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting distributed\n",
      "  Downloading distributed-2022.10.2-py3-none-any.whl (918 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m918.2/918.2 kB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dask\n",
      "  Downloading dask-2022.10.2-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ConfigSpace>=0.5.0\n",
      "  Downloading ConfigSpace-0.6.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m126.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pynisher<1.0.0\n",
      "  Downloading pynisher-0.6.4.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.9/dist-packages (from smac) (1.23.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.9/dist-packages (from smac) (1.1.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from smac) (1.8.1)\n",
      "Collecting emcee>=3.0.0\n",
      "  Downloading emcee-3.1.3-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from smac) (2022.7.9)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from smac) (1.1.0)\n",
      "Collecting pyrfr>=0.8.3\n",
      "  Downloading pyrfr-0.8.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from smac) (5.9.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from ConfigSpace>=0.5.0->smac) (4.3.0)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from ConfigSpace>=0.5.0->smac) (3.0.9)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from ConfigSpace>=0.5.0->smac) (0.29.30)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from pynisher<1.0.0->smac) (63.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22.0->smac) (3.1.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (2022.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (21.3)\n",
      "Collecting partd>=0.3.10\n",
      "  Downloading partd-1.3.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (2.1.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (8.1.3)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (0.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (5.4.1)\n",
      "Collecting msgpack>=0.6.0\n",
      "  Downloading msgpack-1.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 kB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from distributed->smac) (1.26.10)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from distributed->smac) (3.1.2)\n",
      "Collecting zict>=0.1.3\n",
      "  Downloading zict-2.2.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting locket>=1.0.0\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting tornado<6.2,>=6.0.3\n",
      "  Downloading tornado-6.1-cp39-cp39-manylinux2010_x86_64.whl (427 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.2/427.2 kB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sortedcontainers!=2.0.0,!=2.0.1\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting tblib>=1.6.0\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting heapdict\n",
      "  Downloading HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->distributed->smac) (2.1.1)\n",
      "Building wheels for collected packages: smac, pynisher\n",
      "  Building wheel for smac (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smac: filename=smac-1.4.0-py3-none-any.whl size=262348 sha256=3885051043e491b735e7f3d50c9d798f1aaa1f3d9c7f9b9d72968fca8810aa93\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/cc/e7/d683d9404760c4701ea2f64faaf689a8de718f701de63e71ea\n",
      "  Building wheel for pynisher (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pynisher: filename=pynisher-0.6.4-py3-none-any.whl size=7026 sha256=3e8f4331dc62e3dffb68196f07a91884a33aaaf75f4229688691493cab05d62e\n",
      "  Stored in directory: /root/.cache/pip/wheels/1d/de/5e/d4947b76b76ba27581d1e09f395eca1583a802203a41c04873\n",
      "Successfully built smac pynisher\n",
      "Installing collected packages: sortedcontainers, msgpack, heapdict, zict, tornado, tblib, pyrfr, pynisher, locket, emcee, partd, ConfigSpace, dask, distributed, smac\n",
      "  Attempting uninstall: tornado\n",
      "    Found existing installation: tornado 6.2\n",
      "    Uninstalling tornado-6.2:\n",
      "      Successfully uninstalled tornado-6.2\n",
      "Successfully installed ConfigSpace-0.6.0 dask-2022.10.2 distributed-2022.10.2 emcee-3.1.3 heapdict-1.0.1 locket-1.0.0 msgpack-1.0.4 partd-1.3.0 pynisher-0.6.4 pyrfr-0.8.3 smac-1.4.0 sortedcontainers-2.4.0 tblib-1.7.0 tornado-6.1 zict-2.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting graphviz\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting gplearn\n",
      "  Downloading gplearn-0.4.2-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from gplearn) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from gplearn) (1.1.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.23.1)\n",
      "Installing collected packages: gplearn\n",
      "Successfully installed gplearn-0.4.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U jax[cuda112]==0.3.17 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install -U jaxlib[cuda112]==0.3.17 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install optax\n",
    "!pip install dm-haiku\n",
    "!pip install tensorflow-probability==0.17\n",
    "!pip install lark==1.1.2\n",
    "!pip install git+https://github.com/blackjax-devs/blackjax.git\n",
    "!apt update\n",
    "!apt install -y graphviz\n",
    "!apt-get -y install swig\n",
    "!pip install smac\n",
    "!pip install graphviz\n",
    "!pip install gplearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"False\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "SERVER = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not SERVER:\n",
    "    %cd /home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn\n",
    "import jax\n",
    "import haiku as hk\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "import blackjax\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "tfd = tfp.distributions\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nn_util import *\n",
    "from gibbs_sampler import *\n",
    "plt.style.use('ggplot')\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.default_backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if SERVER:\n",
    "    data_dir = \".\"\n",
    "else:\n",
    "    data_dir = \"/home/xabush/code/snet/moses-incons-pen-xp/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p = 100\n",
    "seeds, data_dfs, net_dfs, feat_ls = load_bmm_files(f\"{data_dir}/bmm_data_thr_5_F_8_f{p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 220\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "seed_idx = 6\n",
    "seed, net, data = prepare_data(seeds, seed_idx, data_dfs, net_dfs, test_size=0.2, out_val_size=0.4)\n",
    "print(f\"seed: {seed}\")\n",
    "X_train, X_out_val, X_test, y_train, y_out_val, y_test = data\n",
    "# X_train, X_out_val, y_train, y_out_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=seed, shuffle=True)\n",
    "X_train, X_test, y_train, y_test = jax.device_put(X_train), jax.device_put(X_test), \\\n",
    "                                   jax.device_put(y_train), jax.device_put(y_test)\n",
    "\n",
    "X_out_val, y_out_val = jax.device_put(X_out_val), jax.device_put(y_out_val)\n",
    "# p = X_train.shape[1]\n",
    "rng_key = jax.random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Baseline Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "cv score: 0.7141653843826707, test_score: 0.6964807962228895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "log_param_grid = {\"C\":np.logspace(-2, 1, 10)}\n",
    "log_grid_cv = GridSearchCV(estimator=LogisticRegression(max_iter=10000), param_grid=log_param_grid, verbose=1, scoring=\"roc_auc\", cv=cv).fit(X_train, y_train)\n",
    "clf = LogisticRegression(max_iter=10000, C=log_grid_cv.best_params_[\"C\"])\n",
    "cv_score = np.mean(cross_val_score(clf, X_train, y_train, scoring=\"roc_auc\", cv=cv))\n",
    "clf.fit(X_train, y_train)\n",
    "test_score = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])\n",
    "print(f\"cv score: {cv_score}, test_score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from smac.facade.smac_mf_facade import SMAC4MF\n",
    "from smac.scenario.scenario import Scenario\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sgmcmc import MixedSGMCMC\n",
    "from run_nn_fisher_test_exp import run_logistic_regression\n",
    "from smac.configspace import ConfigurationSpace\n",
    "from ConfigSpace.hyperparameters import (\n",
    "    CategoricalHyperparameter,\n",
    "    UniformFloatHyperparameter,\n",
    "    UniformIntegerHyperparameter,\n",
    ")\n",
    "\n",
    "from ConfigSpace import InCondition, Configuration\n",
    "\n",
    "\n",
    "\n",
    "def get_configspace(input_size)-> ConfigurationSpace:\n",
    "    # Build Configuration Space which defines all parameters and their ranges.\n",
    "\n",
    "    if input_size == 100:\n",
    "        layer_dims  = [50, 60, 70, 80]\n",
    "    else:\n",
    "        layer_dims = [250, 300, 350, 400]\n",
    "\n",
    "    cs  = ConfigurationSpace()\n",
    "\n",
    "    layer_dim = CategoricalHyperparameter(\"layer_dim\", layer_dims, default_value=layer_dims[0])\n",
    "    activation = CategoricalHyperparameter(\"activation\", [\"relu\", \"tanh\"], default_value=\"tanh\")\n",
    "    # lr_schedule = CategoricalHyperparameter(\"lr_schedule\", [\"cyclical\"], default_value=\"cyclical\")\n",
    "    disc_lr = CategoricalHyperparameter(\"disc_lr\", [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.5] , default_value=1e-1)\n",
    "    contin_lr = CategoricalHyperparameter(\"contin_lr\", [1e-5, 1e-4, 1e-3, 1e-2], default_value=1e-5) #TODO Extend range\n",
    "    # num_cycles = CategoricalHyperparameter(\"num_cycles\", [3, 4, 5], default_value=5)\n",
    "    # beta = CategoricalHyperparameter(\"beta\", [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95], default_value=0.9)\n",
    "    batch_sze = CategoricalHyperparameter(\"batch_size\", [32, 64, 128, 256], default_value=32)\n",
    "    thinning_interval = CategoricalHyperparameter(\"thinning_interval\", [50, 100, 150, 200], default_value=100)\n",
    "    # eta = UniformFloatHyperparameter(\"eta\", lower=0.1, upper=100., log=True)\n",
    "    eta = CategoricalHyperparameter(\"eta\", [0.1, 1., 5, 10., 50., 100.], default_value=1.)\n",
    "    # mu = UniformFloatHyperparameter(\"mu\", lower=0.1, upper=100, log=True)\n",
    "    mu = CategoricalHyperparameter(\"mu\", [0.1, 1., 5, 10., 50., 100.], default_value=1.)\n",
    "    temp = CategoricalHyperparameter(\"temp\", [1e-3, 1e-2, 1e-1, 0.5,  1.], default_value=1e-1)\n",
    "    sigma = CategoricalHyperparameter(\"sigma\", [1e-3, 1e-2, 1e-1, 0.5, 1., 10.], default_value=1.)\n",
    "    # Add hyper-parameters\n",
    "    cs.add_hyperparameters([layer_dim, activation, disc_lr, contin_lr, eta, mu, temp, sigma, batch_sze, thinning_interval])\n",
    "\n",
    "    # # Cyclical SG-MCMC condition\n",
    "    # use_cycle_len = InCondition(child=cycle_len, parent=lr_schedule, values=[\"cyclical\"])\n",
    "    # use_beta = InCondition(child=beta, parent=lr_schedule, values=[\"cyclical\"])\n",
    "    #\n",
    "    # cs.add_conditions([use_cycle_len, use_beta])\n",
    "\n",
    "    return cs\n",
    "\n",
    "def generate_train_cs(seed, X, y):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    # X_tr, X_val, y_tr, y_val = train_test_split(X, y, stratify=y, test_size=0.1, shuffle=seed)\n",
    "    def train_cs(config: Configuration, budget: int)-> float:\n",
    "\n",
    "        params = {\"disc_lr\": config[\"disc_lr\"], \"contin_lr\": config[\"contin_lr\"], \"batch_size\": config[\"batch_size\"],\n",
    "                  \"mu\": config[\"mu\"], \"eta\": config[\"eta\"], \"temp\": config[\"temp\"],\n",
    "                  \"sigma\": config[\"sigma\"], \"thinning_interval\": config[\"thinning_interval\"]}\n",
    "\n",
    "        # print(params)\n",
    "\n",
    "        mixed_sgmcmc = MixedSGMCMC(seed=seed, n_samples=budget, n_warmup=1000, lr_schedule=\"exponential\",\n",
    "                            n_chains=5, layer_dims=[config[\"layer_dim\"]], **params)\n",
    "\n",
    "        cv_score_1 = np.mean(cross_val_score(mixed_sgmcmc, X, y,  cv=cv, fit_params={\n",
    "            \"activation_fns\": [config[\"activation\"]], \"J\": net\n",
    "        }))\n",
    "\n",
    "\n",
    "        # mixed_sgmcmc.fit(X_tr, y_tr, activation_fns=[config[\"activation\"]], J=net)\n",
    "        # gamma_means = jnp.mean(jnp.mean(mixed_sgmcmc.states_.discrete_position, axis=0), axis=0)\n",
    "        # gamma_means_idx_s = np.argsort(gamma_means)[::-1]\n",
    "        #\n",
    "        # bnn_sel_fts = gamma_means_idx_s[:ft_len]\n",
    "        # #\n",
    "        # X_tr_sel, X_val_sel = X_tr[:,bnn_sel_fts], X_val[:,bnn_sel_fts]\n",
    "        #\n",
    "        # _, _, val_score = run_logistic_regression(X_tr_sel, X_val_sel, y_tr, y_val, cv, verbose=0)\n",
    "\n",
    "        return 1 - (cv_score_1)\n",
    "\n",
    "\n",
    "    return train_cs\n",
    "\n",
    "\n",
    "def optimize_hyper_parameters(seed, X, y, total_time=60):\n",
    "    cs = get_configspace(X.shape[1])\n",
    "    scenario = Scenario({\n",
    "        \"run_obj\": \"quality\",\n",
    "        \"wallclock-limit\": total_time,\n",
    "        \"cs\": cs,\n",
    "        \"deterministic\": True,\n",
    "        \"cutoff\": 10,  # runtime limit for the target algorithm\n",
    "        \"verbose_level\": \"DEBUG\", \n",
    "        \"seed\": seed\n",
    "    })\n",
    "\n",
    "    max_steps = 5000\n",
    "\n",
    "    train_cs = generate_train_cs(seed, X, y)\n",
    "\n",
    "    intensifier_kwargs = {\"initial_budget\": 1500, \"max_budget\": max_steps}\n",
    "\n",
    "    smac = SMAC4MF(scenario=scenario, rng=np.random.RandomState(seed),\n",
    "                   tae_runner=train_cs, intensifier_kwargs=intensifier_kwargs)\n",
    "\n",
    "    tae = smac.get_tae_runner()\n",
    "\n",
    "    try:\n",
    "        incumbent = smac.optimize()\n",
    "\n",
    "    finally:\n",
    "        incumbent = smac.solver.incumbent\n",
    "\n",
    "\n",
    "    inc_val = tae.run(config=incumbent, budget=max_steps, seed=seed)\n",
    "\n",
    "    return incumbent, 1 - inc_val[1], smac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 220}\n",
      "/usr/local/lib/python3.9/dist-packages/smac/intensification/parallel_scheduling.py:154: UserWarning: Hyperband is executed with 1 workers only. Consider to use pynisher to use all available workers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7264258551052072"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = jax.device_put(X_train), jax.device_put(X_test), \\\n",
    "                                   jax.device_put(y_train), jax.device_put(y_test)\n",
    "p = X_train.shape[1]\n",
    "rng_key = jax.random.PRNGKey(seed)\n",
    "config, cv_score, smac = optimize_hyper_parameters(seed, X_train, y_train, total_time=180)\n",
    "cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smac.get_trajectory()[-1].budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Configuration(values={\n",
       "  'activation': 'relu',\n",
       "  'batch_size': 64,\n",
       "  'contin_lr': 0.001,\n",
       "  'disc_lr': 1e-05,\n",
       "  'eta': 10.0,\n",
       "  'layer_dim': 50,\n",
       "  'mu': 5,\n",
       "  'sigma': 10.0,\n",
       "  'temp': 1.0,\n",
       "  'thinning_interval': 100,\n",
       "})"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num models: 75\n",
      "0.6897060418578944\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from sgmcmc import MixedSGMCMC\n",
    "params = {\"disc_lr\": config[\"disc_lr\"], \"contin_lr\": config[\"contin_lr\"], \"batch_size\": config[\"batch_size\"],\n",
    "          \"mu\": config[\"mu\"], \"eta\": config[\"eta\"], \"temp\": config[\"temp\"],\n",
    "          \"sigma\":config[\"sigma\"], \"thinning_interval\": config[\"thinning_interval\"]}\n",
    "\n",
    "mixed_sgmcmc = MixedSGMCMC(seed=seed, n_samples=2000, n_warmup=500, n_chains=5, lr_schedule=\"exponential\",\n",
    "                     layer_dims=[config[\"layer_dim\"]], **params)\n",
    "\n",
    "# cv_score_bnn = np.mean(cross_val_score(mixed_sgmcmc, X_train, y_train,  cv=cv, fit_params={\n",
    "#             \"activation_fns\": [config[\"activation\"]], \"J\": net\n",
    "#         }))\n",
    "# print(cv_score_bnn)\n",
    "mixed_sgmcmc.fit(X_train, y_train, activation_fns=[config[\"activation\"]], J=net)\n",
    "print(f\"Num models: {mixed_sgmcmc.states_.discrete_position.shape[0]}\")\n",
    "test_score_bnn = mixed_sgmcmc.score(X_test, y_test)\n",
    "\n",
    "print(test_score_bnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_states = mixed_sgmcmc.states_.discrete_position.reshape(mixed_sgmcmc.n_chains, -1,\n",
    "                                         mixed_sgmcmc.states_.discrete_position.shape[-1])\n",
    "contin_states = jax.tree_util.tree_map(lambda x: jnp.moveaxis(x.reshape(mixed_sgmcmc.n_chains, -1, x.shape[-1]), 0, 1), mixed_sgmcmc.states_.contin_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "contin_states = tree_utils.tree_unstack(mixed_sgmcmc.states_.contin_position)\n",
    "disc_states = mixed_sgmcmc.states_.discrete_position\n",
    "\n",
    "disc_logprior_fn = generate_disc_logprior_fn(net, mixed_sgmcmc.eta, mixed_sgmcmc.mu)\n",
    "contin_logprior_fn = generate_contin_logprior_fn(mixed_sgmcmc.sigma)\n",
    "\n",
    "contin_log_probs = []\n",
    "disc_log_probs = []\n",
    "log_lls = []\n",
    "\n",
    "for c in range(mixed_sgmcmc.n_chains):\n",
    "    chain_disc_log_probs = []\n",
    "    chain_contin_log_probs = []\n",
    "    for i in range(len(mixed_sgmcmc.disc_states[c])):\n",
    "        # idxs = jax.random.choice(keys[i], jnp.arange(data_size), (32,), replace=False)\n",
    "        # batches = make_batch(idxs, train_data.x, train_data.y)\n",
    "        chain_disc_log_probs.append(disc_logprior_fn(mixed_sgmcmc.disc_states[c][i]))\n",
    "        chain_contin_log_probs.append(contin_logprior_fn(mixed_sgmcmc.contin_states[c][i]))\n",
    "\n",
    "    disc_log_probs.append(chain_disc_log_probs)\n",
    "    contin_log_probs.append(chain_contin_log_probs)\n",
    "    # log_lls.append(mixed_loglikelihood_fn(sgmcmc.model_, sgmcmc.contin_states[i], batches, sgmcmc.disc_states[i]))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 12))\n",
    "\n",
    "for c in range(mixed_sgmcmc.n_chains):\n",
    "    ax1.plot(disc_log_probs[c], label=f\"chain {c + 1} Disc logprob\")\n",
    "    ax2.plot(contin_log_probs[c], label=f\"chain {c + 1} Contin logprob\")\n",
    "# ax2[0].plot(log_lls, label=f\"log LL\")\n",
    "\n",
    "ax1.set_title(f\"Disc lr: {config_1['disc_lr']:.3f}\")\n",
    "ax2.set_title(f\"Contin lr: {config_1['contin_lr']:.5f}\")\n",
    "\n",
    "ax1.legend()\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'num features')"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma_means = jnp.mean(mixed_sgmcmc.states_.discrete_position, axis=0)\n",
    "gamma_means_idx_s = np.argsort(gamma_means)[::-1]\n",
    "plt.hist(gamma_means)\n",
    "plt.xlabel(\"frequency\")\n",
    "plt.ylabel(\"num features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.046415888336127774}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>feat_sel</th>\n",
       "      <th>num_feats</th>\n",
       "      <th>cv_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>220</td>\n",
       "      <td>bnn</td>\n",
       "      <td>5</td>\n",
       "      <td>0.503485</td>\n",
       "      <td>0.516591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>220</td>\n",
       "      <td>bnn</td>\n",
       "      <td>10</td>\n",
       "      <td>0.681925</td>\n",
       "      <td>0.677579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220</td>\n",
       "      <td>bnn</td>\n",
       "      <td>15</td>\n",
       "      <td>0.693534</td>\n",
       "      <td>0.665701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed feat_sel  num_feats  cv_score  test_score\n",
       "0   220      bnn          5  0.503485    0.516591\n",
       "1   220      bnn         10  0.681925    0.677579\n",
       "2   220      bnn         15  0.693534    0.665701"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload\n",
    "from tqdm import tqdm\n",
    "from sgmcmc import MixedSGMCMC\n",
    "from run_nn_fisher_test_exp import run_logistic_regression\n",
    "\n",
    "X, y = data_dfs[seed_idx].iloc[:,:-1].to_numpy().astype(float), \\\n",
    "       data_dfs[seed_idx].iloc[:,-1].to_numpy().astype(float)\n",
    "\n",
    "net = net_dfs[seed_idx].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, shuffle=True, stratify=y, test_size=0.3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = jax.device_put(X_train), jax.device_put(X_test), \\\n",
    "                                   jax.device_put(y_train), jax.device_put(y_test)\n",
    "\n",
    "\n",
    "rand_v_bnn_sel_fts_dict = {\"seed\": [], \"feat_sel\": [], \"num_feats\": [], \"cv_score\": [], \"test_score\": []}\n",
    "\n",
    "all_feats = np.arange(p - 1)\n",
    "\n",
    "feat_lens = [5, 10, 15]\n",
    "\n",
    "cv = StratifiedKFold(random_state=seed, shuffle=True, n_splits=5)\n",
    "log_best_params, _, _ = run_logistic_regression(X_train, X_test, y_train, y_test, cv, verbose=0)\n",
    "print(log_best_params)\n",
    "log_clf = LogisticRegression(max_iter=10000, **log_best_params)\n",
    "log_clf.fit(X_train, y_train)\n",
    "\n",
    "log_coef = log_clf.coef_[0]\n",
    "\n",
    "log_coef_sorted = np.argsort(log_coef)[::-1]\n",
    "\n",
    "# # print(gamma_means)\n",
    "#\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, shuffle=True, stratify=y, test_size=0.3)\n",
    "\n",
    "rng_key = jax.random.PRNGKey(seed)\n",
    "\n",
    "for ft_len in feat_lens:\n",
    "    # rand_sel_fts = jax.random.choice(rng_key, all_feats, (ft_len,), replace=False)\n",
    "    #\n",
    "    # X_train_rand_sel, X_test_rand_sel = X_train[:,rand_sel_fts], X_test[:,rand_sel_fts]\n",
    "    #\n",
    "    # _, rand_cv_score, rand_test_score = run_logistic_regression(X_train_rand_sel, X_test_rand_sel,\n",
    "    #                                                            y_train, y_test, cv, verbose=0)\n",
    "    #\n",
    "    # log_sel_fts = log_coef_sorted[:ft_len]\n",
    "    #\n",
    "    # X_train_log_sel, X_test_log_sel, = X_train[:,log_sel_fts], X_test[:,log_sel_fts]\n",
    "    #\n",
    "    # _, log_cv_score, log_test_score = run_logistic_regression(X_train_log_sel, X_test_log_sel,\n",
    "    #                                                          y_train, y_test, cv, verbose=0)\n",
    "\n",
    "    bnn_sel_fts = gamma_means_idx_s[:ft_len]\n",
    "\n",
    "    X_train_sel, X_test_sel = X_train[:,bnn_sel_fts], X_test[:,bnn_sel_fts]\n",
    "\n",
    "    _, bnn_cv_score, bnn_test_score = run_logistic_regression(X_train_sel, X_test_sel,\n",
    "                                                     y_train, y_test, cv, verbose=0)\n",
    "\n",
    "    # rand_v_bnn_sel_fts_dict[\"seed\"].append(seed)\n",
    "    # rand_v_bnn_sel_fts_dict[\"feat_sel\"].append(\"random\")\n",
    "    # rand_v_bnn_sel_fts_dict[\"num_feats\"].append(ft_len)\n",
    "    # rand_v_bnn_sel_fts_dict[\"cv_score\"].append(rand_cv_score)\n",
    "    # rand_v_bnn_sel_fts_dict[\"test_score\"].append(rand_test_score)\n",
    "\n",
    "\n",
    "    rand_v_bnn_sel_fts_dict[\"seed\"].append(seed)\n",
    "    rand_v_bnn_sel_fts_dict[\"feat_sel\"].append(\"bnn\")\n",
    "    rand_v_bnn_sel_fts_dict[\"num_feats\"].append(ft_len)\n",
    "    rand_v_bnn_sel_fts_dict[\"cv_score\"].append(bnn_cv_score)\n",
    "    rand_v_bnn_sel_fts_dict[\"test_score\"].append(bnn_test_score)\n",
    "\n",
    "        # rand_v_bnn_sel_fts_dict[\"seed\"].append(seed)\n",
    "        # rand_v_bnn_sel_fts_dict[\"feat_sel\"].append(\"lr\")\n",
    "        # rand_v_bnn_sel_fts_dict[\"num_feats\"].append(ft_len)\n",
    "        # # rand_v_bnn_sel_fts_dict[\"kernel\"].append(svm_log_params[\"kernel\"])\n",
    "        # rand_v_bnn_sel_fts_dict[\"cv_score\"].append(log_cv_score)\n",
    "        # rand_v_bnn_sel_fts_dict[\"test_score\"].append(log_test_score)\n",
    "\n",
    "\n",
    "rand_v_bnn_sel_fts_df = pd.DataFrame(rand_v_bnn_sel_fts_dict)\n",
    "rand_v_bnn_sel_fts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Variable Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_bnn(model, X, y, params, gammas):\n",
    "    eval_fn = lambda p, g: model.apply(p, X, g).ravel()\n",
    "    logits = jax.vmap(eval_fn)(params, gammas)\n",
    "    logits = logits.reshape(-1, logits.shape[-1])\n",
    "    losses = jax.vmap(optax.sigmoid_binary_cross_entropy, in_axes=(0, None))(logits, y)\n",
    "    mean_loss = jnp.mean(losses, axis=-1)\n",
    "    return jnp.sum(mean_loss)\n",
    "\n",
    "def get_feats_dropout_loss(sgmcmc, X, y):\n",
    "    var_loss_dict = {\"feats_idx\": [], \"num_models\": [] , \"loss_on\": [], \"loss_off\": [], \"loss_diff\": []}\n",
    "    # disc_states = sgmcmc.states_.discrete_position.reshape(-1, sgmcmc.states_.discrete_position.shape[-1])\n",
    "    disc_states = sgmcmc.states_.discrete_position\n",
    "    contin_states = tree_utils.tree_unstack(sgmcmc.states_.contin_position)\n",
    "\n",
    "    num_models = disc_states.shape[0]\n",
    "\n",
    "    p = X.shape[1]\n",
    "\n",
    "    for idx in range(p):\n",
    "        # idx = feats_idx[i]\n",
    "        idx_on = np.argwhere(disc_states[:,idx] == 1.).ravel()\n",
    "        loss_on, loss_off = 0., 0.\n",
    "        if idx_on.size == 0: ## irrelevant feature\n",
    "            loss_diff = 1e9\n",
    "        else:\n",
    "            disc_states_on = disc_states[idx_on]\n",
    "            if idx_on.size > 1:\n",
    "                params_on = tree_utils.tree_stack(itemgetter(*idx_on)(contin_states))\n",
    "            else:\n",
    "                params_on = tree_utils.tree_stack([contin_states[idx_on[0]]])\n",
    "            loss_on = evaluate_bnn(sgmcmc.model_, X, y, params_on, disc_states_on)\n",
    "\n",
    "            # Turn-off the variable, and see hwo the loss changes\n",
    "            disc_states_off = disc_states_on.at[:,idx].set(0)\n",
    "            loss_off = evaluate_bnn(sgmcmc.model_, X, y, params_on, disc_states_off)\n",
    "\n",
    "            loss_diff = (loss_on - loss_off) * (len(idx_on) / num_models)\n",
    "\n",
    "\n",
    "        var_loss_dict[\"feats_idx\"].append(idx)\n",
    "        var_loss_dict[\"num_models\"].append(idx_on.size)\n",
    "        var_loss_dict[\"loss_on\"].append(loss_on)\n",
    "        var_loss_dict[\"loss_off\"].append(loss_off)\n",
    "        var_loss_dict[\"loss_diff\"].append(loss_diff)\n",
    "\n",
    "\n",
    "    var_loss_df = pd.DataFrame(var_loss_dict).sort_values(by=\"loss_diff\")\n",
    "\n",
    "    return var_loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_dir_100 = f\"{data_dir}/exp_data_4/bmm/f100\"\n",
    "dropout_loss_df = pd.read_csv(f\"{save_dir_100}/drop_out_loss_s_{seed}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_loss_df = get_feats_dropout_loss(mixed_sgmcmc, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feats_idx</th>\n",
       "      <th>num_models</th>\n",
       "      <th>loss_on</th>\n",
       "      <th>loss_off</th>\n",
       "      <th>loss_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>100</td>\n",
       "      <td>49.446518</td>\n",
       "      <td>69.27913</td>\n",
       "      <td>-19.832611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>100</td>\n",
       "      <td>49.446518</td>\n",
       "      <td>57.379875</td>\n",
       "      <td>-7.9333572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>98</td>\n",
       "      <td>48.346237</td>\n",
       "      <td>50.37117</td>\n",
       "      <td>-1.9844342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>49.446518</td>\n",
       "      <td>50.738968</td>\n",
       "      <td>-1.29245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>100</td>\n",
       "      <td>49.446518</td>\n",
       "      <td>50.721832</td>\n",
       "      <td>-1.2753143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000000000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    feats_idx  num_models    loss_on   loss_off     loss_diff\n",
       "91         91         100  49.446518   69.27913    -19.832611\n",
       "57         57         100  49.446518  57.379875    -7.9333572\n",
       "92         92          98  48.346237   50.37117    -1.9844342\n",
       "80         80         100  49.446518  50.738968      -1.29245\n",
       "26         26         100  49.446518  50.721832    -1.2753143\n",
       "..        ...         ...        ...        ...           ...\n",
       "38         38           0        0.0        0.0  1000000000.0\n",
       "36         36           0        0.0        0.0  1000000000.0\n",
       "34         34           0        0.0        0.0  1000000000.0\n",
       "44         44           0        0.0        0.0  1000000000.0\n",
       "99         99           0        0.0        0.0  1000000000.0\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7195774231678488\n",
      "0.6639384991227146\n"
     ]
    }
   ],
   "source": [
    "num_fts = 5\n",
    "X_out_val_sel, X_test_sel = X_out_val[:,log_coef_sorted[:num_fts]], X_test[:,log_coef_sorted[:num_fts]]\n",
    "\n",
    "_, val_score, test_score = run_logistic_regression(X_out_val_sel, X_test_sel, y_out_val, y_test, cv)\n",
    "print(val_score)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57, 91, 19,  5, 97])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_coef_sorted[:num_fts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7056997899902273\n",
      "0.6538351085134253\n"
     ]
    }
   ],
   "source": [
    "from run_nn_fisher_test_exp import run_logistic_regression\n",
    "feats_sel = dropout_loss_df[\"feats_idx\"][:num_fts].to_list()\n",
    "\n",
    "X_train_val_comb, X_test_sel = np.concatenate([X_train, X_out_val], axis=0)[:,feats_sel], X_test[:,feats_sel]\n",
    "y_train_val_comb = np.concatenate([y_train, y_out_val], axis=0)\n",
    "\n",
    "_, val_score, test_score = run_logistic_regression(X_train_val_comb, X_test_sel, y_train_val_comb, y_test, cv)\n",
    "print(val_score)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[91, 57, 92, 80, 26]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_sel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### GP Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(315, 5)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ft_len = 20\n",
    "# sel_idx = gamma_means_idx_s[:feats_sel]\n",
    "# sel_idx = log_coef_sorted[:ft_len]\n",
    "X_gp_train, X_gp_val, X_gp_test = X_out_val[:,feats_sel].astype(int), X_train[:,feats_sel].astype(int), X_test[:,feats_sel].astype(int)\n",
    "X_gp_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gplearn.genetic import SymbolicTransformer, SymbolicClassifier\n",
    "from gplearn.functions import make_function\n",
    "import operator\n",
    "\n",
    "\n",
    "def get_best_programs(gp, num_models, ascending=True, sort_fit=\"OOB_fitness\"):\n",
    "    gp_dict = {'Gen': [], \"Ind\": [], \"Fitness\": [], 'OOB_fitness': []}\n",
    "\n",
    "    for idGen in range(len(gp._programs)):\n",
    "        for idPopulation in range(gp.population_size):\n",
    "            if (gp._programs[idGen][idPopulation] != None):\n",
    "                gp_dict[\"Gen\"].append(idGen)\n",
    "                gp_dict[\"Ind\"].append(idPopulation)\n",
    "                gp_dict[\"Fitness\"].append(gp._programs[idGen][idPopulation].fitness_)\n",
    "                gp_dict[\"OOB_fitness\"].append(gp._programs[idGen][idPopulation].oob_fitness_)\n",
    "\n",
    "    gp_df = pd.DataFrame(gp_dict).sort_values(sort_fit, ascending=ascending)[:num_models]\n",
    "    programs = []\n",
    "    for i in range(num_models):\n",
    "        gen, ind = int(gp_df.iloc[i][\"Gen\"]), int(gp_df.iloc[i][\"Ind\"])\n",
    "        programs.append(gp._programs[gen][ind])\n",
    "\n",
    "    return programs\n",
    "\n",
    "\n",
    "def gp_transform(est, X, classifier=False, num_models=100, sort_fit=\"Fitness\"):\n",
    "    if classifier:\n",
    "        programs = get_best_programs(gp, num_models, classifier, sort_fit)\n",
    "        out = np.zeros((X.shape[0], len(programs)))\n",
    "        for i, prog in enumerate(programs):\n",
    "            out[:, i] = prog.execute(X)\n",
    "\n",
    "        return out\n",
    "\n",
    "    return est.transform(X)\n",
    "\n",
    "\n",
    "function_set = ['add', 'sub', 'mul', 'div', 'sqrt', 'log',\n",
    "                'abs', 'neg', 'inv', 'max', 'min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    12.92         0.830495        6         0.509506         0.620272      4.09m\n",
      "   1     7.96         0.603489       14         0.477057         0.795524      3.97m\n",
      "   2     7.89         0.579677        3          0.46723         0.713685      3.85m\n",
      "   3     6.19          0.55804        8         0.458254         0.749588      3.94m\n",
      "   4     4.94         0.548983        3         0.469974         0.702709      3.86m\n",
      "   5     4.26          0.54693        3         0.470216         0.701739      3.82m\n",
      "   6     3.95         0.545346        4         0.449821         0.783321      3.79m\n",
      "   7     3.90          0.54423        5          0.46667         0.768218      3.75m\n",
      "   8     3.60         0.544013        5         0.463704         0.780081      3.51m\n",
      "   9     3.62         0.546156        3          0.46797         0.710727      3.51m\n",
      "  10     3.53         0.544534        5         0.465881         0.771373      3.49m\n",
      "  11     3.51          0.54824        3         0.472152         0.693999      3.54m\n",
      "  12     3.34         0.545566        8         0.449636         0.784062      3.60m\n",
      "  13     3.43         0.552366        3         0.472677         0.691896      3.62m\n",
      "  14     3.54         0.548444        5         0.464034         0.778761      3.48m\n",
      "  15     3.49         0.550217        5         0.474703         0.736084      3.53m\n",
      "  16     3.33         0.538254        3         0.467727         0.711697      3.50m\n",
      "  17     3.29         0.543656        3          0.47117         0.697926      3.53m\n",
      "  18     3.42          0.54597        5         0.470939          0.75114      3.65m\n",
      "  19     3.47         0.562762        5         0.467449         0.765102      3.41m\n",
      "  20     3.44         0.572373        3          0.46544         0.720847      3.45m\n",
      "  21     3.25         0.551055        3         0.457047         0.754418      3.47m\n",
      "  22     3.37         0.555811        3         0.465977         0.718697      3.81m\n",
      "  23     3.32         0.542614        5         0.464297         0.777708      3.75m\n",
      "  24     3.57         0.545108        3         0.470245         0.701625      3.50m\n",
      "  25     3.47         0.549635        5         0.457586         0.804553      3.47m\n",
      "  26     3.35         0.556373        3         0.456307         0.757376      3.49m\n",
      "  27     3.50         0.547029        3         0.462309         0.733371      3.56m\n",
      "  28     3.69          0.56271        5         0.474227         0.737989      3.66m\n",
      "  29     3.56         0.545042        3         0.443149         0.810007      3.57m\n",
      "  30     3.44         0.548511        3         0.472677         0.691896      3.59m\n",
      "  31     3.36         0.546594        3         0.474399         0.685011      3.64m\n",
      "  32     3.31         0.573748        3         0.456804         0.755388      3.62m\n",
      "  33     3.33          0.55942        3         0.467727         0.711697      3.47m\n",
      "  34     3.46         0.542501        3         0.453789         0.767448      3.60m\n",
      "  35     3.42         0.544453        5         0.467167          0.76623      3.58m\n",
      "  36     3.47         0.546046        3         0.460772         0.739515      3.52m\n",
      "  37     3.44         0.542275        3         0.442167         0.813935      3.49m\n",
      "  38     3.29         0.542766        3         0.454044         0.766431      3.38m\n",
      "  39     3.50         0.543516        3         0.448597         0.788219      3.47m\n",
      "  40     3.54         0.542774        5         0.459628         0.796387      3.50m\n",
      "  41     3.40         0.547655        3         0.460744          0.73963      3.58m\n",
      "  42     3.41         0.553746        3         0.463718         0.727732      3.52m\n",
      "  43     3.31          0.54453        3         0.446921         0.794923      3.47m\n",
      "  44     3.41          0.54687        3         0.466959         0.714769      3.30m\n",
      "  45     3.27         0.545974        3         0.464001           0.7266      3.27m\n",
      "  46     3.45         0.550788        5         0.463761         0.779852      3.59m\n",
      "  47     3.55         0.554022        5         0.459423         0.797204      3.60m\n",
      "  48     3.49         0.555098        5         0.471252          0.74989      3.45m\n",
      "  49     3.38         0.542825        3         0.456007         0.758575      3.46m\n",
      "  50     3.27         0.543202        3         0.470459         0.700769      3.39m\n",
      "  51     3.30            0.541        3         0.454529         0.764491      3.36m\n",
      "  52     3.28         0.546392        3         0.464001           0.7266      3.41m\n",
      "  53     3.27         0.545018        3         0.469223         0.705715      3.26m\n",
      "  54     3.37         0.547204        5         0.459181         0.798174      3.38m\n",
      "  55     3.43         0.548153        3         0.452651         0.772002      3.35m\n",
      "  56     3.34         0.545626        3          0.46898         0.706685      3.41m\n",
      "  57     3.41          0.55128        3         0.467201         0.713799      3.42m\n",
      "  58     3.40         0.544357        5         0.453365         0.821438      3.20m\n",
      "  59     3.44         0.549343        3         0.462522         0.732515      3.35m\n",
      "  60     3.40         0.567052        5         0.468838         0.759545      3.36m\n",
      "  61     3.52         0.546332        3         0.460304         0.741388      3.20m\n",
      "  62     3.48          0.54516        5         0.458325         0.801596      3.16m\n",
      "  63     3.53          0.54927        3         0.470759          0.69957      3.35m\n",
      "  64     3.26         0.545828        3         0.458797         0.747418      3.25m\n",
      "  65     3.42         0.552041        3          0.45804         0.750443      3.24m\n",
      "  66     3.31         0.544958        3         0.461483         0.736672      3.45m\n",
      "  67     3.44         0.540647        5         0.465191         0.774133      3.45m\n",
      "  68     3.47         0.550581        5         0.469752         0.755888      3.49m\n",
      "  69     3.28         0.542491        3         0.474399         0.685011      3.33m\n",
      "  70     3.46         0.547403        5         0.470463         0.753045      3.29m\n",
      "  71     3.48         0.551788        5         0.455407         0.813267      3.37m\n",
      "  72     3.52         0.561368        3         0.472649         0.692011      3.24m\n",
      "  73     3.30         0.542384        3         0.471724         0.695709      3.17m\n",
      "  74     3.51         0.545989        3         0.471938         0.694854      3.22m\n",
      "  75     3.40         0.545669        3         0.456861         0.755159      3.32m\n",
      "  76     3.50         0.579181        3         0.457572         0.752316      3.34m\n",
      "  77     3.40         0.571043        3         0.453575         0.768304      3.33m\n",
      "  78     3.30         0.541265        3         0.448613         0.788152      3.26m\n",
      "  79     3.43         0.542167        3         0.472435         0.692866      3.26m\n",
      "  80     3.23         0.545564        3         0.466248         0.717612      3.46m\n",
      "  81     3.30         0.586736        3         0.463759          0.72757      3.32m\n",
      "  82     3.34         0.548542        3         0.470216         0.701739      3.25m\n",
      "  83     3.38         0.545216        3          0.46797         0.710727      3.17m\n",
      "  84     3.30         0.556569        3          0.46329         0.729443      3.07m\n",
      "  85     3.29         0.549015        3         0.464769         0.723528      2.98m\n",
      "  86     3.33           0.5498        3          0.44416         0.805965      3.26m\n",
      "  87     3.29         0.547956        3          0.45979         0.743443      3.25m\n",
      "  88     3.17         0.545593        3         0.468894         0.707028      3.00m\n",
      "  89     3.36         0.546147        5         0.469569         0.756622      3.20m\n",
      "  90     3.45          0.54468        3         0.452339         0.773249      3.32m\n",
      "  91     3.36         0.549557        3         0.472123         0.694113      3.20m\n",
      "  92     3.26         0.587205        5         0.462244          0.78592      3.30m\n",
      "  93     3.54         0.617562        3         0.451143         0.778032      3.24m\n",
      "  94     3.44         0.549426        3         0.469448         0.704812      3.12m\n",
      "  95     3.37         0.578819        5         0.454883         0.815366      3.07m\n",
      "  96     3.40         0.553584        3         0.465723         0.719715      3.09m\n",
      "  97     3.50         0.547031        3         0.462009          0.73457      3.11m\n",
      "  98     3.41         0.553745        3         0.462551         0.732401      3.02m\n",
      "  99     3.37         0.554807        3         0.463516          0.72854      2.91m\n",
      " 100     3.29         0.546239        3         0.467727         0.711697      3.04m\n",
      " 101     3.52         0.545349        3         0.470216         0.701739      2.92m\n",
      " 102     3.39         0.543373        3         0.453107         0.770177      2.92m\n",
      " 103     3.64         0.554387        3         0.455325         0.761304      2.93m\n",
      " 104     3.62          0.55041        3         0.464001           0.7266      3.18m\n",
      " 105     3.32          0.54576        3         0.476403         0.676993      3.26m\n",
      " 106     3.44         0.542971        5         0.462555         0.784676      3.24m\n",
      " 107     3.50         0.561834        5         0.461623         0.788407      3.11m\n",
      " 108     3.44         0.543311        3         0.471441         0.696842      3.05m\n",
      " 109     3.56         0.543483        5         0.466417          0.76923      3.11m\n",
      " 110     3.66         0.547919        5         0.464627         0.776389      2.99m\n",
      " 111     3.54         0.545831        5         0.458587         0.800549      2.86m\n",
      " 112     3.45         0.571708        3         0.458312         0.749358      3.01m\n",
      " 113     3.46          0.55512        3         0.464983         0.722672      2.88m\n",
      " 114     3.28         0.544645        3         0.473203         0.689794      3.02m\n",
      " 115     3.21         0.571967        3          0.46723         0.713685      2.96m\n",
      " 116     3.34         0.542616        3         0.459479          0.74469      3.04m\n",
      " 117     3.52         0.569496        3         0.469933         0.702872      3.04m\n",
      " 118     3.36           0.5414        5         0.466262          0.76985      2.92m\n",
      " 119     3.53         0.553489        3         0.453789         0.767448      2.84m\n",
      " 120     3.34         0.546348        5         0.469392         0.757329      2.84m\n",
      " 121     3.56         0.550637        3         0.472406         0.692981      2.95m\n",
      " 122     3.43         0.544945        3         0.467201         0.713799      2.94m\n",
      " 123     3.45         0.542928        3         0.467444         0.712829      2.85m\n",
      " 124     3.45         0.542447        3         0.471695         0.695824      2.88m\n",
      " 125     3.40          0.54349        3         0.464441         0.724842      3.06m\n",
      " 126     3.46         0.552715        3         0.461726         0.735702      3.02m\n",
      " 127     3.56         0.561296        5         0.451847         0.827509      2.91m\n",
      " 128     3.61         0.546161        7         0.464001           0.7266      2.88m\n",
      " 129     3.51         0.548568        5         0.469945         0.702824      2.95m\n",
      " 130     3.61         0.539412        3         0.463262         0.729558      2.96m\n",
      " 131     3.58         0.593569        5          0.46667         0.768218      2.91m\n",
      " 132     3.43         0.541937        3         0.457544         0.752431      2.85m\n",
      " 133     3.37         0.545588        3         0.464926         0.722901      2.78m\n",
      " 134     3.37         0.546892        3         0.474185         0.685866      2.87m\n",
      " 135     3.34         0.546646        5          0.47446         0.737057      2.88m\n",
      " 136     3.49          0.55038        3          0.46723         0.713685      3.07m\n",
      " 137     3.44         0.557519        3         0.458526         0.748503      3.01m\n",
      " 138     3.44         0.553086        3         0.464013         0.726553      2.74m\n",
      " 139     3.27          0.54578        3           0.4612         0.737804      2.75m\n",
      " 140     3.36         0.544853        3         0.449636         0.784062      2.73m\n",
      " 141     3.42         0.543516        5         0.463295         0.781719      2.75m\n",
      " 142     3.50         0.547672        5         0.464442         0.777129      2.72m\n",
      " 143     3.39         0.549122        5         0.460622         0.792411      2.91m\n",
      " 144     3.43         0.546847        5         0.457722          0.80401      2.89m\n",
      " 145     3.56         0.555158        3         0.464769         0.723528      2.74m\n",
      " 146     3.33         0.551345        3         0.463787         0.727456      2.78m\n",
      " 147     3.47         0.549365        3         0.461055         0.738383      2.79m\n",
      " 148     3.41         0.548329        5          0.47305         0.742699      2.63m\n",
      " 149     3.52         0.546894        5         0.472477         0.744989      2.66m\n",
      " 150     3.55         0.544214        3         0.468709         0.707769      2.80m\n",
      " 151     3.53          0.55245        5          0.46633         0.769576      2.82m\n",
      " 152     3.48         0.544671        5         0.452567         0.824628      2.75m\n",
      " 153     3.40         0.549042        5         0.463052         0.782689      2.82m\n",
      " 154     3.56         0.552217        3         0.471412         0.696956      2.83m\n",
      " 155     3.71         0.543828        3         0.471909         0.694969      2.69m\n",
      " 156     3.38         0.541899        3         0.460744          0.73963      2.66m\n",
      " 157     3.43         0.547848        3         0.466248         0.717612      2.78m\n",
      " 158     3.58         0.570675        5         0.468177         0.762188      2.87m\n",
      " 159     3.29         0.546348        3          0.46154         0.736443      2.88m\n",
      " 160     3.25         0.549318        3         0.470216         0.701739      2.63m\n",
      " 161     3.37         0.556335        3         0.472734         0.691667      2.73m\n",
      " 162     3.43         0.570783        5         0.466971         0.767013      2.61m\n",
      " 163     3.51         0.546676        3         0.468409         0.708969      2.71m\n",
      " 164     3.39         0.548288        3         0.462679         0.731889      2.63m\n",
      " 165     3.43         0.562683        3          0.46622         0.717727      2.69m\n",
      " 166     3.38         0.544908        3         0.455325         0.761304      2.74m\n",
      " 167     3.40         0.547015        3         0.466006         0.718582      2.70m\n",
      " 168     3.29         0.546163        5         0.464423         0.777206      2.62m\n",
      " 169     3.36         0.551833        3         0.471384         0.697071      2.55m\n",
      " 170     3.48         0.540722        3         0.454343         0.765231      2.46m\n",
      " 171     3.56          0.55239        3          0.44736         0.793164      2.52m\n",
      " 172     3.48         0.546644        3          0.47218         0.693884      2.46m\n",
      " 173     3.45         0.545114        3         0.474895         0.683023      2.47m\n",
      " 174     3.50         0.555116        5         0.465035         0.774757      2.48m\n",
      " 175     3.52         0.546473        3         0.471227         0.697697      2.43m\n",
      " 176     3.44         0.545797        3         0.476132         0.678078      2.48m\n",
      " 177     3.32         0.548274        3         0.467941         0.710842      2.46m\n",
      " 178     3.46         0.548687        5         0.463606         0.780475      2.55m\n",
      " 179     3.30         0.545511        3         0.466179         0.717889      2.58m\n",
      " 180     3.33         0.552744        3         0.462494          0.73263      2.50m\n",
      " 181     3.29         0.541983        3         0.471667         0.695939      2.46m\n",
      " 182     3.39          0.54394        3         0.466248         0.717612      2.44m\n",
      " 183     3.50         0.561568        3         0.467756         0.711582      2.43m\n",
      " 184     3.29          0.54502        3         0.473688         0.687854      2.45m\n",
      " 185     3.49         0.543852        3         0.462066         0.734341      2.57m\n",
      " 186     3.46         0.552943        3         0.461754         0.735588      2.62m\n",
      " 187     3.24         0.546777        3         0.465566         0.720341      2.45m\n",
      " 188     3.33         0.583216        3         0.454315         0.765346      2.41m\n",
      " 189     3.38         0.543402        3         0.470216         0.701739      2.42m\n",
      " 190     3.27         0.544077        3         0.466688         0.715854      2.42m\n",
      " 191     3.36         0.546749        5           0.4475         0.844899      2.42m\n",
      " 192     3.39         0.552789        5           0.4585         0.800896      2.45m\n",
      " 193     3.36         0.546424        3         0.470984         0.698667      2.48m\n",
      " 194     3.30         0.553235        3          0.46942         0.704926      2.40m\n",
      " 195     3.43          0.54699        5         0.454716         0.816034      2.35m\n",
      " 196     3.44          0.54586        5         0.460062         0.742358      2.33m\n",
      " 197     3.60         0.560107        6         0.463012         0.782848      2.41m\n",
      " 198     3.90         0.556482        6         0.466135         0.770359      2.47m\n",
      " 199     3.60         0.549151        5           0.4713         0.749699      2.46m\n",
      " 200     3.65         0.545067        5         0.442335         0.865558      2.45m\n",
      " 201     3.62         0.566561        3         0.462494          0.73263      2.40m\n",
      " 202     3.58         0.547652        5         0.467741         0.763934      2.40m\n",
      " 203     3.45         0.552999        3         0.473231          0.68968      2.50m\n",
      " 204     3.47         0.547075        3         0.475692         0.679836      2.35m\n",
      " 205     3.56          0.55863        3         0.466006         0.718582      2.35m\n",
      " 206     3.54         0.551042        3         0.466745         0.715625      2.27m\n",
      " 207     3.37         0.546735        3         0.460247         0.741618      2.23m\n",
      " 208     3.29         0.543316        5         0.460572         0.792608      2.24m\n",
      " 209     3.40         0.557311        5         0.468984          0.75896      2.25m\n",
      " 210     3.29         0.546324        3         0.473474         0.688709      2.35m\n",
      " 211     3.26         0.542847        3         0.471695         0.695824      2.37m\n",
      " 212     3.44         0.553921        5         0.455679         0.812183      2.37m\n",
      " 213     3.40          0.55656        3          0.46548         0.720685      2.23m\n",
      " 214     3.40         0.549162        5         0.464802         0.775689      2.34m\n",
      " 215     3.52         0.547125        5         0.456896         0.807314      2.28m\n",
      " 216     3.40         0.541746        5         0.472757          0.74387      2.24m\n",
      " 217     3.54         0.555149        3         0.465948         0.718812      2.20m\n",
      " 218     3.41         0.541545        3         0.460304         0.741388      2.33m\n",
      " 219     3.33         0.553824        3         0.471667         0.695939      2.21m\n",
      " 220     3.45         0.557207        3         0.470188         0.701854      2.32m\n",
      " 221     3.44         0.555961        3         0.472492         0.692637      2.32m\n",
      " 222     3.49         0.544281        3         0.466491         0.716642      2.25m\n",
      " 223     3.44         0.550255        3         0.471938         0.694854      2.38m\n",
      " 224     3.26         0.542853        3         0.466491         0.716642      2.22m\n",
      " 225     3.31         0.543416        3         0.459762         0.743558      2.30m\n",
      " 226     3.37         0.548717        3         0.471695         0.695824      2.20m\n",
      " 227     3.44         0.549464        3         0.471227         0.697697      2.16m\n",
      " 228     3.36         0.545621        3         0.454557         0.764376      2.30m\n",
      " 229     3.31         0.547552        5         0.467284         0.765763      2.21m\n",
      " 230     3.48         0.557765        3         0.464001           0.7266      2.21m\n",
      " 231     3.41         0.560817        3         0.462494          0.73263      2.21m\n",
      " 232     3.43         0.566261        3         0.458554         0.748388      2.14m\n",
      " 233     3.46         0.545045        3         0.466988         0.714655      2.18m\n",
      " 234     3.54         0.553969        3         0.463759          0.72757      2.12m\n",
      " 235     3.44         0.566905        3         0.464955         0.722787      2.27m\n",
      " 236     3.40          0.54969        3          0.45733         0.753286      2.22m\n",
      " 237     3.54         0.548087        3          0.45979         0.743443      2.17m\n",
      " 238     3.48         0.546339        5         0.465395         0.773316      2.16m\n",
      " 239     3.59         0.545852        5         0.458325         0.801596      2.13m\n",
      " 240     3.38         0.548316        3         0.473942         0.686836      2.12m\n",
      " 241     3.38         0.542208        3         0.466705         0.715787      2.00m\n",
      " 242     3.40         0.544279        3         0.454812         0.763358      2.05m\n",
      " 243     3.37         0.540103        5          0.46987         0.755417      2.11m\n",
      " 244     3.36         0.547209        3          0.46868         0.707884      2.09m\n",
      " 245     3.36         0.545579        3         0.465209         0.721769      2.11m\n",
      " 246     3.32         0.570144        3         0.459548         0.744413      2.00m\n",
      " 247     3.31         0.557211        5           0.4741         0.738498      2.08m\n",
      " 248     3.37         0.549509        5         0.467953         0.763085      2.04m\n",
      " 249     3.31         0.544096        3         0.454529         0.764491      2.10m\n",
      " 250     3.45         0.553279        3         0.461015         0.738545      2.04m\n",
      " 251     3.51          0.56011        3         0.461298         0.737413      2.03m\n",
      " 252     3.26         0.548583        5         0.467459         0.765063      2.09m\n",
      " 253     3.38         0.540956        5         0.469364         0.757443      2.02m\n",
      " 254     3.37         0.541132        3         0.466988         0.714655      2.14m\n",
      " 255     3.36         0.548529        5         0.456312         0.809648      2.08m\n",
      " 256     3.38         0.548085        3         0.468709         0.707769      2.02m\n",
      " 257     3.39         0.541435        3         0.473445         0.688824      2.02m\n",
      " 258     3.29         0.546596        3         0.466462         0.716757      1.95m\n",
      " 259     3.39         0.550562        3         0.474341          0.68524      1.94m\n",
      " 260     3.35         0.552839        3         0.468466         0.708739      1.93m\n",
      " 261     3.41         0.547574        5         0.465717         0.772031      1.89m\n",
      " 262     3.39         0.542025        3         0.449664         0.783948      1.97m\n",
      " 263     3.40         0.542884        5         0.472253         0.745886      1.94m\n",
      " 264     3.57         0.549369        3         0.467998         0.710612      1.98m\n",
      " 265     3.53         0.543902        5         0.454949         0.815103      1.98m\n",
      " 266     3.56         0.543333        3         0.470673         0.699914      1.95m\n",
      " 267     3.34         0.546893        3         0.456279         0.757491      1.95m\n",
      " 268     3.42         0.542514        3         0.465723         0.719715      1.83m\n",
      " 269     3.36         0.545887        3          0.46154         0.736443      1.91m\n",
      " 270     3.35         0.546509        3         0.467727         0.711697      1.99m\n",
      " 271     3.42         0.544433        3         0.455568         0.760334      1.90m\n",
      " 272     3.46         0.555472        5         0.463226         0.781992      1.88m\n",
      " 273     3.38         0.541131        3         0.475081         0.682282      1.87m\n",
      " 274     3.58         0.548909        3         0.466676         0.715901      1.83m\n",
      " 275     3.46         0.549141        3         0.472706         0.691782      1.80m\n",
      " 276     3.31         0.550516        5         0.468947         0.759109      1.74m\n",
      " 277     3.44          0.54892        3         0.461269         0.737528      1.74m\n",
      " 278     3.44         0.567836        3         0.469974         0.702709      1.73m\n",
      " 279     3.39         0.548678        3         0.462522         0.732515      1.82m\n",
      " 280     3.45         0.550484        3         0.458029         0.750491      1.89m\n",
      " 281     3.27         0.546404        3         0.465948         0.718812      1.87m\n",
      " 282     3.47         0.546578        3         0.458768         0.747533      1.84m\n",
      " 283     3.41         0.555227        3         0.475109         0.682168      1.85m\n",
      " 284     3.47         0.544409        3         0.459819         0.743329      1.75m\n",
      " 285     3.38         0.546885        3         0.464527         0.724498      1.70m\n",
      " 286     3.50         0.548335        5         0.457178         0.806185      1.71m\n",
      " 287     3.42         0.548785        3         0.464001           0.7266      1.77m\n",
      " 288     3.39         0.547231        3         0.470159         0.701969      1.77m\n",
      " 289     3.28         0.543284        3         0.456776         0.755503      1.77m\n",
      " 290     3.28         0.549338        3         0.458583         0.748274      1.80m\n",
      " 291     3.40         0.545481        3         0.467201         0.713799      1.74m\n",
      " 292     3.32         0.552478        3         0.472734         0.691667      1.82m\n",
      " 293     3.39         0.543384        3         0.477853         0.671193      1.75m\n",
      " 294     3.33         0.548824        5         0.477271         0.725812      1.70m\n",
      " 295     3.37         0.547867        3         0.464741         0.723642      1.77m\n",
      " 296     3.42         0.559495        3         0.456093         0.758231      1.74m\n",
      " 297     3.33         0.540742        3         0.452836         0.771261      1.70m\n",
      " 298     3.41         0.554921        3         0.468951         0.706799      1.70m\n",
      " 299     3.32         0.552696        3         0.470216         0.701739      1.71m\n",
      " 300     3.40         0.551238        3         0.473659         0.687969      1.75m\n",
      " 301     3.38         0.549838        3          0.46723         0.713685      1.69m\n",
      " 302     3.47         0.560183        3         0.462522         0.732515      1.69m\n",
      " 303     3.33         0.550397        3         0.448839         0.787249      1.64m\n",
      " 304     3.43         0.552587        3         0.456562         0.756358      1.61m\n",
      " 305     3.48         0.545496        3         0.462777         0.731498      1.60m\n",
      " 306     3.52         0.553613        3         0.462465         0.732745      1.62m\n",
      " 307     3.35           0.5742        3         0.472221         0.693722      1.63m\n",
      " 308     3.34         0.547935        3         0.467756         0.711582      1.55m\n",
      " 309     3.19          0.54199        3           0.4737         0.687807      1.64m\n",
      " 310     3.30         0.542644        3         0.467998         0.710612      1.57m\n",
      " 311     3.34          0.55547        3         0.471198         0.697812      1.56m\n",
      " 312     3.48         0.539521        5         0.471911         0.747254      1.58m\n",
      " 313     3.48         0.564551        3         0.455794         0.759431      1.56m\n",
      " 314     3.43         0.548613        3         0.461726         0.735702      1.57m\n",
      " 315     3.44           0.5518        3         0.456065         0.758346      1.52m\n",
      " 316     3.37         0.544717        3         0.473942         0.686836      1.57m\n",
      " 317     3.32         0.556858        3         0.467016          0.71454      1.52m\n",
      " 318     3.32         0.547634        3          0.46548         0.720685      1.47m\n",
      " 319     3.28         0.563766        3         0.459265         0.745545      1.51m\n",
      " 320     3.49         0.551765        5         0.469081         0.758572      1.49m\n",
      " 321     3.47         0.541554        3         0.473659         0.687969      1.52m\n",
      " 322     3.40         0.541175        3         0.473203         0.689794      1.49m\n",
      " 323     3.34         0.545689        5         0.462673         0.784206      1.48m\n",
      " 324     3.54         0.541616        3         0.463718         0.727732      1.46m\n",
      " 325     3.35         0.545411        5         0.467963         0.763047      1.43m\n",
      " 326     3.40          0.54538        5         0.469491         0.756934     10.47m\n",
      " 327     3.62         0.545089        5         0.459522         0.796809      1.44m\n",
      " 328     3.37         0.543214        3         0.468738         0.707655      1.46m\n",
      " 329     3.30         0.541434        3         0.466462         0.716757      1.42m\n",
      " 330     3.55         0.545175        3         0.465723         0.719715      1.38m\n",
      " 331     3.69         0.560229        3         0.463019         0.730528      1.42m\n",
      " 332     3.45         0.565468        3         0.452368         0.773134      1.44m\n",
      " 333     3.44          0.56626        3         0.468495         0.708625      1.38m\n",
      " 334     3.45         0.547872        3         0.463262         0.729558      1.35m\n",
      " 335     3.47         0.551929        5         0.466659          0.76826      1.44m\n",
      " 336     3.51         0.549763        3         0.475167         0.681939      1.43m\n",
      " 337     3.26          0.55128        3         0.473203         0.689794      1.41m\n",
      " 338     3.27         0.547376        3         0.458809         0.747371      1.36m\n",
      " 339     3.37         0.543311        3         0.463205         0.729787      1.33m\n",
      " 340     3.29         0.546717        3         0.470188         0.701854      1.42m\n",
      " 341     3.33         0.565574        3         0.473716         0.687739      1.39m\n",
      " 342     3.33          0.54269        5         0.455582         0.812568      1.40m\n",
      " 343     3.44         0.553668        3         0.469691         0.703842      1.38m\n",
      " 344     3.28          0.55334        3         0.453547         0.768418      1.38m\n",
      " 345     3.34         0.547073        5         0.447782          0.84377      1.28m\n",
      " 346     3.23         0.567046        5         0.467331         0.765572      1.28m\n",
      " 347     3.24         0.546317        3         0.473203         0.689794      1.31m\n",
      " 348     3.38          0.55218        3         0.463019         0.730528      1.25m\n",
      " 349     3.37         0.554599        5         0.464802         0.775689      1.28m\n",
      " 350     3.38          0.54266        3         0.473445         0.688824      1.24m\n",
      " 351     3.30         0.542166        3          0.46548         0.720685      1.22m\n",
      " 352     3.32         0.561301        3         0.464429         0.724889      1.30m\n",
      " 353     3.31         0.568445        3         0.466248         0.717612      1.27m\n",
      " 354     3.27         0.546812        3          0.46504         0.722443      1.22m\n",
      " 355     3.30         0.568278        3         0.471667         0.695939      1.21m\n",
      " 356     3.42         0.545886        3         0.471227         0.697697      1.33m\n",
      " 357     3.41         0.547233        3         0.462708         0.731774      1.22m\n",
      " 358     3.26         0.549155        3         0.456804         0.755388      1.25m\n",
      " 359     3.22         0.541096        3         0.463488         0.728655      1.27m\n",
      " 360     3.29         0.546157        3         0.474399         0.685011      1.22m\n",
      " 361     3.37         0.545649        3         0.468495         0.708625      1.18m\n",
      " 362     3.44         0.543932        5         0.446468         0.849024      1.19m\n",
      " 363     3.34         0.549302        3         0.464983         0.722672      1.17m\n",
      " 364     3.40         0.549275        3         0.467727         0.711697      1.17m\n",
      " 365     3.48         0.546357        3         0.461512         0.736558      1.15m\n",
      " 366     3.44         0.545383        3         0.465537         0.720456      1.15m\n",
      " 367     3.33         0.550218        3         0.453547         0.768418      1.09m\n",
      " 368     3.44         0.551516        3         0.455354         0.761189      1.08m\n",
      " 369     3.46         0.546141        3         0.461257         0.737575      1.13m\n",
      " 370     3.40         0.548289        3         0.470944         0.698829      1.15m\n",
      " 371     3.43          0.54722        3         0.464458         0.724774      1.18m\n",
      " 372     3.35         0.553538        3         0.469391         0.705041      1.14m\n",
      " 373     3.53         0.551173        3         0.462251           0.7336      1.07m\n",
      " 374     3.38         0.542604        3         0.465723         0.719715      1.06m\n",
      " 375     3.37         0.572413        3         0.457971          0.75072      1.04m\n",
      " 376     3.42         0.547343        3         0.473445         0.688824      1.03m\n",
      " 377     3.44         0.545002        5         0.469141         0.758333      1.03m\n",
      " 378     3.42          0.56358        5         0.473478         0.740985      1.01m\n",
      " 379     3.49         0.566387        3         0.463262         0.729558      1.02m\n",
      " 380     3.34         0.543775        3         0.473417         0.688939      1.01m\n",
      " 381     3.38         0.545892        3         0.466248         0.717612     59.70s\n",
      " 382     3.16         0.543158        3         0.474953         0.682794     58.22s\n",
      " 383     3.30         0.550365        3          0.46972         0.703727      1.00m\n",
      " 384     3.29         0.546328        3         0.474624         0.684108      1.01m\n",
      " 385     3.30         0.554493        3          0.46868         0.707884      1.00m\n",
      " 386     3.21         0.539862        3         0.469477         0.704697     58.93s\n",
      " 387     3.27         0.546616        5          0.45065         0.832295     59.82s\n",
      " 388     3.44         0.542678        3         0.459108         0.746172     57.12s\n",
      " 389     3.43         0.565726        3         0.466433         0.716872     54.88s\n",
      " 390     3.52         0.546458        5          0.46598         0.770979     58.10s\n",
      " 391     3.54         0.542588        5         0.466601         0.768492     59.37s\n",
      " 392     3.52         0.548217        5         0.459512         0.796851     54.53s\n",
      " 393     3.52         0.544239        5         0.465591         0.772534     53.16s\n",
      " 394     3.43         0.567508        5         0.472077         0.746589     56.72s\n",
      " 395     3.39         0.542668        3          0.45659         0.756244     54.85s\n",
      " 396     3.41         0.548186        3         0.447389          0.79305     55.55s\n",
      " 397     3.27         0.543934        3         0.465226         0.721702     54.28s\n",
      " 398     3.35         0.540292        3         0.462494          0.73263     54.31s\n",
      " 399     3.41         0.546347        3         0.478153         0.669993     52.84s\n",
      " 400     3.38         0.543608        3         0.466519         0.716528     50.11s\n",
      " 401     3.31         0.542921        3         0.473388         0.689053     49.47s\n",
      " 402     3.40          0.55354        3          0.46548         0.720685     49.20s\n",
      " 403     3.31         0.568014        5         0.454687         0.816149     51.95s\n",
      " 404     3.49         0.570262        5          0.46842         0.761218     51.10s\n",
      " 405     3.26         0.544482        3         0.467727         0.711697     50.45s\n",
      " 406     3.31         0.541448        3         0.469477         0.704697     48.10s\n",
      " 407     3.40         0.547424        5         0.469491         0.756934     48.87s\n",
      " 408     3.55         0.546208        3          0.46228         0.733485     51.02s\n",
      " 409     3.37         0.540369        3         0.470701         0.699799     49.06s\n",
      " 410     3.37         0.556327        3         0.468524          0.70851     44.59s\n",
      " 411     3.39         0.553672        3         0.468894         0.707028     46.53s\n",
      " 412     3.46         0.544612        3         0.466462         0.716757     46.61s\n",
      " 413     3.36         0.547495        3          0.45979         0.743443     44.69s\n",
      " 414     3.40         0.556303        3         0.465226         0.721702     46.11s\n",
      " 415     3.39         0.549177        5         0.457254         0.805879     43.68s\n",
      " 416     3.38         0.542878        5         0.471921         0.747212     42.98s\n",
      " 417     3.63         0.557299        5         0.479071         0.718615     45.71s\n",
      " 418     3.68         0.564014        3         0.464741         0.723642     44.58s\n",
      " 419     3.57         0.543777        6         0.468956         0.711755     41.64s\n",
      " 420     3.58         0.539759        5         0.467381         0.765375     40.10s\n",
      " 421     3.53         0.550152        6         0.468016         0.715518     39.90s\n",
      " 422     3.51         0.548645        3          0.46898         0.706685     40.61s\n",
      " 423     3.52          0.56178        3         0.459479          0.74469     38.44s\n",
      " 424     3.48         0.549972        8          0.45985         0.781455     39.96s\n",
      " 425     3.44         0.540484        3         0.473659         0.687969     41.03s\n",
      " 426     3.28         0.543895        3         0.458283         0.749473     39.90s\n",
      " 427     3.33         0.551871        3         0.453818         0.767334     38.96s\n",
      " 428     3.34         0.542065        3         0.466988         0.714655     37.06s\n",
      " 429     3.39         0.564819        3         0.472152         0.693999     36.80s\n",
      " 430     3.37         0.546221        3         0.471667         0.695939     36.85s\n",
      " 431     3.46         0.543974        3          0.47218         0.693884     35.59s\n",
      " 432     3.40         0.541921        3         0.471498         0.696612     35.85s\n",
      " 433     3.34         0.546982        3          0.47437         0.685125     33.85s\n",
      " 434     3.18         0.552401        3          0.47147         0.696727     34.53s\n",
      " 435     3.35         0.543858        5         0.463198         0.782107     34.35s\n",
      " 436     3.33         0.553004        3         0.475877         0.679096     36.03s\n",
      " 437     3.24         0.547542        5         0.463129          0.78238     32.98s\n",
      " 438     3.33         0.545628        3         0.471384         0.697071     31.63s\n",
      " 439     3.38         0.542628        3         0.449381          0.78508     32.07s\n",
      " 440     3.31         0.551218        5          0.46882         0.759618     31.54s\n",
      " 441     3.41         0.545606        3         0.472435         0.692866     31.92s\n",
      " 442     3.45         0.552762        3         0.473174         0.689909     30.88s\n",
      " 443     3.32         0.545831        5         0.464541         0.776735     29.47s\n",
      " 444     3.49         0.544996        3         0.473134         0.690071     28.35s\n",
      " 445     3.43         0.545427        3         0.467756         0.711582     28.68s\n",
      " 446     3.46         0.553026        3         0.469448         0.704812     28.00s\n",
      " 447     3.32         0.558289        5         0.464094         0.778522     28.26s\n",
      " 448     3.47         0.571261        7         0.452068         0.774334     27.35s\n",
      " 449     3.44         0.540035        5         0.468984          0.75896     27.23s\n",
      " 450     3.34         0.546754        3         0.467045         0.714426     25.89s\n",
      " 451     3.54         0.546372        5         0.470833         0.751566     26.50s\n",
      " 452     3.58         0.544693        3          0.45659         0.756244     24.77s\n",
      " 453     3.55         0.574058        5         0.465076         0.774594     24.50s\n",
      " 454     3.53         0.544426        5         0.465376         0.773393     25.07s\n",
      " 455     3.43         0.545235        3         0.463262         0.729558     24.45s\n",
      " 456     3.31         0.543032        5         0.442092         0.866528     23.28s\n",
      " 457     3.37         0.542254        3         0.461754         0.735588     22.46s\n",
      " 458     3.35         0.546584        3         0.464712         0.723757     21.53s\n",
      " 459     3.17          0.54553        3         0.468466         0.708739     20.81s\n",
      " 460     3.29         0.545114        3         0.468241         0.709642     20.93s\n",
      " 461     3.32         0.552339        3         0.470003         0.702595     21.25s\n",
      " 462     3.37         0.555956        3         0.456307         0.757376     20.22s\n",
      " 463     3.56         0.545477        3         0.470927         0.698896     18.98s\n",
      " 464     3.32         0.541295        5         0.458829         0.799579     17.75s\n",
      " 465     3.24         0.548824        5         0.468694         0.760121     18.10s\n",
      " 466     3.35         0.546615        3         0.466619         0.716131     18.46s\n",
      " 467     3.51          0.55153        5         0.441071         0.870615     16.46s\n",
      " 468     3.51         0.545036        3         0.468027         0.710498     15.87s\n",
      " 469     3.34         0.555122        3         0.462991         0.730642     15.55s\n",
      " 470     3.36         0.545341        5         0.470803         0.751687     14.86s\n",
      " 471     3.35          0.54639        3         0.470459         0.700769     14.74s\n",
      " 472     3.31         0.547023        3         0.460033         0.742473     14.24s\n",
      " 473     3.41         0.544844        3         0.458526         0.748503     14.35s\n",
      " 474     3.34          0.54112        3         0.467941         0.710842     13.69s\n",
      " 475     3.35         0.548078        3         0.459577         0.744299     12.74s\n",
      " 476     3.38         0.556548        3         0.467259          0.71357     12.28s\n",
      " 477     3.41         0.548304        3         0.468466         0.708739     11.52s\n",
      " 478     3.45         0.545042        3         0.469391         0.705041     11.17s\n",
      " 479     3.40         0.542642        3          0.47363         0.688083     10.61s\n",
      " 480     3.36         0.542421        3         0.450646          0.78002     10.12s\n",
      " 481     3.43         0.556644        3         0.461783         0.735473      9.69s\n",
      " 482     3.37         0.541303        5          0.47128         0.749775      8.89s\n",
      " 483     3.50         0.552304        5         0.457177         0.806191      8.76s\n",
      " 484     3.42          0.54772        5         0.471017         0.750828      8.38s\n",
      " 485     3.45         0.567777        5         0.459425         0.797197      7.36s\n",
      " 486     3.50         0.543209        3         0.475849          0.67921      6.73s\n",
      " 487     3.40         0.563946        5         0.466836         0.767553      6.20s\n",
      " 488     3.46         0.544124        3         0.465751           0.7196      6.08s\n",
      " 489     3.33         0.541068        3          0.46053         0.740485      5.33s\n",
      " 490     3.29         0.541229        3         0.464244          0.72563      4.72s\n",
      " 491     3.39         0.556839        5         0.460883         0.791365      4.53s\n",
      " 492     3.33         0.547506        3         0.467484         0.712667      3.91s\n",
      " 493     3.61         0.558653        3         0.466277         0.717498      3.34s\n",
      " 494     3.36         0.550062        5         0.471931         0.747174      2.68s\n",
      " 495     3.37         0.543334        5         0.463226         0.781992      2.20s\n",
      " 496     3.47         0.549221        7         0.452254         0.792372      1.67s\n",
      " 497     3.58         0.545486        5         0.462848         0.783506      1.08s\n",
      " 498     3.57          0.54345        3         0.471724         0.695709      0.53s\n",
      " 499     3.57         0.547881        5         0.459979          0.79498      0.00s\n"
     ]
    }
   ],
   "source": [
    "def and_(x1, x2):\n",
    "    return np.logical_and(x1, x2)\n",
    "\n",
    "def or_(x1, x2):\n",
    "    return np.logical_or(x1, x2)\n",
    "\n",
    "def xor_(x1, x2):\n",
    "    return np.logical_xor(x1, x2)\n",
    "\n",
    "def not_(x1):\n",
    "    return np.logical_not(x1)\n",
    "\n",
    "# def if_else_then(x1, x2, x3):\n",
    "#     if x1: return x2\n",
    "#     return x3\n",
    "\n",
    "# and_op = make_function(function=and_, name=\"and_\", arity=2)\n",
    "# or_op = make_function(function=or_, name=\"or_\", arity=2)\n",
    "# xor_op = make_function(function=xor_, name=\"xor_\", arity=2)\n",
    "# not_op = make_function(function=not_, name=\"not_\", arity=1)\n",
    "# if_else_op = make_function(function=if_else_then, name=\"if_else_\", arity=3)\n",
    "\n",
    "# function_set = [and_op, or_op, xor_op, not_op]\n",
    "\n",
    "# gp = SymbolicTransformer(generations=500, population_size=1000, hall_of_fame=100, n_components=10,\n",
    "#                          function_set=function_set, parsimony_coefficient=0.003,\n",
    "#                          random_state=seed, max_samples=0.8, verbose=1,\n",
    "#                          p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.01,\n",
    "#                          p_point_mutation=0.1, p_point_replace=0.05, metric=\"spearman\")\n",
    "\n",
    "gp = SymbolicClassifier(generations=500, population_size=1000,\n",
    "                         function_set=function_set, parsimony_coefficient=0.001,\n",
    "                         random_state=seed, max_samples=0.8, verbose=1,\n",
    "                         p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.01,\n",
    "                         p_point_mutation=0.1, p_point_replace=0.05)\n",
    "\n",
    "gp.fit(X_gp_train, y_out_val)\n",
    "\n",
    "gp_features_val = gp_transform(gp, X_gp_val, classifier=True, sort_fit=\"Fitness\")\n",
    "gp_features_test = gp_transform(gp, X_gp_test, classifier=True, sort_fit=\"Fitness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: program Pages: 1 -->\n<svg width=\"134pt\" height=\"116pt\"\n viewBox=\"0.00 0.00 134.00 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n<title>program</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-112 130,-112 130,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<ellipse fill=\"#136ed4\" stroke=\"black\" cx=\"63\" cy=\"-90\" rx=\"27.9\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"63\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">add</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<ellipse fill=\"#60a6f6\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X0</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M54.65,-72.76C50.29,-64.28 44.85,-53.71 39.96,-44.2\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"42.99,-42.44 35.3,-35.15 36.77,-45.64 42.99,-42.44\"/>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<ellipse fill=\"#60a6f6\" stroke=\"black\" cx=\"99\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"99\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X1</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M71.35,-72.76C75.71,-64.28 81.15,-53.71 86.04,-44.2\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"89.23,-45.64 90.7,-35.15 83.01,-42.44 89.23,-45.64\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.sources.Source at 0x7ff0edaa4250>"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "dot_data = gp._programs[-1][0].export_graphviz()\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param: {'C': 0.021544346900318832}, cv_score: 0.7172214878019469, test_score: 0.692303693586838\n"
     ]
    }
   ],
   "source": [
    "from run_nn_fisher_test_exp import run_logistic_regression\n",
    " \n",
    "X_val_comb, X_test_comb = np.concatenate([X_gp_val, gp_features_val], axis=1), np.concatenate([X_gp_test, gp_features_test], axis=1)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)\n",
    "\n",
    "log_best_param_1, log_cv_score_1, log_test_score_1 = run_logistic_regression(X_val_comb, X_test_comb, y_train,\n",
    "                                                                             y_test, cv)\n",
    "print(f\"Param: {log_best_param_1}, cv_score: {log_cv_score_1}, test_score: {log_test_score_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gen</th>\n",
       "      <th>Ind</th>\n",
       "      <th>Fitness</th>\n",
       "      <th>OOB_fitness</th>\n",
       "      <th>Equation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>37</td>\n",
       "      <td>79</td>\n",
       "      <td>0.445167</td>\n",
       "      <td>0.813935</td>\n",
       "      <td>add(X0, X1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36637</th>\n",
       "      <td>467</td>\n",
       "      <td>648</td>\n",
       "      <td>0.446071</td>\n",
       "      <td>0.870615</td>\n",
       "      <td>add(add(X0, X1), X1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>29</td>\n",
       "      <td>355</td>\n",
       "      <td>0.446149</td>\n",
       "      <td>0.810007</td>\n",
       "      <td>add(X0, X1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6758</th>\n",
       "      <td>86</td>\n",
       "      <td>705</td>\n",
       "      <td>0.447160</td>\n",
       "      <td>0.805965</td>\n",
       "      <td>add(X0, X1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15537</th>\n",
       "      <td>200</td>\n",
       "      <td>822</td>\n",
       "      <td>0.447335</td>\n",
       "      <td>0.865558</td>\n",
       "      <td>add(add(X0, X1), X1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Gen  Ind   Fitness  OOB_fitness              Equation\n",
       "2900    37   79  0.445167     0.813935           add(X0, X1)\n",
       "36637  467  648  0.446071     0.870615  add(add(X0, X1), X1)\n",
       "2273    29  355  0.446149     0.810007           add(X0, X1)\n",
       "6758    86  705  0.447160     0.805965           add(X0, X1)\n",
       "15537  200  822  0.447335     0.865558  add(add(X0, X1), X1)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_df_sorted = gp_df.sort_values('Fitness', ascending=True)[:5]\n",
    "gp_df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: program Pages: 1 -->\n<svg width=\"134pt\" height=\"116pt\"\n viewBox=\"0.00 0.00 134.00 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n<title>program</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-112 130,-112 130,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<ellipse fill=\"#136ed4\" stroke=\"black\" cx=\"63\" cy=\"-90\" rx=\"27.9\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"63\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">add</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<ellipse fill=\"#60a6f6\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X0</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M54.65,-72.76C50.29,-64.28 44.85,-53.71 39.96,-44.2\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"42.99,-42.44 35.3,-35.15 36.77,-45.64 42.99,-42.44\"/>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<ellipse fill=\"#60a6f6\" stroke=\"black\" cx=\"99\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"99\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X1</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M71.35,-72.76C75.71,-64.28 81.15,-53.71 86.04,-44.2\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"89.23,-45.64 90.7,-35.15 83.01,-42.44 89.23,-45.64\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.sources.Source at 0x7ff0f6747460>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_row = gp_df_sorted.iloc[2]\n",
    "program = gp._programs[int(top_row[\"Gen\"])][int(top_row[\"Ind\"])]\n",
    "dot_data = program.export_graphviz()\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param: {'C': 0.1}, cv_score: 0.7144093258021872, test_score: 0.6929302588089644\n"
     ]
    }
   ],
   "source": [
    "from run_nn_fisher_test_exp import run_logistic_regression\n",
    "\n",
    "gp_features_val = np.zeros((X_gp_val.shape[0], gp_df_sorted.shape[0]))\n",
    "gp_features_test = np.zeros((X_gp_test.shape[0], gp_df_sorted.shape[0]))\n",
    "\n",
    "for i in range(gp_df_sorted.shape[0]):\n",
    "    row = gp_df_sorted.iloc[i]\n",
    "    gen, ind = row[\"Gen\"], row[\"Ind\"]\n",
    "    prog = gp._programs[gen][ind]\n",
    "    val_out = prog.execute(X_gp_val)\n",
    "    test_out = prog.execute(X_gp_test)\n",
    "\n",
    "    gp_features_val[:,i] = val_out\n",
    "    gp_features_test[:,i] = test_out\n",
    "\n",
    "X_val_comb, X_test_comb = np.concatenate([X_gp_val, gp_features_val], axis=1), np.concatenate([X_gp_test, gp_features_test], axis=1)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)\n",
    "\n",
    "log_best_param_1, log_cv_score_1, log_test_score_1 = run_logistic_regression(X_val_comb, X_test_comb, y_train,\n",
    "                                                                            y_test, cv)\n",
    "print(f\"Param: {log_best_param_1}, cv_score: {log_cv_score_1}, test_score: {log_test_score_1}\")\n",
    "# clf_1 = LogisticRegression(max_iter=10000, **log_best_param_1)\n",
    "# clf_1.fit(X_train_comb, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 10)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_features_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Multiple Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BNN Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running - seed 422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 422}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'tanh',\n",
      "  'batch_size': 32,\n",
      "  'contin_lr': 1e-05,\n",
      "  'disc_lr': 0.1,\n",
      "  'eta': 100.0,\n",
      "  'layer_dim': 50,\n",
      "  'mu': 0.1,\n",
      "  'sigma': 0.001,\n",
      "  'temp': 0.01,\n",
      "  'thinning_interval': 200,\n",
      "})\n",
      "\n",
      "Done - seed 422\n",
      "LR cv_score: 0.7526809392222926, LR test_score: 0.7613559994776149\n",
      "BNN cv_score: 0.654884294602813, BNN test_score: 0.6084469373643162\n",
      "Running - seed 261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 261}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'relu',\n",
      "  'batch_size': 256,\n",
      "  'contin_lr': 0.001,\n",
      "  'disc_lr': 0.1,\n",
      "  'eta': 5,\n",
      "  'layer_dim': 60,\n",
      "  'mu': 0.1,\n",
      "  'sigma': 0.1,\n",
      "  'temp': 1.0,\n",
      "  'thinning_interval': 50,\n",
      "})\n",
      "\n",
      "Done - seed 261\n",
      "LR cv_score: 0.7433300345227697, LR test_score: 0.731235310215496\n",
      "BNN cv_score: 0.7420122018349963, BNN test_score: 0.6935546155488364\n",
      "Running - seed 968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 968}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'relu',\n",
      "  'batch_size': 128,\n",
      "  'contin_lr': 1e-05,\n",
      "  'disc_lr': 0.0001,\n",
      "  'eta': 50.0,\n",
      "  'layer_dim': 60,\n",
      "  'mu': 1.0,\n",
      "  'sigma': 10.0,\n",
      "  'temp': 0.5,\n",
      "  'thinning_interval': 200,\n",
      "})\n",
      "\n",
      "Done - seed 968\n",
      "LR cv_score: 0.6231194870292615, LR test_score: 0.6331168581382143\n",
      "BNN cv_score: 0.5805510280431017, BNN test_score: 0.551836072792582\n",
      "Running - seed 282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 282}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'relu',\n",
      "  'batch_size': 128,\n",
      "  'contin_lr': 1e-05,\n",
      "  'disc_lr': 0.0001,\n",
      "  'eta': 1.0,\n",
      "  'layer_dim': 60,\n",
      "  'mu': 5,\n",
      "  'sigma': 0.1,\n",
      "  'temp': 0.001,\n",
      "  'thinning_interval': 50,\n",
      "})\n",
      "\n",
      "Done - seed 282\n",
      "LR cv_score: 0.7471289274106175, LR test_score: 0.6055045370647242\n",
      "BNN cv_score: 0.6020570137026444, BNN test_score: 0.6607142390627171\n",
      "Running - seed 739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 739}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'relu',\n",
      "  'batch_size': 128,\n",
      "  'contin_lr': 0.0001,\n",
      "  'disc_lr': 0.01,\n",
      "  'eta': 1.0,\n",
      "  'layer_dim': 60,\n",
      "  'mu': 100.0,\n",
      "  'sigma': 0.1,\n",
      "  'temp': 0.5,\n",
      "  'thinning_interval': 150,\n",
      "})\n",
      "\n",
      "Done - seed 739\n",
      "LR cv_score: 0.7632244008714598, LR test_score: 0.7416620269634149\n",
      "BNN cv_score: 0.7112418447571254, BNN test_score: 0.7111668557917398\n",
      "Running - seed 573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 573}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'tanh',\n",
      "  'batch_size': 64,\n",
      "  'contin_lr': 0.001,\n",
      "  'disc_lr': 0.01,\n",
      "  'eta': 50.0,\n",
      "  'layer_dim': 60,\n",
      "  'mu': 100.0,\n",
      "  'sigma': 0.1,\n",
      "  'temp': 0.5,\n",
      "  'thinning_interval': 150,\n",
      "})\n",
      "\n",
      "Done - seed 573\n",
      "LR cv_score: 0.7532685584669169, LR test_score: 0.7599139543242561\n",
      "BNN cv_score: 0.6862271855029914, BNN test_score: 0.6583894418550407\n",
      "Running - seed 220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 220}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'relu',\n",
      "  'batch_size': 64,\n",
      "  'contin_lr': 0.001,\n",
      "  'disc_lr': 1e-05,\n",
      "  'eta': 10.0,\n",
      "  'layer_dim': 50,\n",
      "  'mu': 5,\n",
      "  'sigma': 10.0,\n",
      "  'temp': 1.0,\n",
      "  'thinning_interval': 200,\n",
      "})\n",
      "\n",
      "Done - seed 220\n",
      "LR cv_score: 0.7141653843826707, LR test_score: 0.6964807962228895\n",
      "BNN cv_score: 0.731101220624186, BNN test_score: 0.6823569452260736\n",
      "Running - seed 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 413}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'relu',\n",
      "  'batch_size': 32,\n",
      "  'contin_lr': 0.0001,\n",
      "  'disc_lr': 0.5,\n",
      "  'eta': 100.0,\n",
      "  'layer_dim': 70,\n",
      "  'mu': 5,\n",
      "  'sigma': 1.0,\n",
      "  'temp': 0.5,\n",
      "  'thinning_interval': 200,\n",
      "})\n",
      "\n",
      "Done - seed 413\n",
      "LR cv_score: 0.8182599003409388, LR test_score: 0.7740667702713354\n",
      "BNN cv_score: 0.7717952151839372, BNN test_score: 0.6843033891435664\n",
      "Running - seed 745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 745}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'tanh',\n",
      "  'batch_size': 64,\n",
      "  'contin_lr': 0.001,\n",
      "  'disc_lr': 0.1,\n",
      "  'eta': 100.0,\n",
      "  'layer_dim': 50,\n",
      "  'mu': 0.1,\n",
      "  'sigma': 0.1,\n",
      "  'temp': 1.0,\n",
      "  'thinning_interval': 50,\n",
      "})\n",
      "\n",
      "Done - seed 745\n",
      "LR cv_score: 0.7897502153316107, LR test_score: 0.7955350483887943\n",
      "BNN cv_score: 0.7729543396347972, BNN test_score: 0.7088837254920194\n",
      "Running - seed 775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 775}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'tanh',\n",
      "  'batch_size': 64,\n",
      "  'contin_lr': 0.001,\n",
      "  'disc_lr': 0.001,\n",
      "  'eta': 1.0,\n",
      "  'layer_dim': 70,\n",
      "  'mu': 1.0,\n",
      "  'sigma': 1.0,\n",
      "  'temp': 1.0,\n",
      "  'thinning_interval': 50,\n",
      "})\n",
      "\n",
      "Done - seed 775\n",
      "LR cv_score: 0.8565464895635675, LR test_score: 0.8316599105387472\n",
      "BNN cv_score: 0.82333967219098, BNN test_score: 0.7970850097306962\n",
      "Running - seed 482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 482}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'relu',\n",
      "  'batch_size': 256,\n",
      "  'contin_lr': 0.0001,\n",
      "  'disc_lr': 0.001,\n",
      "  'eta': 0.1,\n",
      "  'layer_dim': 50,\n",
      "  'mu': 10.0,\n",
      "  'sigma': 1.0,\n",
      "  'temp': 0.1,\n",
      "  'thinning_interval': 150,\n",
      "})\n",
      "\n",
      "Done - seed 482\n",
      "LR cv_score: 0.6728028189693143, LR test_score: 0.7894170664874678\n",
      "BNN cv_score: 0.6488033846294388, BNN test_score: 0.5846048476981918\n",
      "Running - seed 442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 442}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'tanh',\n",
      "  'batch_size': 128,\n",
      "  'contin_lr': 0.001,\n",
      "  'disc_lr': 0.0001,\n",
      "  'eta': 100.0,\n",
      "  'layer_dim': 60,\n",
      "  'mu': 5,\n",
      "  'sigma': 1.0,\n",
      "  'temp': 0.5,\n",
      "  'thinning_interval': 50,\n",
      "})\n",
      "\n",
      "Done - seed 442\n",
      "LR cv_score: 0.7262328722644329, LR test_score: 0.7734746795337828\n",
      "BNN cv_score: 0.6597835855730092, BNN test_score: 0.6779973913933917\n",
      "Running - seed 210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 210}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'tanh',\n",
      "  'batch_size': 256,\n",
      "  'contin_lr': 0.001,\n",
      "  'disc_lr': 0.01,\n",
      "  'eta': 5,\n",
      "  'layer_dim': 60,\n",
      "  'mu': 100.0,\n",
      "  'sigma': 0.1,\n",
      "  'temp': 0.5,\n",
      "  'thinning_interval': 200,\n",
      "})\n",
      "\n",
      "Done - seed 210\n",
      "LR cv_score: 0.7197629793961648, LR test_score: 0.7499665224290342\n",
      "BNN cv_score: 0.5895525196299453, BNN test_score: 0.60488371063054\n",
      "Running - seed 423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 423}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'tanh',\n",
      "  'batch_size': 32,\n",
      "  'contin_lr': 0.001,\n",
      "  'disc_lr': 1e-05,\n",
      "  'eta': 5,\n",
      "  'layer_dim': 70,\n",
      "  'mu': 5,\n",
      "  'sigma': 0.1,\n",
      "  'temp': 0.5,\n",
      "  'thinning_interval': 200,\n",
      "})\n",
      "\n",
      "Done - seed 423\n",
      "LR cv_score: 0.7061500092614644, LR test_score: 0.7146685302917628\n",
      "BNN cv_score: 0.6109080930132599, BNN test_score: 0.6425576158268984\n",
      "Running - seed 760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 760}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'tanh',\n",
      "  'batch_size': 256,\n",
      "  'contin_lr': 0.0001,\n",
      "  'disc_lr': 0.1,\n",
      "  'eta': 10.0,\n",
      "  'layer_dim': 50,\n",
      "  'mu': 1.0,\n",
      "  'sigma': 0.1,\n",
      "  'temp': 1.0,\n",
      "  'thinning_interval': 150,\n",
      "})\n",
      "\n",
      "Done - seed 760\n",
      "LR cv_score: 0.7353473373871384, LR test_score: 0.7261113993530636\n",
      "BNN cv_score: 0.6782626905910922, BNN test_score: 0.7258559002768962\n",
      "Running - seed 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 57}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'tanh',\n",
      "  'batch_size': 128,\n",
      "  'contin_lr': 0.0001,\n",
      "  'disc_lr': 0.5,\n",
      "  'eta': 0.1,\n",
      "  'layer_dim': 50,\n",
      "  'mu': 1.0,\n",
      "  'sigma': 0.1,\n",
      "  'temp': 0.001,\n",
      "  'thinning_interval': 100,\n",
      "})\n",
      "\n",
      "Done - seed 57\n",
      "LR cv_score: 0.585403579871665, LR test_score: 0.5901819901277701\n",
      "BNN cv_score: 0.5248921659866045, BNN test_score: 0.4968860949622664\n",
      "Running - seed 769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 769}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'relu',\n",
      "  'batch_size': 256,\n",
      "  'contin_lr': 0.0001,\n",
      "  'disc_lr': 1e-05,\n",
      "  'eta': 0.1,\n",
      "  'layer_dim': 50,\n",
      "  'mu': 5,\n",
      "  'sigma': 0.01,\n",
      "  'temp': 0.5,\n",
      "  'thinning_interval': 150,\n",
      "})\n",
      "\n",
      "Done - seed 769\n",
      "LR cv_score: 0.7148747799771484, LR test_score: 0.7785897110201249\n",
      "BNN cv_score: 0.621629698197817, BNN test_score: 0.6738378354271659\n",
      "Running - seed 920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 920}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'relu',\n",
      "  'batch_size': 32,\n",
      "  'contin_lr': 0.0001,\n",
      "  'disc_lr': 0.001,\n",
      "  'eta': 50.0,\n",
      "  'layer_dim': 50,\n",
      "  'mu': 10.0,\n",
      "  'sigma': 1.0,\n",
      "  'temp': 1.0,\n",
      "  'thinning_interval': 100,\n",
      "})\n",
      "\n",
      "Done - seed 920\n",
      "LR cv_score: 0.7871427052827948, LR test_score: 0.7797077553200249\n",
      "BNN cv_score: 0.7166067723685469, BNN test_score: 0.6462986015301047\n",
      "Running - seed 226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 226}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'relu',\n",
      "  'batch_size': 256,\n",
      "  'contin_lr': 0.001,\n",
      "  'disc_lr': 0.1,\n",
      "  'eta': 100.0,\n",
      "  'layer_dim': 50,\n",
      "  'mu': 5,\n",
      "  'sigma': 0.01,\n",
      "  'temp': 0.5,\n",
      "  'thinning_interval': 50,\n",
      "})\n",
      "\n",
      "Done - seed 226\n",
      "LR cv_score: 0.661052871052871, LR test_score: 0.6615110578543569\n",
      "BNN cv_score: 0.5281517694720195, BNN test_score: 0.4911105669259691\n",
      "Running - seed 196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 196}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'activation': 'relu',\n",
      "  'batch_size': 128,\n",
      "  'contin_lr': 0.01,\n",
      "  'disc_lr': 1e-05,\n",
      "  'eta': 0.1,\n",
      "  'layer_dim': 80,\n",
      "  'mu': 50.0,\n",
      "  'sigma': 0.1,\n",
      "  'temp': 0.1,\n",
      "  'thinning_interval': 50,\n",
      "})\n",
      "\n",
      "Done - seed 196\n",
      "LR cv_score: 0.7542425416505232, LR test_score: 0.7379998380276886\n",
      "BNN cv_score: 0.7350639235337624, BNN test_score: 0.7017308357671825\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from nn_util import prepare_data\n",
    "from sgmcmc import MixedSGMCMC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "save_dir_100 = f\"{data_dir}/exp_data_4/bmm/f100\"\n",
    "\n",
    "bnn_lr_dict_100 = {\"seed\": [], \"classifier\": [], \"cv_score\": [], \"test_score\": []}\n",
    "\n",
    "log_param_grid = {\"C\":np.logspace(-2, 1, 10)}\n",
    "\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    seed, net, data = prepare_data(seeds, i, data_dfs, net_dfs, test_size=0.3, out_val_size=0.5)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    print(f\"Running - seed {seed}\")\n",
    "\n",
    "    X_train, _, X_test, y_train, _, y_test = data\n",
    "\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = jax.device_put(X_train), jax.device_put(X_test), \\\n",
    "                                       jax.device_put(y_train), jax.device_put(y_test)\n",
    "\n",
    "\n",
    "    log_grid_cv = GridSearchCV(estimator=LogisticRegression(max_iter=10000), param_grid=log_param_grid, verbose=0, scoring=\"roc_auc\", cv=cv).fit(X_train, y_train)\n",
    "    log_cv_score = log_grid_cv.best_score_\n",
    "    clf = LogisticRegression(max_iter=10000, C=log_grid_cv.best_params_[\"C\"])\n",
    "    clf.fit(X_train, y_train)\n",
    "    log_test_score = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])\n",
    "\n",
    "    config, bnn_cv_score, _ = optimize_hyper_parameters(seed, X_train, y_train, total_time=180)\n",
    "    # config = pickle.load(open(f\"{save_dir_100}/bnn_params_s_{seed}.pickle\", \"rb\"))\n",
    "\n",
    "    params = {\"disc_lr\": config[\"disc_lr\"], \"contin_lr\": config[\"contin_lr\"], \"batch_size\": config[\"batch_size\"],\n",
    "              \"mu\": config[\"mu\"], \"eta\": config[\"eta\"], \"temp\": config[\"temp\"],\n",
    "              \"sigma\": config[\"sigma\"], \"thinning_interval\": config[\"thinning_interval\"]}\n",
    "\n",
    "    mixed_sgmcmc = MixedSGMCMC(seed=seed, n_samples=5000, n_warmup=1000, n_chains=5, lr_schedule=\"exponential\",\n",
    "                     layer_dims=[config[\"layer_dim\"]] ,**params)\n",
    "\n",
    "    # bnn_cv_score = np.mean(cross_val_score(mixed_sgmcmc, X_train, y_train, cv=cv,\n",
    "    #                                        fit_params={\"activation_fns\": [config[\"activation\"]],  \"J\": net}))\n",
    "\n",
    "    mixed_sgmcmc.fit(X_train, y_train, activation_fns=[config[\"activation\"]], J=net)\n",
    "\n",
    "    pickle.dump(config, open(f\"{save_dir_100}/bnn_params_s_{seed}.pickle\", \"wb\"))\n",
    "\n",
    "    bnn_test_score = mixed_sgmcmc.score(X_test, y_test)\n",
    "\n",
    "    bnn_disc_mean = jnp.mean(mixed_sgmcmc.states_.discrete_position.reshape(-1, X_train.shape[1]), axis=0)\n",
    "    np.save(f\"{save_dir_100}/bnn_disc_mean_s_{seed}.npy\", bnn_disc_mean)\n",
    "\n",
    "\n",
    "    bnn_lr_dict_100[\"seed\"].append(seed)\n",
    "    bnn_lr_dict_100[\"classifier\"].append(\"LR\")\n",
    "    bnn_lr_dict_100[\"cv_score\"].append(log_cv_score)\n",
    "    bnn_lr_dict_100[\"test_score\"].append(log_test_score)\n",
    "\n",
    "    bnn_lr_dict_100[\"seed\"].append(seed)\n",
    "    bnn_lr_dict_100[\"classifier\"].append(\"BNN\")\n",
    "    bnn_lr_dict_100[\"cv_score\"].append(bnn_cv_score)\n",
    "    bnn_lr_dict_100[\"test_score\"].append(bnn_test_score)\n",
    "\n",
    "    print(f\"Config: {config}\")\n",
    "\n",
    "    print(f\"Done - seed {seed}\\nLR cv_score: {log_cv_score}, LR test_score: {log_test_score}\")\n",
    "    print(f\"BNN cv_score: {bnn_cv_score}, BNN test_score: {bnn_test_score}\")\n",
    "\n",
    "bnn_lr_df_100 = pd.DataFrame(bnn_lr_dict_100)\n",
    "bnn_lr_df_100.to_csv(f\"{save_dir_100}/res_bmm_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# bnn_lr_df.to_csv(f\"{save_dir}/res_bmm_summary.csv\", index=False)\n",
    "bnn_lr_df_100 = pd.read_csv(f\"{data_dir}/exp_data_4/bmm/f100/res_bmm_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BNN</th>\n",
       "      <td>0.669491</td>\n",
       "      <td>0.650125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.731224</td>\n",
       "      <td>0.731608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cv_score  test_score\n",
       "classifier                      \n",
       "BNN         0.669491    0.650125\n",
       "LR          0.731224    0.731608"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_lr_df_100.groupby([\"classifier\"])[[\"cv_score\", \"test_score\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Variable Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [16:19<00:00, 48.97s/it]\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from tqdm import tqdm\n",
    "from sgmcmc import MixedSGMCMC\n",
    "from run_nn_fisher_test_exp import run_logistic_regression\n",
    "import pickle\n",
    "save_dir_100 = f\"{data_dir}/exp_data_4/bmm/f100\"\n",
    "bnn_sel_fts_dict = {\"seed\": [], \"feat_sel\": [], \"num_feats\": [], \"cv_score\": [], \"test_score\": []}\n",
    "\n",
    "all_feats = np.arange(p - 1)\n",
    "\n",
    "feat_lens = [5, 10]\n",
    "\n",
    "for i in tqdm(range(len(seeds))):\n",
    "\n",
    "    seed, net, data = prepare_data(seeds, i, data_dfs, net_dfs, test_size=0.3)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    X_train, X_out_val, X_test, y_train, y_out_val, y_test = data\n",
    "\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = jax.device_put(X_train), jax.device_put(X_test), \\\n",
    "                                       jax.device_put(y_train), jax.device_put(y_test)\n",
    "\n",
    "    X_train_val_comb, y_train_val_comb = np.concatenate([X_train, X_out_val], axis=0), np.concatenate([y_train, y_out_val], axis=0)\n",
    "\n",
    "    log_best_params, _, _ = run_logistic_regression(X_train, X_test, y_train, y_test, cv, verbose=0)\n",
    "    log_clf = LogisticRegression(max_iter=10000, **log_best_params)\n",
    "    log_clf.fit(X_train, y_train)\n",
    "\n",
    "    log_coef = log_clf.coef_[0]\n",
    "\n",
    "    log_coef_sorted = np.argsort(log_coef)[::-1]\n",
    "\n",
    "\n",
    "    config = pickle.load(open(f\"{save_dir_100}/bnn_params_s_{seed}.pickle\", \"rb\"))\n",
    "    params = {\"disc_lr\": config[\"disc_lr\"], \"contin_lr\": config[\"contin_lr\"], \"batch_size\": config[\"batch_size\"],\n",
    "              \"mu\": config[\"mu\"], \"eta\": config[\"eta\"], \"temp\": config[\"temp\"],\n",
    "              \"sigma\": config[\"sigma\"], \"thinning_interval\": config[\"thinning_interval\"]}\n",
    "\n",
    "    mixed_sgmcmc = MixedSGMCMC(seed=seed, n_samples=5000, n_warmup=1000, n_chains=5, lr_schedule=\"exponential\",\n",
    "                     layer_dims=[config[\"layer_dim\"]] ,**params)\n",
    "\n",
    "    # bnn_cv_score = np.mean(cross_val_score(mixed_sgmcmc, X_train, y_train, cv=cv,\n",
    "    #                                        fit_params={\"activation_fns\": [config[\"activation\"]],  \"J\": net}))\n",
    "\n",
    "    mixed_sgmcmc.fit(X_train, y_train, activation_fns=[config[\"activation\"]], J=net)\n",
    "\n",
    "    gamma_means = jnp.mean(mixed_sgmcmc.states_.discrete_position.reshape(-1, X_train.shape[1]), axis=0)\n",
    "    gamma_means_idx_s = np.argsort(gamma_means)[::-1]\n",
    "\n",
    "    dropout_loss_df = get_feats_dropout_loss(mixed_sgmcmc, X_train, y_train)\n",
    "    dropout_loss_df.to_csv(f\"{save_dir_100}/drop_out_loss_s_{seed}.csv\", index=False)\n",
    "\n",
    "    for ft_len in feat_lens:\n",
    "\n",
    "        log_sel_fts = log_coef_sorted[:ft_len]\n",
    "\n",
    "        X_train_log_sel, X_test_log_sel, = X_train[:,log_sel_fts], X_test[:,log_sel_fts]\n",
    "\n",
    "        _, log_cv_score, log_test_score = run_logistic_regression(X_train_log_sel, X_test_log_sel,\n",
    "                                                                 y_train, y_test, cv, verbose=0)\n",
    "\n",
    "        bnn_mean_sel = gamma_means_idx_s[:ft_len]\n",
    "\n",
    "        X_train_sel_1, X_test_sel_1 = X_train_val_comb[:,bnn_mean_sel], X_test[:,bnn_mean_sel]\n",
    "\n",
    "        _, bnn_cv_score_1, bnn_test_score_1 = run_logistic_regression(X_train_sel_1, X_test_sel_1,\n",
    "                                                                  y_train_val_comb, y_test, cv, verbose=0)\n",
    "\n",
    "\n",
    "        bnn_dropout_sel = dropout_loss_df[\"feats_idx\"][:ft_len].to_list()\n",
    "\n",
    "        X_train_sel_2, X_test_sel_2 = X_train_val_comb[:,bnn_dropout_sel], X_test[:,bnn_dropout_sel]\n",
    "\n",
    "        _, bnn_cv_score_2, bnn_test_score_2 = run_logistic_regression(X_train_sel_2, X_test_sel_2,\n",
    "                                                                      y_train_val_comb, y_test, cv, verbose=0)\n",
    "\n",
    "\n",
    "        bnn_sel_fts_dict[\"seed\"].append(seed)\n",
    "        bnn_sel_fts_dict[\"feat_sel\"].append(\"bnn_mean\")\n",
    "        bnn_sel_fts_dict[\"num_feats\"].append(ft_len)\n",
    "        bnn_sel_fts_dict[\"cv_score\"].append(bnn_cv_score_1)\n",
    "        bnn_sel_fts_dict[\"test_score\"].append(bnn_test_score_1)\n",
    "\n",
    "        bnn_sel_fts_dict[\"seed\"].append(seed)\n",
    "        bnn_sel_fts_dict[\"feat_sel\"].append(\"bnn_dropout\")\n",
    "        bnn_sel_fts_dict[\"num_feats\"].append(ft_len)\n",
    "        bnn_sel_fts_dict[\"cv_score\"].append(bnn_cv_score_2)\n",
    "        bnn_sel_fts_dict[\"test_score\"].append(bnn_test_score_2)\n",
    "\n",
    "        bnn_sel_fts_dict[\"seed\"].append(seed)\n",
    "        bnn_sel_fts_dict[\"feat_sel\"].append(\"lr\")\n",
    "        bnn_sel_fts_dict[\"num_feats\"].append(ft_len)\n",
    "        bnn_sel_fts_dict[\"cv_score\"].append(log_cv_score)\n",
    "        bnn_sel_fts_dict[\"test_score\"].append(log_test_score)\n",
    "\n",
    "\n",
    "bnn_sel_fts_df = pd.DataFrame(bnn_sel_fts_dict)\n",
    "bnn_sel_fts_df.to_csv(f\"{save_dir_100}/bnn_sel_feats_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bnn_sel_fts_df = pd.read_csv(f\"{save_dir_100}/bnn_sel_feats_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>cv_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_sel</th>\n",
       "      <th>num_feats</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bnn_dropout</th>\n",
       "      <th>5</th>\n",
       "      <td>0.699383</td>\n",
       "      <td>0.660715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.716268</td>\n",
       "      <td>0.691781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bnn_mean</th>\n",
       "      <th>5</th>\n",
       "      <td>0.554003</td>\n",
       "      <td>0.515844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.590296</td>\n",
       "      <td>0.540238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">lr</th>\n",
       "      <th>5</th>\n",
       "      <td>0.739940</td>\n",
       "      <td>0.705963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.778950</td>\n",
       "      <td>0.746043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       cv_score  test_score\n",
       "feat_sel    num_feats                      \n",
       "bnn_dropout 5          0.699383    0.660715\n",
       "            10         0.716268    0.691781\n",
       "bnn_mean    5          0.554003    0.515844\n",
       "            10         0.590296    0.540238\n",
       "lr          5          0.739940    0.705963\n",
       "            10         0.778950    0.746043"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_sel_fts_df.groupby([\"feat_sel\", \"num_feats\"])[[\"cv_score\", \"test_score\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnn_mean_5_cv_fts = bnn_sel_fts_df[(bnn_sel_fts_df[\"feat_sel\"] == \"bnn_mean\") & (bnn_sel_fts_df[\"num_feats\"] == 5)][\"cv_score\"].to_numpy()\n",
    "bnn_dropout_5_cv_fts = bnn_sel_fts_df[(bnn_sel_fts_df[\"feat_sel\"] == \"bnn_dropout\") & (bnn_sel_fts_df[\"num_feats\"] == 5)][\"cv_score\"].to_numpy()\n",
    "\n",
    "bnn_mean_5_test_fts = bnn_sel_fts_df[(bnn_sel_fts_df[\"feat_sel\"] == \"bnn_mean\") & (bnn_sel_fts_df[\"num_feats\"] == 5)][\"test_score\"].to_numpy()\n",
    "bnn_dropout_5_test_fts = bnn_sel_fts_df[(bnn_sel_fts_df[\"feat_sel\"] == \"bnn_dropout\") & (bnn_sel_fts_df[\"num_feats\"] == 5)][\"test_score\"].to_numpy()\n",
    "\n",
    "labels = [\"mean freq\", \"ens dropout\"]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "bplot1 = ax1.boxplot([bnn_mean_5_cv_fts, bnn_dropout_5_cv_fts], showmeans=True, patch_artist=True, labels=labels)\n",
    "bplot2 = ax2.boxplot([bnn_mean_5_test_fts, bnn_dropout_5_test_fts], showmeans=True, patch_artist=True, labels=labels)\n",
    "\n",
    "# fill with colors\n",
    "colors = ['lightblue', 'lightgreen']\n",
    "for bplot in (bplot1, bplot2):\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "\n",
    "fig.suptitle(\"Feature selection - using 5/100 features for prediction\")\n",
    "plt.legend([bplot1['medians'][0], bplot1['means'][0]], ['median', 'mean'])\n",
    "plt.legend([bplot2['medians'][0], bplot2['means'][0]], ['median', 'mean'])\n",
    "\n",
    "ax1.set_ylabel(\"AUC\")\n",
    "ax1.set_title(\"CV Scores\")\n",
    "\n",
    "ax2.set_ylabel(\"AUC\")\n",
    "ax2.set_title(\"Test Scores\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnn_mean_10_cv_fts = bnn_sel_fts_df[(bnn_sel_fts_df[\"feat_sel\"] == \"bnn_mean\") & (bnn_sel_fts_df[\"num_feats\"] == 10)][\"cv_score\"].to_numpy()\n",
    "bnn_dropout_10_cv_fts = bnn_sel_fts_df[(bnn_sel_fts_df[\"feat_sel\"] == \"bnn_dropout\") & (bnn_sel_fts_df[\"num_feats\"] == 10)][\"cv_score\"].to_numpy()\n",
    "\n",
    "bnn_mean_10_test_fts = bnn_sel_fts_df[(bnn_sel_fts_df[\"feat_sel\"] == \"bnn_mean\") & (bnn_sel_fts_df[\"num_feats\"] == 10)][\"test_score\"].to_numpy()\n",
    "bnn_dropout_10_test_fts = bnn_sel_fts_df[(bnn_sel_fts_df[\"feat_sel\"] == \"bnn_dropout\") & (bnn_sel_fts_df[\"num_feats\"] == 10)][\"test_score\"].to_numpy()\n",
    "\n",
    "labels = [\"mean freq\", \"ens dropout\"]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.grid(color='grey', axis='y', linestyle='-', linewidth=0.25, alpha=0.5)\n",
    "ax2.grid(color='grey', axis='y', linestyle='-', linewidth=0.25, alpha=0.5)\n",
    "\n",
    "bplot1 = ax1.boxplot([bnn_mean_10_cv_fts, bnn_dropout_10_cv_fts], showmeans=True, patch_artist=True, labels=labels)\n",
    "bplot2 = ax2.boxplot([bnn_mean_10_test_fts, bnn_dropout_10_test_fts], showmeans=True, patch_artist=True, labels=labels)\n",
    "\n",
    "# fill with colors\n",
    "colors = ['lightblue', 'lightgreen']\n",
    "for bplot in (bplot1, bplot2):\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "fig.suptitle(\"Feature selection - using 10/100 features for prediction\")\n",
    "plt.legend([bplot1['medians'][0], bplot1['means'][0]], ['median', 'mean'])\n",
    "\n",
    "ax1.set_ylabel(\"AUC\")\n",
    "ax1.set_title(\"CV Scores\")\n",
    "\n",
    "ax2.set_ylabel(\"AUC\")\n",
    "ax2.set_title(\"Test Scores\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>feat_sel</th>\n",
       "      <th>num_feats</th>\n",
       "      <th>cv_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>422</td>\n",
       "      <td>bnn_mean</td>\n",
       "      <td>5</td>\n",
       "      <td>0.527062</td>\n",
       "      <td>0.444663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>422</td>\n",
       "      <td>bnn_dropout</td>\n",
       "      <td>5</td>\n",
       "      <td>0.706824</td>\n",
       "      <td>0.583225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>422</td>\n",
       "      <td>lr</td>\n",
       "      <td>5</td>\n",
       "      <td>0.765435</td>\n",
       "      <td>0.778139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>422</td>\n",
       "      <td>bnn_mean</td>\n",
       "      <td>10</td>\n",
       "      <td>0.531692</td>\n",
       "      <td>0.498160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>422</td>\n",
       "      <td>bnn_dropout</td>\n",
       "      <td>10</td>\n",
       "      <td>0.705108</td>\n",
       "      <td>0.617021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>196</td>\n",
       "      <td>bnn_dropout</td>\n",
       "      <td>5</td>\n",
       "      <td>0.742079</td>\n",
       "      <td>0.628625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>196</td>\n",
       "      <td>lr</td>\n",
       "      <td>5</td>\n",
       "      <td>0.764026</td>\n",
       "      <td>0.723366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>196</td>\n",
       "      <td>bnn_mean</td>\n",
       "      <td>10</td>\n",
       "      <td>0.728344</td>\n",
       "      <td>0.657789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>196</td>\n",
       "      <td>bnn_dropout</td>\n",
       "      <td>10</td>\n",
       "      <td>0.742056</td>\n",
       "      <td>0.694357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>196</td>\n",
       "      <td>lr</td>\n",
       "      <td>10</td>\n",
       "      <td>0.787679</td>\n",
       "      <td>0.750702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     seed     feat_sel  num_feats  cv_score  test_score\n",
       "0     422     bnn_mean          5  0.527062    0.444663\n",
       "1     422  bnn_dropout          5  0.706824    0.583225\n",
       "2     422           lr          5  0.765435    0.778139\n",
       "3     422     bnn_mean         10  0.531692    0.498160\n",
       "4     422  bnn_dropout         10  0.705108    0.617021\n",
       "..    ...          ...        ...       ...         ...\n",
       "115   196  bnn_dropout          5  0.742079    0.628625\n",
       "116   196           lr          5  0.764026    0.723366\n",
       "117   196     bnn_mean         10  0.728344    0.657789\n",
       "118   196  bnn_dropout         10  0.742056    0.694357\n",
       "119   196           lr         10  0.787679    0.750702\n",
       "\n",
       "[120 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_sel_fts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_100 = f\"{data_dir}/exp_data_4/bmm/f100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [2:55:09<00:00, 525.48s/it]  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/2_bnn_bmm_sgmcmc.ipynb Cell 58\u001b[0m in \u001b[0;36m<cell line: 101>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/2_bnn_bmm_sgmcmc.ipynb#Y120sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m         bnn_gp_dict[\u001b[39m\"\u001b[39m\u001b[39mcv_score\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(best_cv_score_oob_fit)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/2_bnn_bmm_sgmcmc.ipynb#Y120sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m         bnn_gp_dict[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(best_test_score_oob_fit)\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/2_bnn_bmm_sgmcmc.ipynb#Y120sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m bnn_gp_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(bnn_gp_dict)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/2_bnn_bmm_sgmcmc.ipynb#Y120sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m bnn_gp_df\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00msave_dir_100\u001b[39m}\u001b[39;00m\u001b[39m/bnn_lr_gp.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    630\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    631\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    632\u001b[0m     )\n\u001b[1;32m    634\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    635\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 636\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    637\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    638\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/internals/construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    494\u001b[0m     arrays \u001b[39m=\u001b[39m [\n\u001b[1;32m    495\u001b[0m         x\n\u001b[1;32m    496\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x\u001b[39m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m    497\u001b[0m         \u001b[39melse\u001b[39;00m x\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    498\u001b[0m         \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays\n\u001b[1;32m    499\u001b[0m     ]\n\u001b[1;32m    500\u001b[0m     \u001b[39m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/internals/construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    118\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    121\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/internals/construction.py:674\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    672\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[1;32m    673\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 674\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    676\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[1;32m    677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    678\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    679\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "from gplearn.genetic import SymbolicTransformer, SymbolicClassifier\n",
    "from gplearn.functions import make_function\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "from run_nn_fisher_test_exp import run_logistic_regression\n",
    "\n",
    "function_set = ['add', 'sub', 'mul', 'div', 'sqrt', 'log',\n",
    "                'abs', 'neg', 'inv', 'max', 'min']\n",
    "\n",
    "bnn_gp_dict = {\"seed\": [], \"orig_num_feats\": [], \"gp_num_feats\": [] ,\"classifier\": [], \"cv_score\": [], \"test_score\": []}\n",
    "\n",
    "feat_lens = [5, 10]\n",
    "\n",
    "num_models = [5, 10, 20, 50]\n",
    "\n",
    "for i in tqdm(range(len(seeds))):\n",
    "\n",
    "    seed, net, data = prepare_data(seeds, i, data_dfs, net_dfs, test_size=0.3)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    X_train, X_out_val, X_test, y_train, y_out_val, y_test = data\n",
    "   \n",
    "    X_train_val_comb, y_train_val_comb = np.concatenate([X_train, X_out_val], axis=0), np.concatenate([y_train, y_out_val], axis=0)\n",
    "    dropout_loss_df = pd.read_csv(f\"{save_dir_100}/drop_out_loss_s_{seed}.csv\")\n",
    "\n",
    "    for ft_len in feat_lens:\n",
    "\n",
    "        bnn_dropout_sel = dropout_loss_df[\"feats_idx\"][:ft_len].to_list()\n",
    "        X_train_sel_1, X_test_sel_1 = X_train_val_comb[:,bnn_dropout_sel], X_test[:,bnn_dropout_sel]\n",
    "\n",
    "        _, bnn_cv_score_1, bnn_test_score_1 = run_logistic_regression(X_train_sel_1, X_test_sel_1,\n",
    "                                                                      y_train_val_comb, y_test, cv, verbose=0)\n",
    "\n",
    "        X_gp_train_sel, X_gp_val_sel, X_gp_test_sel = X_out_val[:,bnn_dropout_sel], X_train[:,bnn_dropout_sel] ,X_test[:,bnn_dropout_sel]\n",
    "\n",
    "        gp = SymbolicClassifier(generations=500, population_size=1000,\n",
    "                         function_set=function_set, parsimony_coefficient=0.001,\n",
    "                         random_state=seed, max_samples=0.8, verbose=0,\n",
    "                         p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.01,\n",
    "                         p_point_mutation=0.1, p_point_replace=0.05)\n",
    "\n",
    "        # gp = SymbolicTransformer(generations=100, population_size=1000, hall_of_fame=100, n_components=20,\n",
    "        #                          function_set=function_set, parsimony_coefficient=0.001,\n",
    "        #                          random_state=seed, max_samples=0.8, verbose=0,\n",
    "        #                          p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.01,\n",
    "        #                          p_point_mutation=0.1, p_point_replace=0.05, metric=\"spearman\")\n",
    "\n",
    "        gp.fit(X_gp_train_sel, y_out_val)\n",
    "\n",
    "        gp_features_val_fit = gp_transform(gp, X_gp_val_sel, classifier=True, num_models=50, sort_fit=\"Fitness\")\n",
    "        gp_features_test_fit = gp_transform(gp, X_gp_test_sel, classifier=True, num_models=50, sort_fit=\"Fitness\")\n",
    "\n",
    "        gp_features_val_oob_fit = gp_transform(gp, X_gp_val_sel, classifier=True, num_models=50, sort_fit=\"OOB_fitness\")\n",
    "        gp_features_test_oob_fit = gp_transform(gp, X_gp_test_sel, classifier=True, num_models=50, sort_fit=\"OOB_fitness\")\n",
    "\n",
    "        best_cv_score_fit, best_test_score_fit = 0.0, 0.0\n",
    "        best_cv_score_oob_fit, best_test_score_oob_fit = 0.0, 0.0\n",
    "        best_n_models_fit, best_n_models_oob_fit = 0, 0\n",
    "        for j in num_models:\n",
    "            X_val_comb_fit, X_test_comb_fit = np.concatenate([X_gp_val_sel, gp_features_val_fit[:,:j]], axis=1), np.concatenate([X_gp_test_sel, gp_features_test_fit[:,:j]], axis=1)\n",
    "\n",
    "            _, log_bnn_gp_cv_fit, log_bnn_gp_test_fit = run_logistic_regression(X_val_comb_fit, X_test_comb_fit, y_train,\n",
    "                                                                                        y_test, cv)\n",
    "\n",
    "            if log_bnn_gp_cv_fit > best_cv_score_fit:\n",
    "                best_cv_score_fit = log_bnn_gp_cv_fit\n",
    "                best_test_score_fit = log_bnn_gp_test_fit\n",
    "                best_n_models_fit = j\n",
    "\n",
    "            X_val_comb_oob_fit, X_test_comb_oob_fit = np.concatenate([X_gp_val_sel, gp_features_val_oob_fit[:,:j]], axis=1), np.concatenate([X_gp_test_sel, gp_features_test_oob_fit[:,:j]], axis=1)\n",
    "\n",
    "            _, log_bnn_gp_cv_oob_fit, log_bnn_gp_test_oob_fit = run_logistic_regression(X_val_comb_oob_fit, X_test_comb_oob_fit, y_train,\n",
    "                                                                                        y_test, cv)\n",
    "            \n",
    "            if log_bnn_gp_cv_oob_fit > best_cv_score_oob_fit:\n",
    "                best_cv_score_oob_fit = log_bnn_gp_cv_oob_fit\n",
    "                best_test_score_oob_fit = log_bnn_gp_test_oob_fit\n",
    "                best_n_models_oob_fit = j\n",
    "\n",
    "        bnn_gp_dict[\"seed\"].append(seed)\n",
    "        bnn_gp_dict[\"classifier\"].append(\"BNN + LR\")\n",
    "        bnn_gp_dict[\"orig_num_feats\"].append(ft_len)\n",
    "        # bnn_gp_dict[\"gp_num_feats\"].append(0)\n",
    "        bnn_gp_dict[\"cv_score\"].append(bnn_cv_score_1)\n",
    "        bnn_gp_dict[\"test_score\"].append(bnn_test_score_1)\n",
    "\n",
    "        bnn_gp_dict[\"seed\"].append(seed)\n",
    "        bnn_gp_dict[\"classifier\"].append(\"BNN + LR + GP1\")\n",
    "        bnn_gp_dict[\"orig_num_feats\"].append(ft_len)\n",
    "        bnn_gp_dict[\"gp_num_feats\"].append(best_n_models_fit)\n",
    "        bnn_gp_dict[\"cv_score\"].append(best_cv_score_fit)\n",
    "        bnn_gp_dict[\"test_score\"].append(best_test_score_fit)\n",
    "\n",
    "        bnn_gp_dict[\"seed\"].append(seed)\n",
    "        bnn_gp_dict[\"classifier\"].append(\"BNN + LR + GP2\")\n",
    "        bnn_gp_dict[\"orig_num_feats\"].append(ft_len)\n",
    "        bnn_gp_dict[\"gp_num_feats\"].append(best_n_models_oob_fit)\n",
    "        bnn_gp_dict[\"cv_score\"].append(best_cv_score_oob_fit)\n",
    "        bnn_gp_dict[\"test_score\"].append(best_test_score_oob_fit)\n",
    "\n",
    "\n",
    "\n",
    "bnn_gp_df = pd.DataFrame(bnn_gp_dict)\n",
    "bnn_gp_df.to_csv(f\"{save_dir_100}/bnn_lr_gp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_num_features = bnn_gp_dict[\"gp_num_feats\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnn_gp_df = pd.DataFrame(bnn_gp_dict)\n",
    "# bnn_gp_df.to_csv(f\"{save_dir_100}/bnn_lr_gp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>orig_num_feats</th>\n",
       "      <th>classifier</th>\n",
       "      <th>cv_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>422</td>\n",
       "      <td>5</td>\n",
       "      <td>BNN + LR</td>\n",
       "      <td>0.706824</td>\n",
       "      <td>0.583225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>422</td>\n",
       "      <td>5</td>\n",
       "      <td>BNN + LR + GP1</td>\n",
       "      <td>0.713262</td>\n",
       "      <td>0.583748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>422</td>\n",
       "      <td>5</td>\n",
       "      <td>BNN + LR + GP2</td>\n",
       "      <td>0.712281</td>\n",
       "      <td>0.583769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>422</td>\n",
       "      <td>10</td>\n",
       "      <td>BNN + LR</td>\n",
       "      <td>0.705108</td>\n",
       "      <td>0.617021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>422</td>\n",
       "      <td>10</td>\n",
       "      <td>BNN + LR + GP1</td>\n",
       "      <td>0.718502</td>\n",
       "      <td>0.612253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>196</td>\n",
       "      <td>5</td>\n",
       "      <td>BNN + LR + GP1</td>\n",
       "      <td>0.741399</td>\n",
       "      <td>0.695400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>196</td>\n",
       "      <td>5</td>\n",
       "      <td>BNN + LR + GP2</td>\n",
       "      <td>0.741960</td>\n",
       "      <td>0.628377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>196</td>\n",
       "      <td>10</td>\n",
       "      <td>BNN + LR</td>\n",
       "      <td>0.742056</td>\n",
       "      <td>0.694357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>196</td>\n",
       "      <td>10</td>\n",
       "      <td>BNN + LR + GP1</td>\n",
       "      <td>0.738861</td>\n",
       "      <td>0.677255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>196</td>\n",
       "      <td>10</td>\n",
       "      <td>BNN + LR + GP2</td>\n",
       "      <td>0.740198</td>\n",
       "      <td>0.677607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     seed  orig_num_feats      classifier  cv_score  test_score\n",
       "0     422               5        BNN + LR  0.706824    0.583225\n",
       "1     422               5  BNN + LR + GP1  0.713262    0.583748\n",
       "2     422               5  BNN + LR + GP2  0.712281    0.583769\n",
       "3     422              10        BNN + LR  0.705108    0.617021\n",
       "4     422              10  BNN + LR + GP1  0.718502    0.612253\n",
       "..    ...             ...             ...       ...         ...\n",
       "115   196               5  BNN + LR + GP1  0.741399    0.695400\n",
       "116   196               5  BNN + LR + GP2  0.741960    0.628377\n",
       "117   196              10        BNN + LR  0.742056    0.694357\n",
       "118   196              10  BNN + LR + GP1  0.738861    0.677255\n",
       "119   196              10  BNN + LR + GP2  0.740198    0.677607\n",
       "\n",
       "[120 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_gp_df = pd.read_csv(f\"{save_dir_100}/bnn_lr_gp.csv\")\n",
    "bnn_gp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>cv_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classifier</th>\n",
       "      <th>orig_num_feats</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">BNN + LR</th>\n",
       "      <th>5</th>\n",
       "      <td>0.699383</td>\n",
       "      <td>0.660807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.716268</td>\n",
       "      <td>0.691755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">BNN + LR + GP1</th>\n",
       "      <th>5</th>\n",
       "      <td>0.706323</td>\n",
       "      <td>0.660795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.720339</td>\n",
       "      <td>0.685887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">BNN + LR + GP2</th>\n",
       "      <th>5</th>\n",
       "      <td>0.710758</td>\n",
       "      <td>0.645503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.719304</td>\n",
       "      <td>0.686982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cv_score  test_score\n",
       "classifier     orig_num_feats                      \n",
       "BNN + LR       5               0.699383    0.660807\n",
       "               10              0.716268    0.691755\n",
       "BNN + LR + GP1 5               0.706323    0.660795\n",
       "               10              0.720339    0.685887\n",
       "BNN + LR + GP2 5               0.710758    0.645503\n",
       "               10              0.719304    0.686982"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_gp_df.groupby([\"classifier\", \"orig_num_feats\"])[[\"cv_score\", \"test_score\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnn_lr_cv_5_fts = bnn_gp_df[(bnn_gp_df[\"classifier\"] == \"BNN + LR\") & (bnn_gp_df[\"orig_num_feats\"] == 5)][\"cv_score\"].to_numpy()\n",
    "bnn_gp_1_cv_5_fts = bnn_gp_df[(bnn_gp_df[\"classifier\"] == \"BNN + LR + GP1\") & (bnn_gp_df[\"orig_num_feats\"] == 5)][\"cv_score\"].to_numpy()\n",
    "bnn_gp_2_cv_5_fts = bnn_gp_df[(bnn_gp_df[\"classifier\"] == \"BNN + LR + GP2\") & (bnn_gp_df[\"orig_num_feats\"] == 5)][\"cv_score\"].to_numpy()\n",
    "\n",
    "bnn_lr_test_5_fts = bnn_gp_df[(bnn_gp_df[\"classifier\"] == \"BNN + LR\") & (bnn_gp_df[\"orig_num_feats\"] == 5)][\"test_score\"].to_numpy()\n",
    "bnn_gp_1_test_5_fts = bnn_gp_df[(bnn_gp_df[\"classifier\"] == \"BNN + LR + GP2\") & (bnn_gp_df[\"orig_num_feats\"] == 5)][\"test_score\"].to_numpy()\n",
    "bnn_gp_2_test_5_fts = bnn_gp_df[(bnn_gp_df[\"classifier\"] == \"BNN + LR + GP2\") & (bnn_gp_df[\"orig_num_feats\"] == 5)][\"test_score\"].to_numpy()\n",
    "\n",
    "labels = [\"BNN + LR\", \"BNN + LR + GP1\", \"BNN + LR + GP2\"]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "bplot1 = ax1.boxplot([bnn_lr_cv_5_fts, bnn_gp_1_cv_5_fts, bnn_gp_2_cv_5_fts], showmeans=True, patch_artist=True, labels=labels)\n",
    "bplot2 = ax2.boxplot([bnn_lr_test_5_fts, bnn_gp_1_test_5_fts, bnn_gp_2_test_5_fts], showmeans=True, patch_artist=True, labels=labels)\n",
    "\n",
    "# fill with colors\n",
    "colors = ['lightblue', 'lightgreen', \"lightpink\"]\n",
    "for bplot in (bplot1, bplot2):\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "\n",
    "fig.suptitle(\"Feature selection - using 5/100 features for prediction\")\n",
    "plt.legend([bplot1['medians'][0], bplot1['means'][0]], ['median', 'mean'])\n",
    "plt.legend([bplot2['medians'][0], bplot2['means'][0]], ['median', 'mean'])\n",
    "\n",
    "ax1.set_ylabel(\"AUC\")\n",
    "ax1.set_title(\"CV Scores\")\n",
    "\n",
    "ax2.set_ylabel(\"AUC\")\n",
    "ax2.set_title(\"Test Scores\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnn_lr_cv_10_fts = bnn_gp_df[(bnn_gp_df[\"classifier\"] == \"BNN + LR\") & (bnn_gp_df[\"orig_num_feats\"] == 10)][\"cv_score\"].to_numpy()\n",
    "bnn_gp_1_cv_10_fts = bnn_gp_df[(bnn_gp_df[\"classifier\"] == \"BNN + LR + GP1\") & (bnn_gp_df[\"orig_num_feats\"] == 10)][\"cv_score\"].to_numpy()\n",
    "bnn_gp_2_cv_10_fts = bnn_gp_df[(bnn_gp_df[\"classifier\"] == \"BNN + LR + GP2\") & (bnn_gp_df[\"orig_num_feats\"] == 10)][\"cv_score\"].to_numpy()\n",
    "\n",
    "bnn_lr_test_10_fts = bnn_gp_df[(bnn_gp_df[\"classifier\"] == \"BNN + LR\") & (bnn_gp_df[\"orig_num_feats\"] == 10)][\"test_score\"].to_numpy()\n",
    "bnn_gp_1_test_10_fts = bnn_gp_df[(bnn_gp_df[\"classifier\"] == \"BNN + LR + GP2\") & (bnn_gp_df[\"orig_num_feats\"] == 10)][\"test_score\"].to_numpy()\n",
    "bnn_gp_2_test_10_fts = bnn_gp_df[(bnn_gp_df[\"classifier\"] == \"BNN + LR + GP2\") & (bnn_gp_df[\"orig_num_feats\"] == 10)][\"test_score\"].to_numpy()\n",
    "\n",
    "labels = [\"BNN + LR\", \"BNN + LR + GP1\", \"BNN + LR + GP2\"]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "bplot1 = ax1.boxplot([bnn_lr_cv_10_fts, bnn_gp_1_cv_10_fts, bnn_gp_2_cv_10_fts], showmeans=True, patch_artist=True, labels=labels)\n",
    "bplot2 = ax2.boxplot([bnn_lr_test_10_fts, bnn_gp_1_test_10_fts, bnn_gp_2_test_10_fts], showmeans=True, patch_artist=True, labels=labels)\n",
    "\n",
    "# fill with colors\n",
    "colors = ['lightblue', 'lightgreen', \"lightpink\"]\n",
    "for bplot in (bplot1, bplot2):\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "\n",
    "fig.suptitle(\"Feature selection - using 10/100 features for prediction\")\n",
    "plt.legend([bplot1['medians'][0], bplot1['means'][0]], ['median', 'mean'])\n",
    "plt.legend([bplot2['medians'][0], bplot2['means'][0]], ['median', 'mean'])\n",
    "\n",
    "ax1.set_ylabel(\"AUC\")\n",
    "ax1.set_title(\"CV Scores\")\n",
    "\n",
    "ax2.set_ylabel(\"AUC\")\n",
    "ax2.set_title(\"Test Scores\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Logistic Regression vs. BNN feat_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m save_dir_500 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/exp_data_4/bmm/f500\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_dir' is not defined"
     ]
    }
   ],
   "source": [
    "save_dir_500 = f\"{data_dir}/exp_data_4/bmm/f500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running - seed 422\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     37\u001b[0m log_test_score \u001b[38;5;241m=\u001b[39m roc_auc_score(y_test, clf\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 39\u001b[0m config, bnn_cv_score \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_hyper_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# config = pickle.load(open(f\"{save_dir}/bnn_params_s_{seed}.pickle\", \"rb\"))\u001b[39;00m\n\u001b[1;32m     42\u001b[0m config_values\u001b[38;5;241m.\u001b[39mappend(config)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36moptimize_hyper_parameters\u001b[0;34m(seed, X, y)\u001b[0m\n\u001b[1;32m     94\u001b[0m tae \u001b[38;5;241m=\u001b[39m smac\u001b[38;5;241m.\u001b[39mget_tae_runner()\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     incumbent \u001b[38;5;241m=\u001b[39m \u001b[43msmac\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     incumbent \u001b[38;5;241m=\u001b[39m smac\u001b[38;5;241m.\u001b[39msolver\u001b[38;5;241m.\u001b[39mincumbent\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/smac/facade/smac_ac_facade.py:723\u001b[0m, in \u001b[0;36mSMAC4AC.optimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    721\u001b[0m incumbent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 723\u001b[0m     incumbent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/smac/optimizer/smbo.py:279\u001b[0m, in \u001b[0;36mSMBO.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunhistory\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    268\u001b[0m     config\u001b[38;5;241m=\u001b[39mrun_info\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[1;32m    269\u001b[0m     cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(MAXINT) \u001b[38;5;28;01mif\u001b[39;00m num_obj \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mfull(num_obj, \u001b[38;5;28mfloat\u001b[39m(MAXINT)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m     budget\u001b[38;5;241m=\u001b[39mrun_info\u001b[38;5;241m.\u001b[39mbudget,\n\u001b[1;32m    275\u001b[0m )\n\u001b[1;32m    277\u001b[0m run_info\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mconfig_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunhistory\u001b[38;5;241m.\u001b[39mconfig_ids[run_info\u001b[38;5;241m.\u001b[39mconfig]\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtae_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# There are 2 criteria that the stats object uses to know\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# if the budged was exhausted.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# The budget time, which can only be known when the run finishes,\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# And the number of ta executions. Because we submit the job at this point,\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# we count this submission as a run. This prevent for using more\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# runner runs than what the scenario allows\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39msubmitted_ta_runs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/smac/tae/serial_runner.py:87\u001b[0m, in \u001b[0;36mSerialRunner.submit_run\u001b[0;34m(self, run_info)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubmit_run\u001b[39m(\u001b[38;5;28mself\u001b[39m, run_info: RunInfo) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;124;03m\"\"\"This function submits a run_info object in a serial fashion.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    As there is a single worker for this task, this\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m        An object containing the configuration and the necessary data to run it\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_info\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/smac/tae/base.py:220\u001b[0m, in \u001b[0;36mBaseRunner.run_wrapper\u001b[0;34m(self, run_info)\u001b[0m\n\u001b[1;32m    217\u001b[0m     cutoff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mceil(run_info\u001b[38;5;241m.\u001b[39mcutoff))\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     status, cost, runtime, additional_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcutoff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbudget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbudget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstance_specific\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance_specific\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    229\u001b[0m     status \u001b[38;5;241m=\u001b[39m StatusType\u001b[38;5;241m.\u001b[39mCRASHED\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/smac/tae/execute_func.py:217\u001b[0m, in \u001b[0;36mAbstractTAFunc.run\u001b[0;34m(self, config, instance, cutoff, seed, budget, instance_specific)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# call ta\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 217\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_ta\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    220\u001b[0m         result \u001b[38;5;241m=\u001b[39m rval[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/smac/tae/execute_func.py:314\u001b[0m, in \u001b[0;36mExecuteTAFuncDict._call_ta\u001b[0;34m(self, obj, config, obj_kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_ta\u001b[39m(\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    309\u001b[0m     obj: Callable,\n\u001b[1;32m    310\u001b[0m     config: Configuration,\n\u001b[1;32m    311\u001b[0m     obj_kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]],\n\u001b[1;32m    312\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, Dict]]:\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mobj_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mgenerate_train_cs.<locals>.train_cs\u001b[0;34m(config, budget)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# print(seed)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m sgmcmc \u001b[38;5;241m=\u001b[39m MixedSGMCMC(seed\u001b[38;5;241m=\u001b[39mseed, n_samples\u001b[38;5;241m=\u001b[39mbudget, n_warmup\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     64\u001b[0m                     n_chains\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, layer_dims\u001b[38;5;241m=\u001b[39m[config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 65\u001b[0m cv_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43msgmcmc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactivation_fns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactivation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJ\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m cv_score\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py:678\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    674\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcloned_parameters)\n\u001b[1;32m    676\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 678\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, test, train)\n\u001b[1;32m    681\u001b[0m result \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/metaestimators.py:311\u001b[0m, in \u001b[0;36m_safe_split\u001b[0;34m(estimator, X, y, indices, train_indices)\u001b[0m\n\u001b[1;32m    308\u001b[0m     X_subset \u001b[38;5;241m=\u001b[39m _safe_indexing(X, indices)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     y_subset \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     y_subset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/__init__.py:361\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/__init__.py:185\u001b[0m, in \u001b[0;36m_array_indexing\u001b[0;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    184\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m array[:, key]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/numpy/lax_numpy.py:3676\u001b[0m, in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   3673\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mdynamic_index_in_dim(arr, idx, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3675\u001b[0m treedef, static_idx, dynamic_idx \u001b[38;5;241m=\u001b[39m _split_index_for_jit(idx, arr\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m-> 3676\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_gather\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreedef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_are_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3677\u001b[0m \u001b[43m               \u001b[49m\u001b[43munique_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/numpy/lax_numpy.py:3685\u001b[0m, in \u001b[0;36m_gather\u001b[0;34m(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   3682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gather\u001b[39m(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[1;32m   3683\u001b[0m             unique_indices, mode, fill_value):\n\u001b[1;32m   3684\u001b[0m   idx \u001b[38;5;241m=\u001b[39m _merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx)\n\u001b[0;32m-> 3685\u001b[0m   indexer \u001b[38;5;241m=\u001b[39m \u001b[43m_index_to_gather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shared with _scatter_update\u001b[39;00m\n\u001b[1;32m   3686\u001b[0m   y \u001b[38;5;241m=\u001b[39m arr\n\u001b[1;32m   3688\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/numpy/lax_numpy.py:3812\u001b[0m, in \u001b[0;36m_index_to_gather\u001b[0;34m(x_shape, idx, normalize_indices)\u001b[0m\n\u001b[1;32m   3809\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m normalize_indices:\n\u001b[1;32m   3810\u001b[0m     advanced_pairs \u001b[38;5;241m=\u001b[39m ((_normalize_index(e, x_shape[j]), i, j)\n\u001b[1;32m   3811\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m e, i, j \u001b[38;5;129;01min\u001b[39;00m advanced_pairs)\n\u001b[0;32m-> 3812\u001b[0m   advanced_indexes, idx_advanced_axes, x_advanced_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madvanced_pairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m   advanced_axes_are_contiguous \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mall(np\u001b[38;5;241m.\u001b[39mdiff(idx_advanced_axes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3815\u001b[0m x_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Current axis in x.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/numpy/lax_numpy.py:3810\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3806\u001b[0m advanced_pairs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   3807\u001b[0m   (asarray(e), i, j) \u001b[38;5;28;01mfor\u001b[39;00m j, (i, e) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(idx_no_nones)\n\u001b[1;32m   3808\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m isscalar(e) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (Sequence, ndarray, np\u001b[38;5;241m.\u001b[39mndarray)))\n\u001b[1;32m   3809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize_indices:\n\u001b[0;32m-> 3810\u001b[0m   advanced_pairs \u001b[38;5;241m=\u001b[39m ((_normalize_index(e, x_shape[j]), i, j)\n\u001b[1;32m   3811\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m e, i, j \u001b[38;5;129;01min\u001b[39;00m advanced_pairs)\n\u001b[1;32m   3812\u001b[0m advanced_indexes, idx_advanced_axes, x_advanced_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39madvanced_pairs)\n\u001b[1;32m   3813\u001b[0m advanced_axes_are_contiguous \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mall(np\u001b[38;5;241m.\u001b[39mdiff(idx_advanced_axes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/numpy/lax_numpy.py:3807\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_advanced_int_indexer(idx):\n\u001b[1;32m   3805\u001b[0m   idx_no_nones \u001b[38;5;241m=\u001b[39m [(i, d) \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(idx) \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m   3806\u001b[0m   advanced_pairs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 3807\u001b[0m     (\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m, i, j) \u001b[38;5;28;01mfor\u001b[39;00m j, (i, e) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(idx_no_nones)\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isscalar(e) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (Sequence, ndarray, np\u001b[38;5;241m.\u001b[39mndarray)))\n\u001b[1;32m   3809\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m normalize_indices:\n\u001b[1;32m   3810\u001b[0m     advanced_pairs \u001b[38;5;241m=\u001b[39m ((_normalize_index(e, x_shape[j]), i, j)\n\u001b[1;32m   3811\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m e, i, j \u001b[38;5;129;01min\u001b[39;00m advanced_pairs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/numpy/lax_numpy.py:1931\u001b[0m, in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m   1929\u001b[0m lax_internal\u001b[38;5;241m.\u001b[39m_check_user_dtype_supported(dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1930\u001b[0m dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(dtype) \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/numpy/lax_numpy.py:1912\u001b[0m, in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   1908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array(np\u001b[38;5;241m.\u001b[39masarray(view), dtype, copy, ndmin\u001b[38;5;241m=\u001b[39mndmin)\n\u001b[1;32m   1910\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected input type for array: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1912\u001b[0m out_array: Array \u001b[38;5;241m=\u001b[39m \u001b[43mlax_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_element_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweak_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweak_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ndmin \u001b[38;5;241m>\u001b[39m ndim(out_array):\n\u001b[1;32m   1914\u001b[0m   out_array \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mexpand_dims(out_array, \u001b[38;5;28mrange\u001b[39m(ndmin \u001b[38;5;241m-\u001b[39m ndim(out_array)))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/lax/lax.py:594\u001b[0m, in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type)\u001b[0m\n\u001b[1;32m    592\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m type_cast(Array, operand)\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 594\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_element_type_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mweak_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweak_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/core.py:328\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    326\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_enable_checks \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    327\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 328\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/core.py:331\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 331\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/core.py:698\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 698\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/dispatch.py:114\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m compiled_fun \u001b[38;5;241m=\u001b[39m xla_primitive_callable(prim, \u001b[38;5;241m*\u001b[39munsafe_map(arg_spec, args),\n\u001b[1;32m    113\u001b[0m                                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/dispatch.py:199\u001b[0m, in \u001b[0;36mxla_primitive_callable.<locals>.<lambda>\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    196\u001b[0m compiled \u001b[38;5;241m=\u001b[39m _xla_callable_uncached(lu\u001b[38;5;241m.\u001b[39mwrap_init(prim_fun), device, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    197\u001b[0m                                   prim\u001b[38;5;241m.\u001b[39mname, donated_invars, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39marg_specs)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prim\u001b[38;5;241m.\u001b[39mmultiple_results:\n\u001b[0;32m--> 199\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: \u001b[43mcompiled\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m compiled\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/dispatch.py:925\u001b[0m, in \u001b[0;36m_execute_trivial\u001b[0;34m(jaxpr, device, consts, avals, handlers, has_unordered_effects, ordered_effects, kept_var_idx, host_callbacks, *args)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mmap\u001b[39m(env\u001b[38;5;241m.\u001b[39msetdefault, jaxpr\u001b[38;5;241m.\u001b[39mconstvars, consts)\n\u001b[1;32m    923\u001b[0m outs \u001b[38;5;241m=\u001b[39m [xla\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(v\u001b[38;5;241m.\u001b[39mval) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(v) \u001b[38;5;129;01mis\u001b[39;00m core\u001b[38;5;241m.\u001b[39mLiteral \u001b[38;5;28;01melse\u001b[39;00m env[v]\n\u001b[1;32m    924\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m jaxpr\u001b[38;5;241m.\u001b[39moutvars]\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_copy_device_array_to_device(x, device) \u001b[38;5;28;01mif\u001b[39;00m device_array\u001b[38;5;241m.\u001b[39mtype_is_device_array(x)\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m h(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39mdevice_put(x, device)) \u001b[38;5;28;01mfor\u001b[39;00m h, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(handlers, outs)]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/dispatch.py:926\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mmap\u001b[39m(env\u001b[38;5;241m.\u001b[39msetdefault, jaxpr\u001b[38;5;241m.\u001b[39mconstvars, consts)\n\u001b[1;32m    923\u001b[0m outs \u001b[38;5;241m=\u001b[39m [xla\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(v\u001b[38;5;241m.\u001b[39mval) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(v) \u001b[38;5;129;01mis\u001b[39;00m core\u001b[38;5;241m.\u001b[39mLiteral \u001b[38;5;28;01melse\u001b[39;00m env[v]\n\u001b[1;32m    924\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m jaxpr\u001b[38;5;241m.\u001b[39moutvars]\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_copy_device_array_to_device(x, device) \u001b[38;5;28;01mif\u001b[39;00m device_array\u001b[38;5;241m.\u001b[39mtype_is_device_array(x)\n\u001b[0;32m--> 926\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m h(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[43mdevice_put\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m h, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(handlers, outs)]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/dispatch.py:1199\u001b[0m, in \u001b[0;36mdevice_put\u001b[0;34m(x, device)\u001b[0m\n\u001b[1;32m   1197\u001b[0m x \u001b[38;5;241m=\u001b[39m xla\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(x)\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1199\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdevice_put_handlers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1201\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo device_put handler for type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/dispatch.py:1210\u001b[0m, in \u001b[0;36m_device_put_array\u001b[0;34m(x, device)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mfloat0:\n\u001b[1;32m   1209\u001b[0m   x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(x\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;28mbool\u001b[39m))\n\u001b[0;32m-> 1210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_from_pyval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from nn_util import prepare_data\n",
    "from sgmcmc import MixedSGMCMC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "config_values = []\n",
    "\n",
    "bnn_lr_dict_500 = {\"seed\": [], \"classifier\": [], \"cv_score\": [], \"test_score\": []}\n",
    "\n",
    "log_param_grid = {\"C\":np.logspace(-2, 1, 10)}\n",
    "\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    seed, net, data = prepare_data(seeds, i, data_dfs, net_dfs, test_size=0.3)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    print(f\"Running - seed {seed}\")\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = jax.device_put(X_train), jax.device_put(X_test), \\\n",
    "                                       jax.device_put(y_train), jax.device_put(y_test)\n",
    "\n",
    "\n",
    "    log_grid_cv = GridSearchCV(estimator=LogisticRegression(max_iter=10000), param_grid=log_param_grid, verbose=0, scoring=\"roc_auc\", cv=cv).fit(X_train, y_train)\n",
    "    log_cv_score = log_grid_cv.best_score_\n",
    "    clf = LogisticRegression(max_iter=10000, C=log_grid_cv.best_params_[\"C\"])\n",
    "    clf.fit(X_train, y_train)\n",
    "    log_test_score = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])\n",
    "\n",
    "    config, bnn_cv_score = optimize_hyper_parameters(seed, X_train, y_train)\n",
    "\n",
    "    # config = pickle.load(open(f\"{save_dir}/bnn_params_s_{seed}.pickle\", \"rb\"))\n",
    "    config_values.append(config)\n",
    "\n",
    "    pickle.dump(config, open(f\"{save_dir_500}/bnn_params_s_{seed}.pickle\", \"wb\"))\n",
    "\n",
    "    params = {\"disc_lr\": config[\"disc_lr\"], \"contin_lr\": config[\"contin_lr\"], \"batch_size\": config[\"batch_size\"],\n",
    "              \"mu\": config[\"mu\"], \"eta\": config[\"eta\"], \"temp\": config[\"temp\"],\n",
    "              \"sigma\": config[\"sigma\"], \"cycle_len\": config[\"cycle_len\"], \"beta\": config[\"beta\"]}\n",
    "\n",
    "    mixed_sgmcmc = MixedSGMCMC(seed=seed, n_samples=2000, n_warmup=0,\n",
    "                               n_chains=1, layer_dims=[config[\"layer_dim\"]], **params)\n",
    "\n",
    "    # bnn_cv_score = np.mean(cross_val_score(mixed_sgmcmc, X_train, y_train, cv=cv,\n",
    "    #                                        fit_params={\"activation_fns\": [config[\"activation\"]],  \"J\": net}))\n",
    "\n",
    "    mixed_sgmcmc.fit(X_train, y_train, activation_fns=[config[\"activation\"]], J=net)\n",
    "\n",
    "    bnn_test_score = mixed_sgmcmc.score(X_test, y_test)\n",
    "\n",
    "    bnn_disc_mean = jnp.mean(mixed_sgmcmc.states_.discrete_position, axis=0)\n",
    "    np.save(f\"{save_dir_500}/bnn_disc_mean_s_{seed}.npy\", bnn_disc_mean)\n",
    "\n",
    "\n",
    "    bnn_lr_dict_500[\"seed\"].append(seed)\n",
    "    bnn_lr_dict_500[\"classifier\"].append(\"LR\")\n",
    "    bnn_lr_dict_500[\"cv_score\"].append(log_cv_score)\n",
    "    bnn_lr_dict_500[\"test_score\"].append(log_test_score)\n",
    "\n",
    "    bnn_lr_dict_500[\"seed\"].append(seed)\n",
    "    bnn_lr_dict_500[\"classifier\"].append(\"BNN\")\n",
    "    bnn_lr_dict_500[\"cv_score\"].append(bnn_cv_score)\n",
    "    bnn_lr_dict_500[\"test_score\"].append(bnn_test_score)\n",
    "\n",
    "    print(f\"Config: {config}\")\n",
    "\n",
    "    print(f\"Done - seed {seed}\\nLR cv_score: {log_cv_score}, LR test_score: {log_test_score}\")\n",
    "    print(f\"BNN cv_score: {bnn_cv_score}, BNN test_score: {bnn_test_score}\")\n",
    "\n",
    "bnn_lr_df_500 = pd.DataFrame(bnn_lr_dict_500)\n",
    "bnn_lr_df_500.to_csv(f\"{save_dir_500}/res_bmm_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>classifier</th>\n",
       "      <th>cv_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>422</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.558040</td>\n",
       "      <td>0.492985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>422</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.525300</td>\n",
       "      <td>0.543771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.537980</td>\n",
       "      <td>0.549244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>261</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.527738</td>\n",
       "      <td>0.529176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>968</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.552338</td>\n",
       "      <td>0.455197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>968</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.508821</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>282</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.567325</td>\n",
       "      <td>0.502264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>282</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.566687</td>\n",
       "      <td>0.534309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>739</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.677096</td>\n",
       "      <td>0.722995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>739</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.676894</td>\n",
       "      <td>0.713584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>573</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.583870</td>\n",
       "      <td>0.503202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>573</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.576286</td>\n",
       "      <td>0.515734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>220</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.669890</td>\n",
       "      <td>0.569500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>220</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.664444</td>\n",
       "      <td>0.571500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>413</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.567172</td>\n",
       "      <td>0.445625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>413</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.575036</td>\n",
       "      <td>0.437812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>745</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.587840</td>\n",
       "      <td>0.590287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>745</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.589883</td>\n",
       "      <td>0.522572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>775</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.600236</td>\n",
       "      <td>0.537896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>775</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.600186</td>\n",
       "      <td>0.494910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>482</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.598407</td>\n",
       "      <td>0.472448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>482</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.598691</td>\n",
       "      <td>0.490350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>442</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.623667</td>\n",
       "      <td>0.612323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>442</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.645693</td>\n",
       "      <td>0.614570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>210</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.561111</td>\n",
       "      <td>0.480215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>210</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.564912</td>\n",
       "      <td>0.490662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>423</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.473460</td>\n",
       "      <td>0.467267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>423</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.508795</td>\n",
       "      <td>0.500409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>760</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.610281</td>\n",
       "      <td>0.586875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>760</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.617220</td>\n",
       "      <td>0.550938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>57</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.570461</td>\n",
       "      <td>0.515767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>57</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.550247</td>\n",
       "      <td>0.557086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>769</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.656270</td>\n",
       "      <td>0.582351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>769</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.642265</td>\n",
       "      <td>0.525438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>920</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.642312</td>\n",
       "      <td>0.537905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>920</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.630198</td>\n",
       "      <td>0.527488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>226</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.589758</td>\n",
       "      <td>0.397500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>226</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.587198</td>\n",
       "      <td>0.456250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>196</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.569685</td>\n",
       "      <td>0.599589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>196</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.522728</td>\n",
       "      <td>0.651880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seed classifier  cv_score  test_score\n",
       "0    422         LR  0.558040    0.492985\n",
       "1    422        BNN  0.525300    0.543771\n",
       "2    261         LR  0.537980    0.549244\n",
       "3    261        BNN  0.527738    0.529176\n",
       "4    968         LR  0.552338    0.455197\n",
       "5    968        BNN  0.508821    0.555556\n",
       "6    282         LR  0.567325    0.502264\n",
       "7    282        BNN  0.566687    0.534309\n",
       "8    739         LR  0.677096    0.722995\n",
       "9    739        BNN  0.676894    0.713584\n",
       "10   573         LR  0.583870    0.503202\n",
       "11   573        BNN  0.576286    0.515734\n",
       "12   220         LR  0.669890    0.569500\n",
       "13   220        BNN  0.664444    0.571500\n",
       "14   413         LR  0.567172    0.445625\n",
       "15   413        BNN  0.575036    0.437812\n",
       "16   745         LR  0.587840    0.590287\n",
       "17   745        BNN  0.589883    0.522572\n",
       "18   775         LR  0.600236    0.537896\n",
       "19   775        BNN  0.600186    0.494910\n",
       "20   482         LR  0.598407    0.472448\n",
       "21   482        BNN  0.598691    0.490350\n",
       "22   442         LR  0.623667    0.612323\n",
       "23   442        BNN  0.645693    0.614570\n",
       "24   210         LR  0.561111    0.480215\n",
       "25   210        BNN  0.564912    0.490662\n",
       "26   423         LR  0.473460    0.467267\n",
       "27   423        BNN  0.508795    0.500409\n",
       "28   760         LR  0.610281    0.586875\n",
       "29   760        BNN  0.617220    0.550938\n",
       "30    57         LR  0.570461    0.515767\n",
       "31    57        BNN  0.550247    0.557086\n",
       "32   769         LR  0.656270    0.582351\n",
       "33   769        BNN  0.642265    0.525438\n",
       "34   920         LR  0.642312    0.537905\n",
       "35   920        BNN  0.630198    0.527488\n",
       "36   226         LR  0.589758    0.397500\n",
       "37   226        BNN  0.587198    0.456250\n",
       "38   196         LR  0.569685    0.599589\n",
       "39   196        BNN  0.522728    0.651880"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_lr_df_500 = pd.read_csv(f\"{save_dir_500}/res_bmm_summary.csv\")\n",
    "bnn_lr_df_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BNN</th>\n",
       "      <td>0.583961</td>\n",
       "      <td>0.539200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.589860</td>\n",
       "      <td>0.531072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cv_score  test_score\n",
       "classifier                      \n",
       "BNN         0.583961    0.539200\n",
       "LR          0.589860    0.531072"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_lr_df_500.groupby([\"classifier\"])[[\"cv_score\", \"test_score\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Configuration(values={\n",
       "  'activation': 'tanh',\n",
       "  'batch_size': 64,\n",
       "  'beta': 0.9901000815779375,\n",
       "  'contin_lr': 0.000553849670496185,\n",
       "  'disc_lr': 0.015499881752989718,\n",
       "  'eta': 13.42890174428199,\n",
       "  'layer_dim': 50,\n",
       "  'mu': 355.8300138691474,\n",
       "  'sigma': 6.261816495065096,\n",
       "  'temp': 0.0040914487548523466,\n",
       "})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>cv_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_sel</th>\n",
       "      <th>num_feats</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">bnn</th>\n",
       "      <th>5</th>\n",
       "      <td>0.508902</td>\n",
       "      <td>0.506224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.517699</td>\n",
       "      <td>0.511703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.517699</td>\n",
       "      <td>0.511703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">random</th>\n",
       "      <th>5</th>\n",
       "      <td>0.523177</td>\n",
       "      <td>0.518667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.564427</td>\n",
       "      <td>0.578557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.566813</td>\n",
       "      <td>0.574128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cv_score  test_score\n",
       "feat_sel num_feats                      \n",
       "bnn      5          0.508902    0.506224\n",
       "         10         0.517699    0.511703\n",
       "         15         0.517699    0.511703\n",
       "random   5          0.523177    0.518667\n",
       "         10         0.564427    0.578557\n",
       "         15         0.566813    0.574128"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_v_bnn_sel_fts_df.groupby([\"feat_sel\", \"num_feats\"])[[\"cv_score\", \"test_score\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'num features')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma_means = jnp.mean(mixed_sgmcmc.states_.discrete_position, axis=0)\n",
    "bnn_disc_mean_s_442 = np.load(f\"{save_dir_500}/bnn_disc_mean_s_{seed}.npy\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.hist(bnn_disc_mean_s_442)\n",
    "ax1.set_xlabel(\"frequency\")\n",
    "ax1.set_ylabel(\"num features\")\n",
    "\n",
    "ax2.hist(gamma_means)\n",
    "ax2.set_xlabel(\"frequency\")\n",
    "ax2.set_ylabel(\"num features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "bnn_sel_ft_5 = rand_v_bnn_sel_fts_df[(rand_v_bnn_sel_fts_df[\"feat_sel\"] == \"bnn\" ) & (rand_v_bnn_sel_fts_df[\"num_feats\"] == 5)]\n",
    "rand_fisher_sel_ft_5 = rand_v_bnn_sel_fts_df[(rand_v_bnn_sel_fts_df[\"feat_sel\"] == \"random\") & (rand_v_bnn_sel_fts_df[\"num_feats\"] == 5)]\n",
    "\n",
    "bnn_sel_ft_10 = rand_v_bnn_sel_fts_df[(rand_v_bnn_sel_fts_df[\"feat_sel\"] == \"bnn\" ) & (rand_v_bnn_sel_fts_df[\"num_feats\"] == 10)]\n",
    "rand_fisher_sel_ft_10 = rand_v_bnn_sel_fts_df[(rand_v_bnn_sel_fts_df[\"feat_sel\"] == \"random\") & (rand_v_bnn_sel_fts_df[\"num_feats\"] == 10)]\n",
    "\n",
    "cv_score_bnn_sel_ft_5 = bnn_sel_ft_5[\"cv_score\"]\n",
    "cv_score_rand_sel_ft_5 = rand_fisher_sel_ft_5[\"cv_score\"]\n",
    "\n",
    "cv_score_bnn_sel_ft_10 = bnn_sel_ft_10[\"cv_score\"]\n",
    "cv_score_rand_sel_ft_10 = rand_fisher_sel_ft_10[\"cv_score\"]\n",
    "\n",
    "test_score_bnn_sel_ft_5 = bnn_sel_ft_5[\"test_score\"]\n",
    "test_score_rand_sel_ft_5 = rand_fisher_sel_ft_5[\"test_score\"]\n",
    "\n",
    "test_score_bnn_sel_ft_10 = bnn_sel_ft_10[\"test_score\"]\n",
    "test_score_rand_sel_ft_10 = rand_fisher_sel_ft_10[\"test_score\"]\n",
    "\n",
    "p_val_cv_score_5 = stats.ttest_ind(cv_score_bnn_sel_ft_5, cv_score_rand_sel_ft_5, alternative=\"two-sided\", equal_var=False).pvalue\n",
    "p_val_test_score_5 = stats.ttest_ind(test_score_bnn_sel_ft_5, test_score_rand_sel_ft_5, alternative=\"two-sided\", equal_var=False).pvalue\n",
    "\n",
    "p_val_cv_score_10 = stats.ttest_ind(cv_score_bnn_sel_ft_10, cv_score_rand_sel_ft_10, alternative=\"two-sided\", equal_var=False).pvalue\n",
    "p_val_test_score_10 = stats.ttest_ind(test_score_bnn_sel_ft_10, test_score_rand_sel_ft_5, alternative=\"two-sided\", equal_var=False).pvalue\n",
    "\n",
    "print(\"=========== Num feats: 5 =====================\")\n",
    "print(f\"CV Scores: BNN vs Random - p_value (two-sided): {p_val_cv_score_5}\")\n",
    "print(f\"Test Scores: BNN vs Random - p_value (two-sided): {p_val_test_score_5}\")\n",
    "\n",
    "print(\"\\n=========== Num feats: 10 =====================\")\n",
    "print(f\"CV Scores: BNN vs Random - p_value (two-sided): {p_val_cv_score_10}\")\n",
    "print(f\"Test Scores: BNN vs Random - p_value (two-sided): {p_val_test_score_10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Num feats: 5 =====================\n",
      "CV Scores: BNN vs Random - p_value (one-sided): 9.539647905564428e-15\n",
      "Test Scores: BNN vs Random - p_value (one-sided): 2.4414652092392635e-12\n",
      "\n",
      "=========== Num feats: 10 =====================\n",
      "CV Scores: BNN vs Random - p_value (one-sided): 2.301495416941404e-08\n",
      "Test Scores: BNN vs Random - p_value (one-sided): 1.1606598065503922e-11\n"
     ]
    }
   ],
   "source": [
    "p_val_cv_score_5 = stats.ttest_ind(cv_score_bnn_sel_ft_5, cv_score_rand_sel_ft_5, alternative=\"greater\", equal_var=False).pvalue\n",
    "p_val_test_score_5 = stats.ttest_ind(test_score_bnn_sel_ft_5, test_score_rand_sel_ft_5, alternative=\"greater\", equal_var=False).pvalue\n",
    "\n",
    "p_val_cv_score_10 = stats.ttest_ind(cv_score_bnn_sel_ft_10, cv_score_rand_sel_ft_10, alternative=\"greater\", equal_var=False).pvalue\n",
    "p_val_test_score_10 = stats.ttest_ind(test_score_bnn_sel_ft_10, test_score_rand_sel_ft_5, alternative=\"greater\", equal_var=False).pvalue\n",
    "\n",
    "print(\"=========== Num feats: 5 =====================\")\n",
    "print(f\"CV Scores: BNN vs Random - p_value (one-sided): {p_val_cv_score_5}\")\n",
    "print(f\"Test Scores: BNN vs Random - p_value (one-sided): {p_val_test_score_5}\")\n",
    "\n",
    "print(\"\\n=========== Num feats: 10 =====================\")\n",
    "print(f\"CV Scores: BNN vs Random - p_value (one-sided): {p_val_cv_score_10}\")\n",
    "print(f\"Test Scores: BNN vs Random - p_value (one-sided): {p_val_test_score_10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seeds[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_348/1906722657.py:9: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X, y = data_dfs[seed_idx].iloc[:,:-1].to_numpy().astype(np.float),  \\\n",
      "/tmp/ipykernel_348/1906722657.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  data_dfs[seed_idx].iloc[:,-1].to_numpy().astype(np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration(values={\n",
      "  'activation': 'relu',\n",
      "  'batch_size': 32,\n",
      "  'beta': 0.9223176022517695,\n",
      "  'contin_lr': 2.921565057716431e-05,\n",
      "  'cycle_len': 7,\n",
      "  'disc_lr': 0.0255358112072285,\n",
      "  'eta': 0.15141244695593806,\n",
      "  'layer_dim': 300,\n",
      "  'mu': 0.5222315154956412,\n",
      "  'sigma': 7.401408430020509,\n",
      "  'temp': 0.09070826467316453,\n",
      "})\n",
      "\n",
      "0.5378787869760631\n",
      "0.5632159934179469\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "import pickle\n",
    "from sgmcmc import MixedSGMCMC\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "seed_idx = 4\n",
    "\n",
    "seed = seeds[seed_idx]\n",
    "X, y = data_dfs[seed_idx].iloc[:,:-1].to_numpy().astype(np.float),  \\\n",
    "       data_dfs[seed_idx].iloc[:,-1].to_numpy().astype(np.float)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, shuffle=True, stratify=y, test_size=0.3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = jax.device_put(X_train), jax.device_put(X_test), \\\n",
    "                                   jax.device_put(y_train), jax.device_put(y_test)\n",
    "net = net_dfs[seed_idx].to_numpy()\n",
    "\n",
    "bnn_params_s_442 = pickle.load(open(f\"{save_dir_500}/bnn_params_s_{seed}.pickle\", \"rb\"))\n",
    "\n",
    "print(bnn_params_s_442)\n",
    "\n",
    "params = {\"disc_lr\": 0.1, \"contin_lr\": 0.001, \"batch_size\": bnn_params_s_442[\"batch_size\"],\n",
    "          \"mu\": 1000, \"eta\": 10, \"temp\": bnn_params_s_442[\"temp\"],\n",
    "          \"sigma\": bnn_params_s_442[\"sigma\"], \"cycle_len\": 5, \"beta\": 0.95}\n",
    "\n",
    "cv = StratifiedKFold(shuffle=True, random_state=seed)\n",
    "mixed_sgmcmc = MixedSGMCMC(seed=442, n_samples=1000, n_warmup=0,\n",
    "                           n_chains=1, layer_dims=[bnn_params_s_442[\"layer_dim\"]], **params)\n",
    "\n",
    "bnn_cv_score = np.mean(cross_val_score(mixed_sgmcmc, X_train, y_train, cv=cv,\n",
    "                                       fit_params={\"activation_fns\": [bnn_params_s_442[\"activation\"]],  \"J\": net}))\n",
    "\n",
    "print(bnn_cv_score)\n",
    "mixed_sgmcmc.fit(X_train, y_train, activation_fns=[bnn_params_s_442[\"activation\"]], J=net)\n",
    "\n",
    "print(mixed_sgmcmc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mixed_sgmcmc.disc_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'num features')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma_means = jnp.mean(mixed_sgmcmc.states_.discrete_position, axis=0)\n",
    "bnn_disc_mean_s_442 = np.load(f\"{save_dir_500}/bnn_disc_mean_s_{seed}.npy\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.hist(bnn_disc_mean_s_442)\n",
    "ax1.set_xlabel(\"frequency\")\n",
    "ax1.set_ylabel(\"num features\")\n",
    "\n",
    "ax2.hist(gamma_means)\n",
    "ax2.set_xlabel(\"frequency\")\n",
    "ax2.set_ylabel(\"num features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50/723526.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X, y = data_dfs[seed_idx].iloc[:,:-1].to_numpy().astype(np.float), \\\n",
      "/tmp/ipykernel_50/723526.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  data_dfs[seed_idx].iloc[:,-1].to_numpy().astype(np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.46415888336127775}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:21<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from run_moses_cosmic_exp import run_logistc_regression\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed_idx = 2\n",
    "seed = seeds[seed_idx]\n",
    "X, y = data_dfs[seed_idx].iloc[:,:-1].to_numpy().astype(np.float), \\\n",
    "       data_dfs[seed_idx].iloc[:,-1].to_numpy().astype(np.float)\n",
    "\n",
    "net = net_dfs[seed_idx].to_numpy()\n",
    "\n",
    "gamma_means_idx_s = np.argsort(gamma_means)[::-1]\n",
    "\n",
    "log_v_bnn_sel_fts_dict = {\"seed\": [], \"feat_sel\": [], \"num_feats\": [], \"cv_score\": [], \"test_score\": []}\n",
    "\n",
    "all_feats = np.arange(p - 1)\n",
    "\n",
    "feat_lens = [5, 10]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, shuffle=True, stratify=y, test_size=0.3)\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(random_state=seed, shuffle=True, n_splits=5)\n",
    "log_best_params, _, _ = run_logistc_regression(X_train, X_test, y_train, y_test, cv, verbose=0)\n",
    "print(log_best_params)\n",
    "log_clf = LogisticRegression(max_iter=10000, **log_best_params)\n",
    "log_clf.fit(X_train, y_train)\n",
    "\n",
    "log_coef = log_clf.coef_[0]\n",
    "\n",
    "log_coef_sorted = np.argsort(log_coef)[::-1]\n",
    "\n",
    "for seed in tqdm(seeds):\n",
    "\n",
    "    cv = StratifiedKFold(random_state=seed, shuffle=True, n_splits=5)\n",
    "\n",
    "    # print(gamma_means)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, shuffle=True, stratify=y, test_size=0.3)\n",
    "\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "\n",
    "    for ft_len in feat_lens:\n",
    "\n",
    "        log_sel_fts = log_coef_sorted[:ft_len]\n",
    "\n",
    "        X_train_log_sel, X_test_log_sel, = X_train[:,log_sel_fts], X_test[:,log_sel_fts]\n",
    "\n",
    "        _, log_cv_score, log_test_score = run_logistc_regression(X_train_log_sel, X_test_log_sel,\n",
    "                                                                 y_train, y_test, cv, verbose=0)\n",
    "\n",
    "        bnn_sel_fts = gamma_means_idx_s[:ft_len]\n",
    "\n",
    "        X_train_sel, X_test_sel = X_train[:,bnn_sel_fts], X_test[:,bnn_sel_fts]\n",
    "\n",
    "        _, bnn_cv_score, bnn_test_score = run_logistc_regression(X_train_sel, X_test_sel,\n",
    "                                                                 y_train, y_test, cv, verbose=0)\n",
    "\n",
    "\n",
    "        log_v_bnn_sel_fts_dict[\"seed\"].append(seed)\n",
    "        log_v_bnn_sel_fts_dict[\"feat_sel\"].append(\"lr\")\n",
    "        log_v_bnn_sel_fts_dict[\"num_feats\"].append(ft_len)\n",
    "        # log_v_bnn_sel_fts_dict[\"kernel\"].append(svm_log_params[\"kernel\"])\n",
    "        log_v_bnn_sel_fts_dict[\"cv_score\"].append(log_cv_score)\n",
    "        log_v_bnn_sel_fts_dict[\"test_score\"].append(log_test_score)\n",
    "\n",
    "        log_v_bnn_sel_fts_dict[\"seed\"].append(seed)\n",
    "        log_v_bnn_sel_fts_dict[\"feat_sel\"].append(\"bnn\")\n",
    "        log_v_bnn_sel_fts_dict[\"num_feats\"].append(ft_len)\n",
    "        # log_v_bnn_sel_fts_dict[\"kernel\"].append(svm_bnn_params[\"kernel\"])\n",
    "        log_v_bnn_sel_fts_dict[\"cv_score\"].append(bnn_cv_score)\n",
    "        log_v_bnn_sel_fts_dict[\"test_score\"].append(bnn_test_score)\n",
    "\n",
    "\n",
    "log_v_bnn_sel_fts_df = pd.DataFrame(log_v_bnn_sel_fts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>cv_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_sel</th>\n",
       "      <th>num_feats</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bnn</th>\n",
       "      <th>5</th>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.526661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.615466</td>\n",
       "      <td>0.612497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">lr</th>\n",
       "      <th>5</th>\n",
       "      <td>0.655775</td>\n",
       "      <td>0.642661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.657482</td>\n",
       "      <td>0.651044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cv_score  test_score\n",
       "feat_sel num_feats                      \n",
       "bnn      5          0.513978    0.526661\n",
       "         10         0.615466    0.612497\n",
       "lr       5          0.655775    0.642661\n",
       "         10         0.657482    0.651044"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_v_bnn_sel_fts_df.groupby([\"feat_sel\", \"num_feats\"])[[\"cv_score\", \"test_score\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
