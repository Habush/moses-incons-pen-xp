{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\r\n",
      "Collecting jaxlib[cuda112]==0.3.15\r\n",
      "  Downloading https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.15%2Bcuda11.cudnn82-cp39-none-manylinux2014_x86_64.whl (162.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m162.7/162.7 MB\u001B[0m \u001B[31m14.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25h\u001B[33mWARNING: jaxlib 0.3.15+cuda11.cudnn82 does not provide the extra 'cuda112'\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jaxlib[cuda112]==0.3.15) (1.8.1)\r\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.9/dist-packages (from jaxlib[cuda112]==0.3.15) (1.23.1)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from jaxlib[cuda112]==0.3.15) (1.1.0)\r\n",
      "Installing collected packages: jaxlib\r\n",
      "  Attempting uninstall: jaxlib\r\n",
      "    Found existing installation: jaxlib 0.3.8+cuda11.cudnn82\r\n",
      "    Uninstalling jaxlib-0.3.8+cuda11.cudnn82:\r\n",
      "      Successfully uninstalled jaxlib-0.3.8+cuda11.cudnn82\r\n",
      "Successfully installed jaxlib-0.3.15+cuda11.cudnn82\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mLooking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\r\n",
      "Collecting jax[cuda112]==0.3.17\r\n",
      "  Downloading jax-0.3.17.tar.gz (1.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m48.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[33mWARNING: jax 0.3.17 does not provide the extra 'cuda112'\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (1.23.1)\r\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (3.3.0)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (1.8.1)\r\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (4.3.0)\r\n",
      "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (0.6.0)\r\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax[cuda112]==0.3.17) (5.8.0)\r\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax[cuda112]==0.3.17) (3.8.1)\r\n",
      "Building wheels for collected packages: jax\r\n",
      "  Building wheel for jax (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for jax: filename=jax-0.3.17-py3-none-any.whl size=1217849 sha256=e8fa436f6fe1db12d6b4895d2acfbf437c51e9fba25956cbacaaa7deef685fa5\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/36/cd/88/2d90379f7549c27d5654e893f74210f30f0c645c23a71e6f56\r\n",
      "Successfully built jax\r\n",
      "Installing collected packages: jax\r\n",
      "  Attempting uninstall: jax\r\n",
      "    Found existing installation: jax 0.3.14\r\n",
      "    Uninstalling jax-0.3.14:\r\n",
      "      Successfully uninstalled jax-0.3.14\r\n",
      "Successfully installed jax-0.3.17\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting optax\r\n",
      "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m154.9/154.9 kB\u001B[0m \u001B[31m8.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.9/dist-packages (from optax) (4.3.0)\r\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from optax) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from optax) (1.23.1)\r\n",
      "Collecting chex>=0.1.5\r\n",
      "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m85.3/85.3 kB\u001B[0m \u001B[31m27.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.9/dist-packages (from optax) (0.3.17)\r\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.9/dist-packages (from optax) (0.3.15+cuda11.cudnn82)\r\n",
      "Collecting toolz>=0.9.0\r\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m55.8/55.8 kB\u001B[0m \u001B[31m15.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting dm-tree>=0.1.5\r\n",
      "  Downloading dm_tree-0.1.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m153.0/153.0 kB\u001B[0m \u001B[31m33.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: etils[epath] in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (0.6.0)\r\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (3.3.0)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (1.8.1)\r\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax>=0.1.55->optax) (5.8.0)\r\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax>=0.1.55->optax) (3.8.1)\r\n",
      "Installing collected packages: dm-tree, toolz, chex, optax\r\n",
      "Successfully installed chex-0.1.5 dm-tree-0.1.8 optax-0.1.4 toolz-0.12.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting dm-haiku\r\n",
      "  Downloading dm_haiku-0.0.9-py3-none-any.whl (352 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m352.1/352.1 kB\u001B[0m \u001B[31m44.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (1.23.1)\r\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (1.1.0)\r\n",
      "Collecting tabulate>=0.8.9\r\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\r\n",
      "Collecting jmp>=0.0.2\r\n",
      "  Downloading jmp-0.0.2-py3-none-any.whl (16 kB)\r\n",
      "Installing collected packages: tabulate, jmp, dm-haiku\r\n",
      "Successfully installed dm-haiku-0.0.9 jmp-0.0.2 tabulate-0.9.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting tensorflow-probability==0.17\r\n",
      "  Downloading tensorflow_probability-0.17.0-py2.py3-none-any.whl (6.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.5/6.5 MB\u001B[0m \u001B[31m83.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (1.1.0)\r\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow-probability==0.17) (1.14.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (2.1.0)\r\n",
      "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (0.4.0)\r\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (5.1.1)\r\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (0.1.8)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (1.23.1)\r\n",
      "Installing collected packages: tensorflow-probability\r\n",
      "Successfully installed tensorflow-probability-0.17.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting git+https://github.com/blackjax-devs/blackjax.git\r\n",
      "  Cloning https://github.com/blackjax-devs/blackjax.git to /tmp/pip-req-build-ucq6fohk\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/blackjax-devs/blackjax.git /tmp/pip-req-build-ucq6fohk\r\n",
      "  Resolved https://github.com/blackjax-devs/blackjax.git to commit d6801f8a74721f881b5699bd536eea863e2a0009\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting fastprogress>=0.2.0\r\n",
      "  Downloading fastprogress-1.0.3-py3-none-any.whl (12 kB)\r\n",
      "Requirement already satisfied: jax>=0.3.13 in /usr/local/lib/python3.9/dist-packages (from blackjax==0.9.6+85.gd6801f8) (0.3.17)\r\n",
      "Requirement already satisfied: jaxlib>=0.3.10 in /usr/local/lib/python3.9/dist-packages (from blackjax==0.9.6+85.gd6801f8) (0.3.15+cuda11.cudnn82)\r\n",
      "Collecting jaxopt>=0.5.5\r\n",
      "  Downloading jaxopt-0.5.5-py3-none-any.whl (132 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m132.3/132.3 kB\u001B[0m \u001B[31m21.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting typing-extensions>=4.4.0\r\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+85.gd6801f8) (1.23.1)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+85.gd6801f8) (1.8.1)\r\n",
      "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+85.gd6801f8) (0.6.0)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+85.gd6801f8) (1.1.0)\r\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+85.gd6801f8) (3.3.0)\r\n",
      "Requirement already satisfied: matplotlib>=2.0.1 in /usr/local/lib/python3.9/dist-packages (from jaxopt>=0.5.5->blackjax==0.9.6+85.gd6801f8) (3.5.2)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+85.gd6801f8) (1.4.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+85.gd6801f8) (2.8.2)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+85.gd6801f8) (4.34.4)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+85.gd6801f8) (3.0.9)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+85.gd6801f8) (9.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+85.gd6801f8) (0.11.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+85.gd6801f8) (21.3)\r\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax>=0.3.13->blackjax==0.9.6+85.gd6801f8) (5.8.0)\r\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax>=0.3.13->blackjax==0.9.6+85.gd6801f8) (3.8.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+85.gd6801f8) (1.14.0)\r\n",
      "Building wheels for collected packages: blackjax\r\n",
      "  Building wheel for blackjax (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for blackjax: filename=blackjax-0.9.6+85.gd6801f8-py3-none-any.whl size=126529 sha256=218280f7bc0ebefdf0680f63d0d25c940dc496b7f27ed83a4676a70bbbbf421f\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5_8xagro/wheels/e6/1e/f6/a6e0408a4e374b9cdb789b1769716b4ed61eef520a2dd702b1\r\n",
      "Successfully built blackjax\r\n",
      "Installing collected packages: typing-extensions, fastprogress, jaxopt, blackjax\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.3.0\r\n",
      "    Uninstalling typing_extensions-4.3.0:\r\n",
      "      Successfully uninstalled typing_extensions-4.3.0\r\n",
      "Successfully installed blackjax-0.9.6+85.gd6801f8 fastprogress-1.0.3 jaxopt-0.5.5 typing-extensions-4.4.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "Get:1 https://deb.nodesource.com/node_16.x focal InRelease [4583 B]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]                \u001B[0m\u001B[33m\u001B[33m\r\n",
      "Get:3 https://deb.nodesource.com/node_16.x focal/main amd64 Packages [773 B]   \u001B[0m\r\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]        \u001B[0m\u001B[33m\u001B[33m\r\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]      \u001B[0m\u001B[33m\r\n",
      "Get:6 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]      \u001B[0m\r\n",
      "Get:7 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease [18.1 kB] \u001B[0m\u001B[33m\r\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB][33m\r\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB] \u001B[0m\u001B[33m\u001B[33m\r\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB][0m\u001B[33m\u001B[33m\r\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]0m\u001B[33m\r\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [1894 kB]\r\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [30.4 kB]\r\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1275 kB]\r\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2828 kB]\r\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\r\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\r\n",
      "Get:18 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 Packages [29.5 kB]\r\n",
      "Get:19 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [972 kB]\r\n",
      "Get:20 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [27.7 kB]3m\u001B[33m\r\n",
      "Get:21 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2358 kB]\r\n",
      "Get:22 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1779 kB]\r\n",
      "Fetched 24.7 MB in 2s (15.8 MB/s)33m                         \u001B[0m\u001B[33m\u001B[33m\u001B[33m\u001B[33m\u001B[33m\r\n",
      "Reading package lists... Done\r\n",
      "Building dependency tree       \r\n",
      "Reading state information... Done\r\n",
      "112 packages can be upgraded. Run 'apt list --upgradable' to see them.\r\n",
      "Reading package lists... Done\r\n",
      "Building dependency tree       \r\n",
      "Reading state information... Done\r\n",
      "The following additional packages will be installed:\r\n",
      "  fonts-liberation libann0 libcdt5 libcgraph6 libgts-0.7-5 libgts-bin libgvc6\r\n",
      "  libgvpr2 liblab-gamut1 libpathplan4\r\n",
      "Suggested packages:\r\n",
      "  gsfonts graphviz-doc\r\n",
      "The following NEW packages will be installed:\r\n",
      "  fonts-liberation graphviz libann0 libcdt5 libcgraph6 libgts-0.7-5 libgts-bin\r\n",
      "  libgvc6 libgvpr2 liblab-gamut1 libpathplan4\r\n",
      "0 upgraded, 11 newly installed, 0 to remove and 112 not upgraded.\r\n",
      "Need to get 2701 kB of archives.\r\n",
      "After this operation, 11.3 MB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 fonts-liberation all 1:1.07.4-11 [822 kB]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libann0 amd64 1.1.2+doc-7build1 [26.0 kB]\r\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 libcdt5 amd64 2.42.2-3build2 [18.7 kB]\r\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 libcgraph6 amd64 2.42.2-3build2 [41.3 kB]\r\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgts-0.7-5 amd64 0.7.6+darcs121130-4 [150 kB]\r\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/universe amd64 libpathplan4 amd64 2.42.2-3build2 [21.9 kB]\r\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgvc6 amd64 2.42.2-3build2 [647 kB]\r\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgvpr2 amd64 2.42.2-3build2 [167 kB]\r\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/universe amd64 liblab-gamut1 amd64 2.42.2-3build2 [177 kB]\r\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/universe amd64 graphviz amd64 2.42.2-3build2 [590 kB]\r\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgts-bin amd64 0.7.6+darcs121130-4 [41.3 kB]\r\n",
      "Fetched 2701 kB in 0s (27.3 MB/s)      \u001B[0m\u001B[33m\r\n",
      "\n",
      "\u001B7\u001B[0;23r\u001B8\u001B[1ASelecting previously unselected package fonts-liberation.\r\n",
      "(Reading database ... 78556 files and directories currently installed.)\r\n",
      "Preparing to unpack .../00-fonts-liberation_1%3a1.07.4-11_all.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  0%]\u001B[49m\u001B[39m [..........................................................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  2%]\u001B[49m\u001B[39m [#.........................................................] \u001B8Unpacking fonts-liberation (1:1.07.4-11) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  4%]\u001B[49m\u001B[39m [##........................................................] \u001B8Selecting previously unselected package libann0.\r\n",
      "Preparing to unpack .../01-libann0_1.1.2+doc-7build1_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  7%]\u001B[49m\u001B[39m [###.......................................................] \u001B8Unpacking libann0 (1.1.2+doc-7build1) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  9%]\u001B[49m\u001B[39m [#####.....................................................] \u001B8Selecting previously unselected package libcdt5:amd64.\r\n",
      "Preparing to unpack .../02-libcdt5_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 11%]\u001B[49m\u001B[39m [######....................................................] \u001B8Unpacking libcdt5:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 13%]\u001B[49m\u001B[39m [#######...................................................] \u001B8Selecting previously unselected package libcgraph6:amd64.\r\n",
      "Preparing to unpack .../03-libcgraph6_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 16%]\u001B[49m\u001B[39m [#########.................................................] \u001B8Unpacking libcgraph6:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 18%]\u001B[49m\u001B[39m [##########................................................] \u001B8Selecting previously unselected package libgts-0.7-5:amd64.\r\n",
      "Preparing to unpack .../04-libgts-0.7-5_0.7.6+darcs121130-4_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 20%]\u001B[49m\u001B[39m [###########...............................................] \u001B8Unpacking libgts-0.7-5:amd64 (0.7.6+darcs121130-4) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 22%]\u001B[49m\u001B[39m [############..............................................] \u001B8Selecting previously unselected package libpathplan4:amd64.\r\n",
      "Preparing to unpack .../05-libpathplan4_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 24%]\u001B[49m\u001B[39m [##############............................................] \u001B8Unpacking libpathplan4:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 27%]\u001B[49m\u001B[39m [###############...........................................] \u001B8Selecting previously unselected package libgvc6.\r\n",
      "Preparing to unpack .../06-libgvc6_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 29%]\u001B[49m\u001B[39m [################..........................................] \u001B8Unpacking libgvc6 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 31%]\u001B[49m\u001B[39m [##################........................................] \u001B8Selecting previously unselected package libgvpr2:amd64.\r\n",
      "Preparing to unpack .../07-libgvpr2_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 33%]\u001B[49m\u001B[39m [###################.......................................] \u001B8Unpacking libgvpr2:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 36%]\u001B[49m\u001B[39m [####################......................................] \u001B8Selecting previously unselected package liblab-gamut1:amd64.\r\n",
      "Preparing to unpack .../08-liblab-gamut1_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 38%]\u001B[49m\u001B[39m [#####################.....................................] \u001B8Unpacking liblab-gamut1:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 40%]\u001B[49m\u001B[39m [#######################...................................] \u001B8Selecting previously unselected package graphviz.\r\n",
      "Preparing to unpack .../09-graphviz_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 42%]\u001B[49m\u001B[39m [########################..................................] \u001B8Unpacking graphviz (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 44%]\u001B[49m\u001B[39m [#########################.................................] \u001B8Selecting previously unselected package libgts-bin.\r\n",
      "Preparing to unpack .../10-libgts-bin_0.7.6+darcs121130-4_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 47%]\u001B[49m\u001B[39m [###########################...............................] \u001B8Unpacking libgts-bin (0.7.6+darcs121130-4) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 49%]\u001B[49m\u001B[39m [############################..............................] \u001B8Setting up liblab-gamut1:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 51%]\u001B[49m\u001B[39m [#############################.............................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 53%]\u001B[49m\u001B[39m [##############################............................] \u001B8Setting up libgts-0.7-5:amd64 (0.7.6+darcs121130-4) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 56%]\u001B[49m\u001B[39m [################################..........................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 58%]\u001B[49m\u001B[39m [#################################.........................] \u001B8Setting up libpathplan4:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 60%]\u001B[49m\u001B[39m [##################################........................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 62%]\u001B[49m\u001B[39m [####################################......................] \u001B8Setting up libann0 (1.1.2+doc-7build1) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 64%]\u001B[49m\u001B[39m [#####################################.....................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 67%]\u001B[49m\u001B[39m [######################################....................] \u001B8Setting up fonts-liberation (1:1.07.4-11) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 69%]\u001B[49m\u001B[39m [#######################################...................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 71%]\u001B[49m\u001B[39m [#########################################.................] \u001B8Setting up libcdt5:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 73%]\u001B[49m\u001B[39m [##########################################................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 76%]\u001B[49m\u001B[39m [###########################################...............] \u001B8Setting up libcgraph6:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 78%]\u001B[49m\u001B[39m [#############################################.............] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 80%]\u001B[49m\u001B[39m [##############################################............] \u001B8Setting up libgts-bin (0.7.6+darcs121130-4) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 82%]\u001B[49m\u001B[39m [###############################################...........] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 84%]\u001B[49m\u001B[39m [################################################..........] \u001B8Setting up libgvc6 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 87%]\u001B[49m\u001B[39m [##################################################........] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 89%]\u001B[49m\u001B[39m [###################################################.......] \u001B8Setting up libgvpr2:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 91%]\u001B[49m\u001B[39m [####################################################......] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 93%]\u001B[49m\u001B[39m [######################################################....] \u001B8Setting up graphviz (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 96%]\u001B[49m\u001B[39m [#######################################################...] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 98%]\u001B[49m\u001B[39m [########################################################..] \u001B8Processing triggers for libc-bin (2.31-0ubuntu9.7) ...\r\n",
      "Processing triggers for man-db (2.9.1-1) ...\r\n",
      "Processing triggers for fontconfig (2.13.1-2ubuntu3) ...\r\n",
      "\r\n",
      "Reading package lists... Done\r\n",
      "Building dependency tree       \r\n",
      "Reading state information... Done\r\n",
      "The following additional packages will be installed:\r\n",
      "  swig4.0\r\n",
      "Suggested packages:\r\n",
      "  swig-doc swig-examples swig4.0-examples swig4.0-doc\r\n",
      "The following NEW packages will be installed:\r\n",
      "  swig swig4.0\r\n",
      "0 upgraded, 2 newly installed, 0 to remove and 112 not upgraded.\r\n",
      "Need to get 1086 kB of archives.\r\n",
      "After this operation, 5413 kB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig4.0 amd64 4.0.1-5build1 [1081 kB]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig all 4.0.1-5build1 [5528 B]\r\n",
      "Fetched 1086 kB in 1s (1555 kB/s)\r\n",
      "Selecting previously unselected package swig4.0.\r\n",
      "(Reading database ... 78769 files and directories currently installed.)\r\n",
      "Preparing to unpack .../swig4.0_4.0.1-5build1_amd64.deb ...\r\n",
      "Unpacking swig4.0 (4.0.1-5build1) ...\r\n",
      "Selecting previously unselected package swig.\r\n",
      "Preparing to unpack .../swig_4.0.1-5build1_all.deb ...\r\n",
      "Unpacking swig (4.0.1-5build1) ...\r\n",
      "Setting up swig4.0 (4.0.1-5build1) ...\r\n",
      "Setting up swig (4.0.1-5build1) ...\r\n",
      "Processing triggers for man-db (2.9.1-1) ...\r\n",
      "Collecting smac\r\n",
      "  Downloading smac-1.4.0.tar.gz (202 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m202.9/202.9 kB\u001B[0m \u001B[31m29.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.9/dist-packages (from smac) (1.1.1)\r\n",
      "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.9/dist-packages (from smac) (1.23.1)\r\n",
      "Collecting distributed\r\n",
      "  Downloading distributed-2022.12.1-py3-none-any.whl (930 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m930.1/930.1 kB\u001B[0m \u001B[31m73.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from smac) (2022.7.9)\r\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from smac) (1.8.1)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from smac) (5.9.1)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from smac) (1.1.0)\r\n",
      "Collecting pynisher<1.0.0\r\n",
      "  Downloading pynisher-0.6.4.tar.gz (11 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting emcee>=3.0.0\r\n",
      "  Downloading emcee-3.1.3-py2.py3-none-any.whl (46 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.2/46.2 kB\u001B[0m \u001B[31m13.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting ConfigSpace>=0.5.0\r\n",
      "  Downloading ConfigSpace-0.6.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.6/5.6 MB\u001B[0m \u001B[31m38.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0mm\r\n",
      "\u001B[?25hCollecting pyrfr>=0.8.3\r\n",
      "  Downloading pyrfr-0.8.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.5/4.5 MB\u001B[0m \u001B[31m23.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0mm\r\n",
      "\u001B[?25hCollecting dask\r\n",
      "  Downloading dask-2022.12.1-py3-none-any.whl (1.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m83.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from ConfigSpace>=0.5.0->smac) (0.29.30)\r\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from ConfigSpace>=0.5.0->smac) (3.0.9)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from ConfigSpace>=0.5.0->smac) (4.4.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from pynisher<1.0.0->smac) (63.1.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22.0->smac) (3.1.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (2.1.0)\r\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (8.1.3)\r\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (0.12.0)\r\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (2022.5.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (5.4.1)\r\n",
      "Collecting partd>=0.3.10\r\n",
      "  Downloading partd-1.3.0-py3-none-any.whl (18 kB)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (21.3)\r\n",
      "Collecting zict>=0.1.3\r\n",
      "  Downloading zict-2.2.0-py2.py3-none-any.whl (23 kB)\r\n",
      "Collecting tblib>=1.6.0\r\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\r\n",
      "Collecting msgpack>=0.6.0\r\n",
      "  Downloading msgpack-1.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m322.4/322.4 kB\u001B[0m \u001B[31m54.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.9/dist-packages (from distributed->smac) (6.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from distributed->smac) (3.1.2)\r\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from distributed->smac) (1.26.10)\r\n",
      "Collecting sortedcontainers!=2.0.0,!=2.0.1\r\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\r\n",
      "Collecting locket>=1.0.0\r\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\r\n",
      "Collecting heapdict\r\n",
      "  Downloading HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->distributed->smac) (2.1.1)\r\n",
      "Building wheels for collected packages: smac, pynisher\r\n",
      "  Building wheel for smac (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for smac: filename=smac-1.4.0-py3-none-any.whl size=262348 sha256=efe67a31d6cb497f8d48d79d3476a4893afb00c899a45951d82e0e79ef228686\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/cc/e7/d683d9404760c4701ea2f64faaf689a8de718f701de63e71ea\r\n",
      "  Building wheel for pynisher (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for pynisher: filename=pynisher-0.6.4-py3-none-any.whl size=7026 sha256=605336df47f91330797503af9523856dbac1a14989e070926d35e8efd2cfcac0\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/1d/de/5e/d4947b76b76ba27581d1e09f395eca1583a802203a41c04873\r\n",
      "Successfully built smac pynisher\r\n",
      "Installing collected packages: sortedcontainers, msgpack, heapdict, zict, tblib, pyrfr, pynisher, locket, emcee, partd, ConfigSpace, dask, distributed, smac\r\n",
      "Successfully installed ConfigSpace-0.6.0 dask-2022.12.1 distributed-2022.12.1 emcee-3.1.3 heapdict-1.0.1 locket-1.0.0 msgpack-1.0.4 partd-1.3.0 pynisher-0.6.4 pyrfr-0.8.3 smac-1.4.0 sortedcontainers-2.4.0 tblib-1.7.0 zict-2.2.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting graphviz\r\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m47.0/47.0 kB\u001B[0m \u001B[31m12.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: graphviz\r\n",
      "Successfully installed graphviz-0.20.1\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting gplearn\r\n",
      "  Downloading gplearn-0.4.2-py3-none-any.whl (25 kB)\r\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from gplearn) (1.1.0)\r\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from gplearn) (1.1.1)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.8.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (3.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.23.1)\r\n",
      "Installing collected packages: gplearn\r\n",
      "Successfully installed gplearn-0.4.2\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting optuna\r\n",
      "  Downloading optuna-3.0.5-py3-none-any.whl (348 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m348.5/348.5 kB\u001B[0m \u001B[31m46.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting cliff\r\n",
      "  Downloading cliff-4.1.0-py3-none-any.whl (81 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m81.0/81.0 kB\u001B[0m \u001B[31m27.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from optuna) (4.64.0)\r\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (1.4.39)\r\n",
      "Requirement already satisfied: importlib-metadata<5.0.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (4.12.0)\r\n",
      "Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (1.8.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from optuna) (1.23.1)\r\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from optuna) (5.4.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (21.3)\r\n",
      "Collecting alembic>=1.5.0\r\n",
      "  Downloading alembic-1.9.1-py3-none-any.whl (210 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m210.4/210.4 kB\u001B[0m \u001B[31m46.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting cmaes>=0.8.2\r\n",
      "  Downloading cmaes-0.9.0-py3-none-any.whl (23 kB)\r\n",
      "Collecting colorlog\r\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\r\n",
      "Collecting Mako\r\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.7/78.7 kB\u001B[0m \u001B[31m25.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata<5.0.0->optuna) (3.8.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->optuna) (3.0.9)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy>=1.3.0->optuna) (1.1.2)\r\n",
      "Collecting PrettyTable>=0.7.2\r\n",
      "  Downloading prettytable-3.5.0-py3-none-any.whl (26 kB)\r\n",
      "Collecting autopage>=0.4.0\r\n",
      "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\r\n",
      "Collecting stevedore>=2.0.1\r\n",
      "  Downloading stevedore-4.1.1-py3-none-any.whl (50 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m50.0/50.0 kB\u001B[0m \u001B[31m17.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting cmd2>=1.0.0\r\n",
      "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m147.1/147.1 kB\u001B[0m \u001B[31m40.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting pyperclip>=1.6\r\n",
      "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\r\n",
      "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.9/dist-packages (from cmd2>=1.0.0->cliff->optuna) (18.2.0)\r\n",
      "Collecting pbr!=2.1.0,>=2.0.0\r\n",
      "  Downloading pbr-5.11.0-py2.py3-none-any.whl (112 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m112.6/112.6 kB\u001B[0m \u001B[31m33.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\r\n",
      "Building wheels for collected packages: pyperclip\r\n",
      "  Building wheel for pyperclip (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11125 sha256=315e0ce15a37d633982d174bb911a1fce3baa3cf723a82e473fe5f63dbb9c15b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0c/09/9e/49e21a6840ef7955b06d47394afef0058f0378c0914e48b8b8\r\n",
      "Successfully built pyperclip\r\n",
      "Installing collected packages: pyperclip, PrettyTable, pbr, Mako, colorlog, cmd2, cmaes, autopage, stevedore, alembic, cliff, optuna\r\n",
      "Successfully installed Mako-1.2.4 PrettyTable-3.5.0 alembic-1.9.1 autopage-0.5.1 cliff-4.1.0 cmaes-0.9.0 cmd2-2.4.2 colorlog-6.7.0 optuna-3.0.5 pbr-5.11.0 pyperclip-1.8.2 stevedore-4.1.1\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U jaxlib[cuda112]==0.3.15 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install -U jax[cuda112]==0.3.17 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install optax\n",
    "!pip install dm-haiku\n",
    "!pip install tensorflow-probability==0.17\n",
    "!pip install git+https://github.com/blackjax-devs/blackjax.git\n",
    "!apt update\n",
    "!apt install -y graphviz\n",
    "!pip install graphviz\n",
    "!pip install gplearn\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"False\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "SERVER = 1\n",
    "\n",
    "if not SERVER:\n",
    "    %cd /home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "tfd = tfp.distributions\n",
    "import jax\n",
    "import haiku as hk\n",
    "import numpy as np\n",
    "import optax\n",
    "from nn_util import *\n",
    "plt.style.use('ggplot')\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if SERVER:\n",
    "    data_dir = \".\"\n",
    "else:\n",
    "    data_dir = \"/home/xabush/code/snet/moses-incons-pen-xp/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### GDSC Cell Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Tamoxifen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(406, 37265)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdsc_dir = f\"{data_dir}/cell_line/gdsc2\"\n",
    "gdsc_exp_tamox_data = pd.read_csv(f\"{gdsc_dir}/tamoxifen_response_gene_expr.csv\")\n",
    "gdsc_exp_tamox_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X, target = gdsc_exp_tamox_data.iloc[:,:-1], gdsc_exp_tamox_data.iloc[:,-1]\n",
    "# change to -log10(IC_50) to make it comparable\n",
    "target = -np.log10(np.exp(target)) # exp b/c the values are natural logs of raw IC_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     symbol method_of_action        cosmic_moa intogen_moa    gene_id\n0     ABCB1              Act               NaN         Act  SIDG00064\n1      ABI1        ambiguous       TSG, fusion   ambiguous  SIDG00145\n2      ABL1              Act  oncogene, fusion         Act  SIDG00150\n3      ABL2              Act  oncogene, fusion         Act  SIDG00151\n4     ACKR3              Act  oncogene, fusion         Act  SIDG00205\n..      ...              ...               ...         ...        ...\n778  ZNF814              Act               NaN         Act  SIDG42334\n779   ZNF93              LoF               NaN         LoF  SIDG41755\n780   ZNRF3              LoF               NaN         LoF  SIDG42403\n781   ZRSR2              LoF               TSG         LoF  SIDG42422\n782    ZXDB              LoF               NaN         LoF  SIDG42467\n\n[783 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>method_of_action</th>\n      <th>cosmic_moa</th>\n      <th>intogen_moa</th>\n      <th>gene_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ABCB1</td>\n      <td>Act</td>\n      <td>NaN</td>\n      <td>Act</td>\n      <td>SIDG00064</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ABI1</td>\n      <td>ambiguous</td>\n      <td>TSG, fusion</td>\n      <td>ambiguous</td>\n      <td>SIDG00145</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ABL1</td>\n      <td>Act</td>\n      <td>oncogene, fusion</td>\n      <td>Act</td>\n      <td>SIDG00150</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ABL2</td>\n      <td>Act</td>\n      <td>oncogene, fusion</td>\n      <td>Act</td>\n      <td>SIDG00151</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ACKR3</td>\n      <td>Act</td>\n      <td>oncogene, fusion</td>\n      <td>Act</td>\n      <td>SIDG00205</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>778</th>\n      <td>ZNF814</td>\n      <td>Act</td>\n      <td>NaN</td>\n      <td>Act</td>\n      <td>SIDG42334</td>\n    </tr>\n    <tr>\n      <th>779</th>\n      <td>ZNF93</td>\n      <td>LoF</td>\n      <td>NaN</td>\n      <td>LoF</td>\n      <td>SIDG41755</td>\n    </tr>\n    <tr>\n      <th>780</th>\n      <td>ZNRF3</td>\n      <td>LoF</td>\n      <td>NaN</td>\n      <td>LoF</td>\n      <td>SIDG42403</td>\n    </tr>\n    <tr>\n      <th>781</th>\n      <td>ZRSR2</td>\n      <td>LoF</td>\n      <td>TSG</td>\n      <td>LoF</td>\n      <td>SIDG42422</td>\n    </tr>\n    <tr>\n      <th>782</th>\n      <td>ZXDB</td>\n      <td>LoF</td>\n      <td>NaN</td>\n      <td>LoF</td>\n      <td>SIDG42467</td>\n    </tr>\n  </tbody>\n</table>\n<p>783 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_driver_genes_df = pd.read_csv(f\"{data_dir}/cell_line/driver_genes_20221018.csv\")\n",
    "cancer_driver_genes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols = X.columns.to_list()\n",
    "driver_syms = cancer_driver_genes_df[\"symbol\"].to_list()\n",
    "sym_list = [sym.strip() for sym in cols if sym in driver_syms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(406, 768)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_selected = X[sym_list]\n",
    "X_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "seed = 739\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_selected, target, random_state=seed, shuffle=True, test_size=0.2)\n",
    "X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X_train_df, y_train_df, shuffle=True, \n",
    "                                                random_state=seed, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer, PowerTransformer, RobustScaler, MinMaxScaler, Normalizer, StandardScaler\n",
    "\n",
    "train_transformer = QuantileTransformer(random_state=seed, output_distribution=\"normal\").fit(X_train_df)\n",
    "# train_transformer = PowerTransformer().fit(X_train_df)\n",
    "train_transformed = train_transformer.transform(X_train_df)\n",
    "val_transformed = train_transformer.transform(X_val_df)\n",
    "test_transformed = train_transformer.transform(X_test_df)\n",
    "\n",
    "X_train_df = pd.DataFrame(train_transformed, columns=X_train_df.columns)\n",
    "X_val_df = pd.DataFrame(val_transformed, columns=X_val_df.columns)\n",
    "X_test_df = pd.DataFrame(test_transformed, columns=X_test_df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_df.values, y_train_df.values\n",
    "X_val, y_val = X_val_df.values, y_val_df.values\n",
    "X_test, y_test = X_test_df.values, y_test_df.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "import jax\n",
    "from optax import Params\n",
    "from jax import numpy as jnp\n",
    "from optax import GradientTransformation\n",
    "from typing import Any, NamedTuple\n",
    "import tree_utils\n",
    "\n",
    "class TrainingState(NamedTuple):\n",
    "    params: hk.Params\n",
    "    avg_params: hk.Params\n",
    "    opt_state: optax.OptState\n",
    "\n",
    "Momentum = Any  # An arbitrary pytree of `jnp.ndarrays`\n",
    "GradMomentEstimates = optax.Params  # Same type as parameters\n",
    "PreconditionerState = NamedTuple  # State of a preconditioner\n",
    "\n",
    "\n",
    "class OptaxSGLDState(NamedTuple):\n",
    "    \"\"\"Optax state for the SGLD optimizer\"\"\"\n",
    "    count: jnp.ndarray\n",
    "    momentum: Momentum\n",
    "    preconditioner_state: PreconditionerState\n",
    "\n",
    "class BNNState(NamedTuple):\n",
    "    params: hk.Params\n",
    "\n",
    "def sgd_gradient_update(step_size_fn,\n",
    "                         momentum_decay=0.,\n",
    "                         preconditioner=None):\n",
    "  \"\"\"Optax implementation of the SGD optimizer.\n",
    "  \"\"\"\n",
    "\n",
    "  if preconditioner is None:\n",
    "    preconditioner = get_identity_preconditioner()\n",
    "\n",
    "  def init_fn(params):\n",
    "    return OptaxSGLDState(\n",
    "        count=jnp.zeros([], jnp.int32),\n",
    "        momentum=jax.tree_map(jnp.zeros_like, params),\n",
    "        preconditioner_state=preconditioner.init(params))\n",
    "\n",
    "  def update_fn(key, gradient, state):\n",
    "    lr = step_size_fn(state.count)\n",
    "    lr_sqrt = jnp.sqrt(lr)\n",
    "\n",
    "    preconditioner_state = preconditioner.update_preconditioner(\n",
    "        gradient, state.preconditioner_state)\n",
    "\n",
    "    def update_momentum(m, g):\n",
    "      return momentum_decay * m + g * lr_sqrt\n",
    "      \n",
    "\n",
    "\n",
    "    momentum = jax.tree_map(update_momentum, state.momentum, gradient)\n",
    "    updates = preconditioner.multiply_by_m_inv(momentum, preconditioner_state)\n",
    "    updates = jax.tree_map(lambda m: m * lr_sqrt, updates)\n",
    "    return updates, OptaxSGLDState(\n",
    "        count=state.count + 1,\n",
    "        momentum=momentum,\n",
    "        preconditioner_state=preconditioner_state)\n",
    "\n",
    "  return GradientTransformation(init_fn, update_fn)\n",
    "\n",
    "\n",
    "def sgld_gradient_update(step_size_fn,\n",
    "                         momentum_decay=0.,\n",
    "                         preconditioner=None):\n",
    "  \"\"\"Optax implementation of the SGLD optimizer.\n",
    "\n",
    "  If momentum_decay is set to zero, we get the SGLD method [1]. Otherwise,\n",
    "  we get the underdamped SGLD (SGHMC) method [2].\n",
    "\n",
    "  Args:\n",
    "    step_size_fn: a function taking training step as input and prodng the\n",
    "      step size as output.\n",
    "    momentum_decay: float, momentum decay parameter (default: 0).\n",
    "    preconditioner: Preconditioner, an object representing the preconditioner\n",
    "      or None; if None, identity preconditioner is used (default: None).  [1]\n",
    "        \"Bayesian Learning via Stochastic Gradient Langevin Dynamics\" Max\n",
    "        Welling, Yee Whye Teh; ICML 2011  [2] \"Stochastic Gradient Hamiltonian\n",
    "        Monte Carlo\" Tianqi Chen, Emily B. Fox, Carlos Guestrin; ICML 2014\n",
    "  \"\"\"\n",
    "\n",
    "  if preconditioner is None:\n",
    "    preconditioner = get_identity_preconditioner()\n",
    "\n",
    "  def init_fn(params):\n",
    "    return OptaxSGLDState(\n",
    "        count=jnp.zeros([], jnp.int32),\n",
    "        momentum=jax.tree_map(jnp.zeros_like, params),\n",
    "        preconditioner_state=preconditioner.init(params))\n",
    "\n",
    "  def update_fn(key, gradient, state):\n",
    "    lr = step_size_fn(state.count)\n",
    "    lr_sqrt = jnp.sqrt(lr)\n",
    "    noise_std = jnp.sqrt(2 * (1 - momentum_decay))\n",
    "\n",
    "    preconditioner_state = preconditioner.update_preconditioner(\n",
    "        gradient, state.preconditioner_state)\n",
    "\n",
    "    noise_std = jnp.sqrt(2 * (1 - momentum_decay))\n",
    "    noise, _ = tree_utils.normal_like_tree(gradient, key)\n",
    "    noise = preconditioner.multiply_by_m_sqrt(noise, preconditioner_state)\n",
    "\n",
    "    def update_momentum(m, g, n):\n",
    "      return momentum_decay * m + g * lr_sqrt + n * noise_std\n",
    "      \n",
    "\n",
    "\n",
    "    momentum = jax.tree_map(update_momentum, state.momentum, gradient, noise)\n",
    "    updates = preconditioner.multiply_by_m_inv(momentum, preconditioner_state)\n",
    "    updates = jax.tree_map(lambda m: m * lr_sqrt, updates)\n",
    "    return updates, OptaxSGLDState(\n",
    "        count=state.count + 1,\n",
    "        momentum=momentum,\n",
    "        preconditioner_state=preconditioner_state)\n",
    "\n",
    "  return GradientTransformation(init_fn, update_fn)\n",
    "\n",
    "\n",
    "class Preconditioner(NamedTuple):\n",
    "  \"\"\"Preconditioner transformation\"\"\"\n",
    "  init: Any  # TODO @izmailovpavel: fix\n",
    "  update_preconditioner: Any\n",
    "  multiply_by_m_sqrt: Any\n",
    "  multiply_by_m_inv: Any\n",
    "  multiply_by_m_sqrt_inv: Any\n",
    "\n",
    "\n",
    "class RMSPropPreconditionerState(PreconditionerState):\n",
    "  grad_moment_estimates: GradMomentEstimates\n",
    "\n",
    "\n",
    "def get_rmsprop_preconditioner(running_average_factor=0.99, eps=1.e-7):\n",
    "\n",
    "  def init_fn(params):\n",
    "    return RMSPropPreconditionerState(\n",
    "        grad_moment_estimates=jax.tree_map(jnp.zeros_like, params))\n",
    "\n",
    "  def update_preconditioner_fn(gradient, preconditioner_state):\n",
    "    grad_moment_estimates = jax.tree_map(\n",
    "        lambda e, g: e * running_average_factor + \\\n",
    "                     g**2 * (1 - running_average_factor),\n",
    "        preconditioner_state.grad_moment_estimates, gradient)\n",
    "    return RMSPropPreconditionerState(\n",
    "        grad_moment_estimates=grad_moment_estimates)\n",
    "\n",
    "  def multiply_by_m_inv_fn(vec, preconditioner_state):\n",
    "    return jax.tree_map(lambda e, v: v / (eps + jnp.sqrt(e)),\n",
    "                        preconditioner_state.grad_moment_estimates, vec)\n",
    "\n",
    "  def multiply_by_m_sqrt_fn(vec, preconditioner_state):\n",
    "    return jax.tree_map(lambda e, v: v * jnp.sqrt(eps + jnp.sqrt(e)),\n",
    "                        preconditioner_state.grad_moment_estimates, vec)\n",
    "\n",
    "  def multiply_by_m_sqrt_inv_fn(vec, preconditioner_state):\n",
    "    return jax.tree_map(lambda e, v: v / jnp.sqrt(eps + jnp.sqrt(e)),\n",
    "                        preconditioner_state.grad_moment_estimates, vec)\n",
    "\n",
    "  return Preconditioner(\n",
    "      init=init_fn,\n",
    "      update_preconditioner=update_preconditioner_fn,\n",
    "      multiply_by_m_inv=multiply_by_m_inv_fn,\n",
    "      multiply_by_m_sqrt=multiply_by_m_sqrt_fn,\n",
    "      multiply_by_m_sqrt_inv=multiply_by_m_sqrt_inv_fn)\n",
    "\n",
    "\n",
    "class IdentityPreconditionerState(PreconditionerState):\n",
    "  \"\"\"Identity preconditioner is stateless.\"\"\"\n",
    "\n",
    "\n",
    "def get_identity_preconditioner():\n",
    "\n",
    "  def init_fn(_):\n",
    "    return IdentityPreconditionerState()\n",
    "\n",
    "  def update_preconditioner_fn(*args, **kwargs):\n",
    "    return IdentityPreconditionerState()\n",
    "\n",
    "  def multiply_by_m_inv_fn(vec, _):\n",
    "    return vec\n",
    "\n",
    "  def multiply_by_m_sqrt_fn(vec, _):\n",
    "    return vec\n",
    "\n",
    "  def multiply_by_m_sqrt_inv_fn(vec, _):\n",
    "    return vec\n",
    "\n",
    "  return Preconditioner(\n",
    "      init=init_fn,\n",
    "      update_preconditioner=update_preconditioner_fn,\n",
    "      multiply_by_m_inv=multiply_by_m_inv_fn,\n",
    "      multiply_by_m_sqrt=multiply_by_m_sqrt_fn,\n",
    "      multiply_by_m_sqrt_inv=multiply_by_m_sqrt_inv_fn)\n",
    "\n",
    "def make_cyclical_lr_fn(lr_0, total, num_cycles):\n",
    "    k = total // num_cycles\n",
    "    def schedule_fn(step):\n",
    "        rk = (step % k)\n",
    "        cos_inner = jnp.pi * rk\n",
    "        cos_inner /= k\n",
    "        cos_out = jnp.cos(cos_inner) + 1\n",
    "        lr = 0.5*cos_out*lr_0\n",
    "\n",
    "        return lr\n",
    "\n",
    "    return schedule_fn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### BNN without BG"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "class BayesNN():\n",
    "    def __init__(self, sgd_optim, sgld_optim, temperature, sigma, data_size, hidden_sizes, act_fn=jax.nn.relu):\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.act_fn = act_fn\n",
    "        self.sgd_optim = sgd_optim\n",
    "        self.sgld_optim = sgld_optim\n",
    "        self.optimiser = sgd_optim\n",
    "        self._forward = hk.without_apply_rng(hk.transform(self._forward_fn))\n",
    "        self.loss = jax.jit(self.loss)\n",
    "        self.update = jax.jit(self.update)\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.sigma = sigma\n",
    "        self.data_size = data_size\n",
    "        self.add_noise = False\n",
    "\n",
    "        # weight_decay = self.sigma*self.temperature\n",
    "        # self.weight_prior = tfd.Normal(0, self.sigma)\n",
    "        self.weight_prior = tfd.StudentT(df=2, loc=0, scale=self.sigma)\n",
    "        # self.weight_prior = tfd.Laplace(0, self.sigma)\n",
    "\n",
    "    def init(self, rng, x):\n",
    "        params = self._forward.init(rng, x)\n",
    "        opt_state = self.optimiser.init(params)\n",
    "        return params, opt_state\n",
    "\n",
    "    def apply(self, params, x):\n",
    "        return self._forward.apply(params, x).ravel()\n",
    "\n",
    "\n",
    "    def update(self, key, params, opt_state, x, y):\n",
    "        if self.add_noise:\n",
    "            self.optimiser = self.sgld_optim\n",
    "        else:\n",
    "            self.optimiser = self.sgd_optim\n",
    "        grads = jax.grad(self.loss)(params, x, y)\n",
    "        updates, opt_state = self.optimiser.update(key, grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state\n",
    "\n",
    "    def _forward_fn(self, x):\n",
    "        init_fn = hk.initializers.VarianceScaling()\n",
    "        for hd in self.hidden_sizes:\n",
    "            x = hk.Linear(hd, w_init=init_fn, b_init=init_fn)(x)\n",
    "            x = self.act_fn(x)\n",
    "\n",
    "        x = hk.Linear(1)(x)\n",
    "        return x\n",
    "\n",
    "    def log_prior(self, params):\n",
    "        \"\"\"Computes the Gaussian prior log-density.\"\"\"\n",
    "        logprob_tree = jax.tree_util.tree_leaves(jax.tree_util.tree_map(lambda x: jnp.sum(self.weight_prior.log_prob(x.reshape(-1))/self.temperature),\n",
    "                                                                        params))\n",
    "\n",
    "        return sum(logprob_tree)\n",
    "\n",
    "    def log_likelihood(self, params, x, y):\n",
    "        preds = self.apply(params, x).ravel()\n",
    "        log_prob = jnp.sum(tfd.Normal(preds, self.temperature).log_prob(y))\n",
    "        batch_size = x.shape[0]\n",
    "        log_prob = (self.data_size / batch_size)*log_prob\n",
    "        return log_prob\n",
    "\n",
    "    def loss(self, params, x, y):\n",
    "        logprob_prior = self.log_prior(params)\n",
    "        logprob_likelihood = self.log_likelihood(params, x, y)\n",
    "        return logprob_likelihood + logprob_prior\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def visualize_weights(param, color=\"C0\"):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        net - Object of class BaseNetwork\n",
    "        color - Color in which we want to visualize the histogram (for easier separation of activation functions)\n",
    "    \"\"\"\n",
    "    weights = jax.tree_util.tree_leaves(param)\n",
    "    weights = [w.reshape(-1) for w in weights if len(w.shape) > 1]\n",
    "\n",
    "    ## Plotting\n",
    "    columns = len(weights)\n",
    "    fig, ax = plt.subplots(1, columns, figsize=(columns*3.5, 2.5))\n",
    "    fig_index = 0\n",
    "    for g_idx, g in enumerate(weights):\n",
    "        key = f'Layer {g_idx * 2} - weights'\n",
    "        key_ax = ax[g_idx%columns]\n",
    "        sns.histplot(data=g, bins=30, ax=key_ax, color=color, kde=True)\n",
    "        key_ax.set_title(str(key))\n",
    "        key_ax.set_xlabel(\"Grad magnitude\")\n",
    "    fig.suptitle(f\"weight distribution for each layer\", fontsize=14, y=1.05)\n",
    "    fig.subplots_adjust(wspace=0.45)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def visualize_gradients(model, params, x, y, color=\"C0\"):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        model - Object of class BaseNetwork\n",
    "        color - Color in which we want to visualize the histogram (for easier separation of activation functions)\n",
    "    \"\"\"\n",
    "    grads = jax.grad(model.loss)(params, x, y)\n",
    "    grads = jax.device_get(grads)\n",
    "    # We limit our visualization to the weight parameters and exclude the bias to reduce the number of plots\n",
    "    grads = jax.tree_util.tree_leaves(grads)\n",
    "    grads = [g.reshape(-1) for g in grads if len(g.shape) > 1]\n",
    "\n",
    "    ## Plotting\n",
    "    columns = len(grads)\n",
    "    fig, ax = plt.subplots(1, columns, figsize=(columns*3.5, 2.5))\n",
    "    fig_index = 0\n",
    "    for g_idx, g in enumerate(grads):\n",
    "        key = f'Layer {g_idx * 2} - weights'\n",
    "        key_ax = ax[g_idx%columns]\n",
    "        sns.histplot(data=g, bins=30, ax=key_ax, color=color, kde=True)\n",
    "        key_ax.set_title(str(key))\n",
    "        key_ax.set_xlabel(\"Grad magnitude\")\n",
    "    fig.suptitle(f\"Gradient magnitude distribution for activation function {model.act_fn.__class__.__name__}\", fontsize=14, y=1.05)\n",
    "    fig.subplots_adjust(wspace=0.45)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def visualize_activations(model, color=\"C0\"):\n",
    "    activations = model.activations\n",
    "\n",
    "    ## Plotting\n",
    "    columns = 4\n",
    "    rows = int(np.ceil(len(activations)/columns))\n",
    "    fig, ax = plt.subplots(rows, columns, figsize=(columns*2.7, rows*2.5))\n",
    "    act_fn_name = model.act_fn.__class__.__name__\n",
    "    for idx, activ in enumerate(activations):\n",
    "        key_ax = ax[idx//columns][idx%columns]\n",
    "        sns.histplot(data=jax.device_get(activ).reshape(-1), bins=50, ax=key_ax, color=color, kde=True, stat=\"density\")\n",
    "        key_ax.set_title(f\"Layer {idx} - {'Dense' if idx%2==0 else act_fn_name}\")\n",
    "    fig.suptitle(f\"Activation distribution for activation function {act_fn_name}\", fontsize=14)\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "    plt.show()\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import torch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def init_bnn_model(seed, train_loader, epochs, lr_0, num_cycles, temp, sigma, hidden_sizes, act_fn):\n",
    "    torch.manual_seed(seed)\n",
    "    num_batches = len(train_loader)\n",
    "    data_size = X.shape[0]\n",
    "    total_steps = num_batches*epochs\n",
    "    step_size_fn = make_cyclical_lr_fn(lr_0, total_steps, num_cycles)\n",
    "    sgd_optim = sgd_gradient_update(step_size_fn, momentum_decay=0, preconditioner=get_rmsprop_preconditioner())\n",
    "    sgld_optim = sgld_gradient_update(step_size_fn, momentum_decay=0, preconditioner=get_rmsprop_preconditioner())\n",
    "\n",
    "    model = BayesNN(sgd_optim, sgld_optim,\n",
    "                      temp, sigma, data_size, hidden_sizes, act_fn)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_bnn_model(seed, train_loader, epochs, num_cycles, beta, lr_0,\n",
    "                       hidden_sizes, temp, sigma, act_fn=jax.nn.relu):\n",
    "\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    model = init_bnn_model(seed, train_loader, epochs, lr_0, num_cycles, temp, sigma, hidden_sizes, act_fn)\n",
    "\n",
    "    # cycle_len = epochs // num_cycles\n",
    "    num_batches = len(train_loader)\n",
    "    M = (epochs*num_batches) // num_cycles\n",
    "    init_params, init_opt_state = model.init(rng_key, next(iter(train_loader))[0])\n",
    "\n",
    "\n",
    "    states = []\n",
    "    params, opt_state = init_params, init_opt_state\n",
    "    step = 0\n",
    "    key = rng_key\n",
    "    for epoch in range(epochs):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            _, key = jax.random.split(key, 2)\n",
    "            rk = (step % M) / M\n",
    "            params, opt_state = model.update(key, params, opt_state, batch_x, batch_y)\n",
    "            if rk > beta:\n",
    "                model.add_noise = True\n",
    "                states.append(params)\n",
    "            else:\n",
    "                model.add_noise = False\n",
    "            step += 1\n",
    "\n",
    "    return model, states\n",
    "\n",
    "def eval_bnn_model(model, X, y, params):\n",
    "\n",
    "    if isinstance(params, list):\n",
    "        y_preds = np.zeros((len(params), len(y)))\n",
    "        for i, param in enumerate(params):\n",
    "            preds = model.apply(param, X).ravel()\n",
    "            y_preds[i] = preds\n",
    "\n",
    "        y_preds = np.mean(y_preds, axis=0)\n",
    "        rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "    else:\n",
    "        y_preds = model.apply(params, gammas, X).ravel()\n",
    "        rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "\n",
    "    return rmse\n",
    "\n",
    "def score_bnn_model(model, X, y, params):\n",
    "    if isinstance(params, list):\n",
    "        y_preds = np.zeros((len(params), len(y)))\n",
    "        for i, param in enumerate(params):\n",
    "            preds = model.apply(param, X).ravel()\n",
    "            # preds_mean = preds[::2]\n",
    "            y_preds[i] = preds\n",
    "\n",
    "        y_preds = np.mean(y_preds, axis=0)\n",
    "        rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "        if np.isfinite(y_preds).all():\n",
    "            r2 = r2_score(y, y_preds)\n",
    "        else:\n",
    "            r2 = np.nan\n",
    "    else:\n",
    "        y_preds = model.apply(params, X)\n",
    "        preds_mean = y_preds[::2]\n",
    "        rmse = jnp.sqrt(jnp.mean((y - preds_mean)**2))\n",
    "        if np.isfinite(y_preds).all():\n",
    "            r2 = r2_score(y, y_preds)\n",
    "        else:\n",
    "            r2 = np.nan\n",
    "\n",
    "    return rmse, r2\n",
    "\n",
    "def eval_per_model_score(model, X, y, params):\n",
    "    scores = []\n",
    "\n",
    "    for param in params:\n",
    "        preds = model.apply(param, X).ravel()\n",
    "        # preds_mean = preds[::2]\n",
    "        rmse = jnp.sqrt(jnp.mean((y - preds)**2))\n",
    "        scores.append(rmse)\n",
    "\n",
    "\n",
    "\n",
    "    return np.array(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "790"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 1000\n",
    "num_cycles = 10\n",
    "batch_size = 80\n",
    "beta = 0.80\n",
    "lr_0 = 1e-3\n",
    "hidden_sizes = [1000, 500, 300, 100]\n",
    "temp, sigma = 1e-2, 1.0\n",
    "data_loader = NumpyLoader(NumpyData(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "bnn_model, bnn_states = train_bnn_model(seed, data_loader, epochs, num_cycles, beta, lr_0,\n",
    "                    hidden_sizes, temp, sigma, act_fn=jax.nn.swish)\n",
    "len(bnn_states)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.5375946164131165, r2_score: 0.09307623715185953\n",
      "Test RMSE: 0.5161241888999939, r2_score: 0.23625649115521785\n"
     ]
    }
   ],
   "source": [
    "bnn_rmse_val, bnn_r2_val = score_bnn_model(bnn_model, X_val, y_val, bnn_states)\n",
    "bnn_rmse_test, bnn_r2_test = score_bnn_model(bnn_model, X_test, y_test, bnn_states)\n",
    "print(f\"Val RMSE: {bnn_rmse_val}, r2_score: {bnn_r2_val}\")\n",
    "print(f\"Test RMSE: {bnn_rmse_test}, r2_score: {bnn_r2_test}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([ 79.,   0., 237., 158., 158.,  79.,   0.,   0.,   0.,  79.]),\n array([0.5248595 , 0.53419375, 0.543528  , 0.5528623 , 0.56219655,\n        0.5715308 , 0.580865  , 0.5901993 , 0.59953356, 0.6088678 ,\n        0.6182021 ], dtype=float32),\n <BarContainer object of 10 artists>)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD4CAYAAADmWv3KAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAO/ElEQVR4nO3df4zkdX3H8edyu9DEH+EuQy8s0EDTIxGJpSlFW4tStVapFUyTd8S2cpRoEyFoo42nbYI/agtaW0k0pALWo2mEd6wiVirixYbaFMUYiT9oEBCPOw7ulrsqV5Leckz/mO/hht7uzM7Md+b2fc9HspmZz/c783nve7/7um8+953ZmW63iySpnmOmXYAkqR0GvCQVZcBLUlEGvCQVZcBLUlGz0y6g4aU8kjScmeU2HCkBzyOPPDLtEqaq0+mwsLAw7TKmyh7YA7AHhwzSh/n5+RW3u0QjSUUZ8JJUlAEvSUUZ8JJUlAEvSUUZ8JJUlAEvSUUZ8JJUlAEvSUUdMe9k1eDW7VuAvXumM/mGEzi4vjOduSWtigG/Fu3dw4Gr3j2VqY/dcjUY8NKa4BKNJBVlwEtSUQa8JBVlwEtSUQa8JBVlwEtSUQa8JBVlwEtSUQa8JBVlwEtSUQa8JBVlwEtSUQa8JBVlwEtSUQa8JBVlwEtSUQa8JBVlwEtSUQa8JBVlwEtSUQa8JBVlwEtSUQa8JBVlwEtSUQa8JBVlwEtSUQa8JBU122+HiDgFuBHYCHSBT2bmNRGxAbgZOBV4CIjM3BcRM8A1wPnAk8DmzPx2O+VLkpYzyBn8U8A7M/MM4CXAZRFxBrAF2JaZm4BtzWOA1wKbmq+3AteOvWpJUl99Az4zdx06A8/MJ4B7gZOAC4CtzW5bgQub+xcAN2ZmNzPvAo6PiBPHXbgkaWV9l2iWiohTgV8BvgFszMxdzaZH6S3hQC/8H17ytB3N2C5W0Ol0VlNKObOzswP3YP/2OQ60XM9y5ubmWN/Sz2o1PajKHtiDQ8bRh4EDPiKeC/wz8I7M/GlEPLMtM7sR0R2lkIWFhVGevuZ1Op2Be7BucbHlapa3uLjY2s9qNT2oyh7Yg0MG6cP8/PyK2we6iiYi5uiF+z9l5uea4ccOLb00t7ub8Z3AKUuefnIzJkmaoEGuopkBbgDuzcy/XbLpVuBi4Krm9gtLxi+PiJuAFwM/WbKUI0makEGWaF4K/BHw3Yj4TjP2XnrBnhFxKfBj4NCazW30LpG8n95lkpeMs2BJ0mD6Bnxmfh2YWWbzKw+zfxe4bMS6JEkj8p2sklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklTU7LQL0NoyMzvLugfubeW192+fY93i4uHnfc7z6P7PE63M29eGEzi4vjOduaURGPBanSd+yoFr3t/KSx9YYdtxb7+ytXn7OXbL1WDAaw1yiUaSijLgJamovks0EfEp4HXA7sw8sxl7H/AWYE+z23sz87Zm23uAS4GDwBWZeXsLdUuS+hhkDf7TwMeBG581/neZ+TdLByLiDOCNwAuBeeCrEXF6Zh4cQ62SpFXou0STmXcCewd8vQuAmzLzfzPzR8D9wDkj1CdJGtIoV9FcHhFvBr4FvDMz9wEnAXct2WdHM9ZXp3N0X6UwOzs7cA/2b59b8YqTNs0cM3NUzQswNzfH+gkdn6s5DqqyBz3j6MOwAX8t8EGg29x+FPjjUQpZWFgY5elrXqfTGbgHy10rPgndp7tH1bwAi4uLEzs+V3McVGUPegbpw/z8/Irbhwr4zHzs0P2IuA74l+bhTuCUJbue3IxJkiZsqMskI+LEJQ/fAHyvuX8r8MaIOC4iTgM2Ad8crURJ0jAGuUzyM8B5QCcidgBXAudFxFn0lmgeAv4EIDO/HxEJ/AB4CrjMK2gkaTr6BnxmXnSY4RtW2P9DwIdGKUqSNDrfySpJRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklTUbL8dIuJTwOuA3Zl5ZjO2AbgZOBV4CIjM3BcRM8A1wPnAk8DmzPx2O6VLklYyyBn8p4HXPGtsC7AtMzcB25rHAK8FNjVfbwWuHU+ZkqTV6hvwmXknsPdZwxcAW5v7W4ELl4zfmJndzLwLOD4iThxTrZKkVei7RLOMjZm5q7n/KLCxuX8S8PCS/XY0Y7voo9PpDFlKDbOzswP3YP/2OQ60XM9yZo6ZOarmBZibm2P9hI7P1RwHVdmDnnH0YdiAf0ZmdiOiO+rrLCwsjPoSa1qn0xm4B+sWF1uuZnndp0f+Ua+peQEWFxcndnyu5jioyh70DNKH+fn5FbcPexXNY4eWXprb3c34TuCUJfud3IxJkiZs2DP4W4GLgaua2y8sGb88Im4CXgz8ZMlSjiRpgga5TPIzwHlAJyJ2AFfSC/aMiEuBHwPR7H4bvUsk76d3meQlLdQsSRpA34DPzIuW2fTKw+zbBS4btShJ0uh8J6skFWXAS1JRBrwkFWXAS1JRBrwkFWXAS1JRBrwkFWXAS1JRBrwkFWXAS1JRBrwkFWXAS1JRBrwkFTXyX3SatnX7FmDvnulMvuEEDq73T4tVNzM7y7oH7p3IXPu3z/3sL3Z5fE1M1RxZ8wHP3j0cuOrdU5n62C1Xg7+A9T3xUw5c8/6JTLX0b+16fE1Q0RxxiUaSijLgJakoA16SijLgJakoA16SijLgJakoA16SijLgJakoA16SijLgJakoA16SijLgJakoA16SijLgJakoA16SijLgJakoA16SijLgJakoA16SijLgJamokf7odkQ8BDwBHASeysyzI2IDcDNwKvAQEJm5b7QyJUmrNY4z+N/KzLMy8+zm8RZgW2ZuArY1jyVJE9bGEs0FwNbm/lbgwhbmkCT1MdISDdAFvhIRXeDvM/OTwMbM3NVsfxTYOMgLdTqdoQrYv32OA0M9c3Rzc3OsH7LuZ5udnR24B9P8nmeOmTmq5p3m3OM8vtaS1fwujMuRmCPj6MOoAf+bmbkzIn4euCMi/mvpxszsNuHf18LCwlAFrFtcHOp547C4uDh03c/W6XQGfq1pfs/dpwf6cZaZd5pzj/P4WktW87swLkdijgzSh/n5+RW3j7REk5k7m9vdwOeBc4DHIuJEgOZ29yhzSJKGM3TAR8RzIuJ5h+4Drwa+B9wKXNzsdjHwhVGLlCSt3ihn8BuBr0fEPcA3gS9l5peBq4DfjogfAq9qHkuSJmzoNfjMfBD45cOMPw68cpSiJEmj852sklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRRnwklTUbFsvHBGvAa4B1gHXZ+ZVbc0lSfr/WjmDj4h1wCeA1wJnABdFxBltzCVJOry2lmjOAe7PzAcz8wBwE3BBS3NJkg6jrSWak4CHlzzeAbx4pSfMz88PN9P8PJz7reGee4QZuAfT/p5f/XtH17zTnvsoNHQeDD/hEZkjo/ahtTX4VZqZdgGSVE1bSzQ7gVOWPD65GZMkTUhbZ/B3A5si4jR6wf5G4E0tzSVJOoxWzuAz8yngcuB24N7eUH6/jbkkSYc30+12p12DJKkFvpNVkooy4CWpqCPlMsmy+n1kQ0RsBj7Cz64y+nhmXr9k+/OBHwC3ZOblEym6BaP0ISJ+Abie3pVZXeD8zHxoMpWPz4g9+DDwu/ROyu4A3p6Za259dZCPMImIAN5H72d9T2a+qRm/GPiLZre/zMytEyl6zIbtQUScBVwLPB84CHwoM29eaS7P4Fu0io9suDkzz2q+rn/Wtg8Cd7ZcaqvG0IcbgY9k5gvovUt6d+tFj9koPYiI3wBeCrwIOBP4NeDlk6l8fAbpQURsAt4DvDQzXwi8oxnfAFxJ7w2T5wBXRsT6yVU/HqP0AHgSeHMz9hrgYxFx/ErzGfDtGukjGyLiV4GNwFdaqm9Shu5Dc/DPZuYdAJm5PzOfbK/U1oxyLHSBnwOOBY4D5oDHWqmyXYP04C3AJzJzH0BmHvrH/HeAOzJzb7PtDnoht9YM3YPMvC8zf9jcf4Teic4JK03mEk27Bv3Iht+PiJcB9wF/mpkPR8QxwEeBPwRe1Xql7Rq6D8DpwH9HxOeA04CvAlsy82DLNY/b0D3IzP+MiK8Bu+i96/vjmXlv6xWP3yA9OB0gIv6D3hLG+zLzy8s896T2Sm3NKD14RkScQ+8f/AdWmswz+On7InBqZr6I3lnJoXXFtwG3ZeaOqVU2Wcv1YRY4F3gXvaWJXwQ2T6PACThsDyLil4AX0HtH+EnAKyLi3KlV2a5ZYBNwHnARcF2/ZYiCVuxBRJwI/CNwSWY+3e+F1J6+H9mQmY8veXg98OHm/q8D50bE24DnAsdGxP7M3NJivW0ZpQ87gO9k5oMAEXEL8BLghraKbckoPXgDcFdm7geIiH+ld3z8e2vVtmOQjzDZAXwjMxeBH0XEffTCbie9wFv63H9rrdL2jNKDu5uLLr4E/Hlm3tVvMgO+XX0/siEiTszMXc3D19N75y+Z+QdL9tkMnL1Gwx1G6EPz3OMj4oTM3AO8AjjyPvavv1F6sB14S0T8Nb0lmpcDH5tE0WM2yEeY3ELvrPUfIqJDb7niQXpLEX+15D9WX03vPyLXmqF7EBHHAp8HbszMzw4ymUs0LVruIxsi4gMR8fpmtysi4vsRcQ9wBQWXH0bpQ7PW/i5gW0R8l17AXTfp72FUIx4Ln6UXcN8F7qF32dwXJ/oNjMGAPbgdeDwifgB8DfizzHw8M/fSu6Ls7ubrA83YmjJKD4AAXgZsjojvNF9nrTSfH1UgSUV5Bi9JRRnwklSUAS9JRRnwklSUAS9JRRnwklSUAS9JRf0f1K7AhqT7rU8AAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnn_val_losses = eval_per_model_score(bnn_model, X_val, y_val, bnn_states)\n",
    "plt.hist(bnn_val_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "11862"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J = np.load(f\"{data_dir}/cell_line/cancer_genes_net.npy\")\n",
    "np.count_nonzero(J)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def disc_sgd_gradient_update(step_size_fn,\n",
    "                         momentum_decay=0.,\n",
    "                         preconditioner=None):\n",
    "  \"\"\"Optax implementation of the SGD optimizer.\n",
    "  \"\"\"\n",
    "\n",
    "  if preconditioner is None:\n",
    "    preconditioner = get_identity_preconditioner()\n",
    "\n",
    "  def init_fn(gamma):\n",
    "    return OptaxSGLDState(\n",
    "        count=jnp.zeros([], jnp.int32),\n",
    "        momentum=jax.tree_map(jnp.zeros_like, gamma),\n",
    "        preconditioner_state=preconditioner.init(gamma))\n",
    "\n",
    "  def update_fn(key, gamma, gradient, state):\n",
    "    lr = step_size_fn(state.count)\n",
    "    lr_sqrt = jnp.sqrt(lr)\n",
    "\n",
    "    preconditioner_state = preconditioner.update_preconditioner(\n",
    "        gradient, state.preconditioner_state)\n",
    "\n",
    "    def update_momentum(m, g):\n",
    "      return momentum_decay * m + g * lr_sqrt\n",
    "      \n",
    "    def proposal(theta, g, step_size):\n",
    "        diff = (0.5*g*-(2*theta - 1)) - (1./(2*step_size))\n",
    "        prob = jax.nn.sigmoid(diff)\n",
    "        prob_inv = 1 - prob\n",
    "        prob = prob[...,None]\n",
    "        prob_inv = prob_inv[...,None]\n",
    "        delta = jnp.argmax(jnp.concatenate([prob, prob_inv], axis=1), axis=-1)  \n",
    "\n",
    "        theta_delta = (1 - theta)*delta + theta*(1 - delta)\n",
    "        return theta_delta*1.\n",
    "\n",
    "    momentum = jax.tree_map(update_momentum, state.momentum, gradient)\n",
    "    # updates = preconditioner.multiply_by_m_inv(momentum, preconditioner_state)\n",
    "    # updates = jax.tree_map(lambda m: m * lr_sqrt, updates)\n",
    "    gamma = proposal(gamma, gradient, lr)\n",
    "\n",
    "    return gamma, OptaxSGLDState(\n",
    "        count=state.count + 1,\n",
    "        momentum=momentum,\n",
    "        preconditioner_state=preconditioner_state)\n",
    "\n",
    "  return GradientTransformation(init_fn, update_fn)\n",
    "\n",
    "\n",
    "def disc_sgld_gradient_update(step_size_fn, \n",
    "                         momentum_decay=0.,\n",
    "                         preconditioner=None):\n",
    "  \"\"\"Optax implementation of the SGLD optimizer.\n",
    "\n",
    "  If momentum_decay is set to zero, we get the SGLD method [1]. Otherwise,\n",
    "  we get the underdamped SGLD (SGHMC) method [2].\n",
    "\n",
    "  Args:\n",
    "    step_size_fn: a function taking training step as input and prodng the\n",
    "      step size as output.\n",
    "    seed: int, random seed.\n",
    "    momentum_decay: float, momentum decay parameter (default: 0).\n",
    "    preconditioner: Preconditioner, an object representing the preconditioner\n",
    "      or None; if None, identity preconditioner is used (default: None).  [1]\n",
    "        \"Bayesian Learning via Stochastic Gradient Langevin Dynamics\" Max\n",
    "        Welling, Yee Whye Teh; ICML 2011  [2] \"Stochastic Gradient Hamiltonian\n",
    "        Monte Carlo\" Tianqi Chen, Emily B. Fox, Carlos Guestrin; ICML 2014\n",
    "  \"\"\"\n",
    "\n",
    "  if preconditioner is None:\n",
    "    preconditioner = get_identity_preconditioner()\n",
    "\n",
    "  def init_fn(gamma):\n",
    "    return OptaxSGLDState(\n",
    "        count=jnp.zeros([], jnp.int32),\n",
    "        momentum=jax.tree_map(jnp.zeros_like, gamma),\n",
    "        preconditioner_state=preconditioner.init(gamma))\n",
    "\n",
    "  def update_fn(key, gamma, gradient, state):\n",
    "    lr = step_size_fn(state.count)\n",
    "    lr_sqrt = jnp.sqrt(lr)\n",
    "\n",
    "    preconditioner_state = preconditioner.update_preconditioner(\n",
    "        gradient, state.preconditioner_state)\n",
    "\n",
    "    def update_momentum(m, g):\n",
    "      return momentum_decay * m + g * lr_sqrt\n",
    "      \n",
    "    def proposal(key, theta, g, step_size):\n",
    "        diff = (-0.5*g*(2*theta - 1)) - (1./(2*step_size))\n",
    "        delta = jax.random.bernoulli(key, jax.nn.sigmoid(diff))\n",
    "        theta_delta = (1 - theta)*delta + theta*(1 - delta)\n",
    "        return theta_delta*1.\n",
    "\n",
    "\n",
    "\n",
    "    momentum = jax.tree_map(update_momentum, state.momentum, gradient)\n",
    "    # updates = preconditioner.multiply_by_m_inv(momentum, preconditioner_state)\n",
    "    # updates = jax.tree_map(lambda m: m * lr_sqrt, updates)\n",
    "    gamma = proposal(key, gamma, gradient, lr)\n",
    "\n",
    "\n",
    "    return gamma, OptaxSGLDState(\n",
    "        count=state.count + 1,\n",
    "        momentum=momentum,\n",
    "        preconditioner_state=preconditioner_state)\n",
    "\n",
    "  return GradientTransformation(init_fn, update_fn)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### BNN with BG"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "class BgBayesNN():\n",
    "    def __init__(self, sgd_optim, sgld_optim, disc_sgd_optim, disc_sgld_optim, \n",
    "                        temperature, sigma, data_size, hidden_sizes, \n",
    "                        J, eta, mu,\n",
    "                        act_fn=jax.nn.relu):\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.act_fn = act_fn\n",
    "        self.sgd_optim = sgd_optim\n",
    "        self.sgld_optim = sgld_optim\n",
    "        self.optimiser = sgd_optim\n",
    "\n",
    "        self.disc_optimiser = disc_sgd_optim\n",
    "        self.disc_sgd_optim = disc_sgd_optim\n",
    "        self.disc_sgld_optim = disc_sgld_optim\n",
    "\n",
    "        self._forward = hk.transform(self._forward_fn)\n",
    "        self.loss = jax.jit(self.loss)\n",
    "        self.update = jax.jit(self.update)\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.sigma = sigma\n",
    "        self.data_size = data_size\n",
    "        self.add_noise = False\n",
    "        self.J = J\n",
    "        self.eta = eta\n",
    "        self.mu = mu\n",
    "\n",
    "        # weight_decay = self.sigma*self.temperature\n",
    "        # self.weight_prior = tfd.Normal(0, self.sigma)\n",
    "        self.weight_prior = tfd.StudentT(df=2, loc=0, scale=self.sigma)\n",
    "        # self.weight_prior = tfd.Horseshoe(scale=self.sigma)\n",
    "        # self.weight_prior = tfd.Laplace(0, self.sigma)\n",
    "\n",
    "    def init(self, rng, x):\n",
    "        gamma = tfd.Bernoulli(0.5*jnp.ones(x.shape[-1])).sample(seed=rng)*1.\n",
    "        params = self._forward.init(rng, x, gamma)\n",
    "        opt_state = self.optimiser.init(params)\n",
    "        disc_opt_state = self.disc_optimiser.init(gamma)\n",
    "        return params, gamma, opt_state, disc_opt_state\n",
    "\n",
    "    def apply(self, params, gamma, x):\n",
    "        return self._forward.apply(params, None, x, gamma).ravel()\n",
    "\n",
    "    \n",
    "    def loss(self, params, gamma, x, y):\n",
    "        logprob_prior = self.log_prior(params)\n",
    "        logprob_likelihood = self.log_likelihood(params, gamma, x, y)\n",
    "        return logprob_likelihood + logprob_prior\n",
    "\n",
    "    def update(self, key, params, gamma, opt_state, disc_opt_state, x, y):\n",
    "        if self.add_noise:\n",
    "            self.optimiser = self.sgld_optim\n",
    "            self.disc_optimiser = self.disc_sgld_optim\n",
    "        else:\n",
    "            self.optimiser = self.sgd_optim\n",
    "            self.disc_optimiser = self.disc_sgd_optim\n",
    "\n",
    "        contin_loss = lambda p: self.log_prior(params) + self.log_likelihood(p, gamma, x, y)\n",
    "\n",
    "        grads = jax.grad(contin_loss)(params)\n",
    "        updates, opt_state = self.optimiser.update(key, grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "\n",
    "        disc_loss = lambda g: self.ising_prior(g) + self.log_likelihood(params, g, x, y)\n",
    "        disc_logprob, disc_grads = jax.value_and_grad(disc_loss)(gamma)\n",
    "        gamma, disc_opt_state = self.disc_optimiser.update(key, gamma, disc_grads, disc_opt_state)\n",
    "        return params, gamma, opt_state, disc_opt_state\n",
    "\n",
    "    def _forward_fn(self, x, gamma):\n",
    "        x = x @ jnp.diag(gamma)\n",
    "        init_fn = hk.initializers.VarianceScaling()\n",
    "        for hd in self.hidden_sizes:\n",
    "            x = hk.Linear(hd, w_init=init_fn)(x)\n",
    "            x = self.act_fn(x)\n",
    "\n",
    "        x = hk.Linear(1)(x)\n",
    "        return x\n",
    "\n",
    "    def log_prior(self, params):\n",
    "        \"\"\"Computes the Gaussian prior log-density.\"\"\"\n",
    "        logprob_tree = jax.tree_util.tree_leaves(jax.tree_util.tree_map(lambda x: jnp.sum(self.weight_prior.log_prob(x.reshape(-1))/self.temperature), \n",
    "                                                                            params))\n",
    "        \n",
    "        return sum(logprob_tree)\n",
    "\n",
    "    def log_likelihood(self, params, gamma, x, y):\n",
    "        preds = self.apply(params, gamma, x).ravel()\n",
    "        # preds_mean, preds_std = preds[::2], preds[1::2]\n",
    "        # print(preds.shape)\n",
    "        # print(preds_mean.shape)\n",
    "        # print(preds_std.shape)\n",
    "        # preds_std = jax.nn.softplus(preds_std.squeeze())\n",
    "        # preds_mean = preds_mean.squeeze()\n",
    "        # preds_std = (preds_std**2)*self.temperature\n",
    "        log_prob = jnp.sum(tfd.Normal(preds, self.temperature).log_prob(y))\n",
    "        # log_prob = jnp.sum(tfd.MultivariateNormalDiag(preds_mean, preds_std).log_prob(y))\n",
    "        batch_size = x.shape[0]\n",
    "        log_prob = (self.data_size / batch_size)*log_prob\n",
    "        return log_prob\n",
    "\n",
    "    def ising_prior(self, gamma):\n",
    "        \"\"\"Log probability of the Ising model - prior over the discrete variables\"\"\"\n",
    "        return (0.5*self.eta*(gamma.T @ self.J @ gamma) - self.mu*jnp.sum(gamma)) / self.temperature\n",
    "        # x = (2 * gamma) - 1\n",
    "        # xg = x @ self.J\n",
    "        # xgx = (xg * x).sum(-1)\n",
    "        # return (0.5*self.eta*xgx + self.mu*jnp.sum(x)) / self.temperature"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def init_bg_bnn_model(seed, train_loader, epochs, lr_0, disc_lr_0, num_cycles, temp, sigma, hidden_sizes, J, eta, mu, act_fn):\n",
    "    torch.manual_seed(seed)\n",
    "    num_batches = len(train_loader)\n",
    "    data_size = X.shape[0]\n",
    "    total_steps = num_batches*epochs\n",
    "    step_size_fn = make_cyclical_lr_fn(lr_0, total_steps, num_cycles)\n",
    "    # disc_step_size_fn = make_cyclical_lr_fn(disc_lr_0, total_steps, num_cycles)\n",
    "    disc_step_size_fn = lambda step: disc_lr_0\n",
    "    sgd_optim = sgd_gradient_update(step_size_fn, momentum_decay=0, preconditioner=get_rmsprop_preconditioner())\n",
    "    sgld_optim = sgld_gradient_update(step_size_fn, momentum_decay=0, preconditioner=get_rmsprop_preconditioner())\n",
    "    disc_sgd_optim = disc_sgld_gradient_update(disc_step_size_fn, momentum_decay=0, preconditioner=get_identity_preconditioner())\n",
    "    disc_sgld_optim = disc_sgld_gradient_update(disc_step_size_fn, momentum_decay=0, preconditioner=get_identity_preconditioner())\n",
    "\n",
    "    model = BgBayesNN(sgd_optim, sgld_optim, disc_sgd_optim, disc_sgld_optim,\n",
    "                      temp, sigma, data_size, hidden_sizes,\n",
    "                      J, eta, mu, act_fn)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_bg_bnn_model(seed, train_loader, epochs, num_cycles, beta, lr_0, disc_lr_0,\n",
    "                    hidden_sizes, temp, sigma, eta, mu, J, act_fn=jax.nn.relu):\n",
    "\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    model = init_bg_bnn_model(seed, train_loader, epochs, lr_0, disc_lr_0, num_cycles, temp, sigma, hidden_sizes, J, eta, mu, act_fn)\n",
    "\n",
    "    cycle_len = epochs // num_cycles\n",
    "    num_batches = len(train_loader)\n",
    "    M = (epochs*num_batches) // num_cycles\n",
    "    init_params, init_gamma, init_opt_state, init_disc_opt_state = model.init(rng_key, next(iter(train_loader))[0])\n",
    "\n",
    "\n",
    "    states = []\n",
    "    disc_states = []\n",
    "    val_losses = []\n",
    "    params, gamma, opt_state, disc_opt_state = init_params, init_gamma, init_opt_state, init_disc_opt_state\n",
    "    step = 0\n",
    "    key = rng_key\n",
    "    for epoch in range(epochs):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            _, key = jax.random.split(key, 2)\n",
    "            rk = (step % M) / M\n",
    "            params, gamma, opt_state, disc_opt_state = model.update(key, params, gamma, opt_state, disc_opt_state, batch_x, batch_y)\n",
    "            if rk > beta:\n",
    "                states.append(params)\n",
    "                disc_states.append(gamma)\n",
    "                model.add_noise = True\n",
    "            else:\n",
    "                model.add_noise = False\n",
    "            step += 1\n",
    "        # if (epoch % cycle_len) + 1 > (cycle_len - n_models_per_cycle):\n",
    "        #     # print(epoch)\n",
    "        #     states.append(params)\n",
    "        #     disc_states.append(gamma)\n",
    "        # val_loss = eval_bg_bnn_model(model, X_val, y_val, params, gamma)\n",
    "        # val_losses.append(val_loss)\n",
    "\n",
    "    return model, states, disc_states\n",
    "\n",
    "def eval_bg_bnn_model(model, X, y, params, gammas):\n",
    "\n",
    "    if isinstance(params, list):\n",
    "        y_preds = np.zeros((len(params), len(y)))\n",
    "        for i, (param, gamma) in enumerate(zip(params, gammas)):\n",
    "            preds = model.apply(param, gamma, X).ravel()\n",
    "            y_preds[i] = preds\n",
    "\n",
    "        y_preds = np.mean(y_preds, axis=0)\n",
    "        rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "    else:\n",
    "        y_preds = model.apply(params, gammas, X).ravel()\n",
    "        rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "\n",
    "    return rmse\n",
    "\n",
    "def score_bg_bnn_model(model, X, y, params, gammas):\n",
    "    if isinstance(params, list):\n",
    "        y_preds = np.zeros((len(params), len(y)))\n",
    "        for i, (param, gamma) in enumerate(zip(params, gammas)):\n",
    "            preds = model.apply(param, gamma, X).ravel()\n",
    "            # preds_mean = preds[::2]\n",
    "            y_preds[i] = preds\n",
    "\n",
    "        y_preds = np.mean(y_preds, axis=0)\n",
    "        rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "        if np.isfinite(y_preds).all():\n",
    "            r2 = r2_score(y, y_preds)\n",
    "        else:\n",
    "            r2 = np.nan\n",
    "    else:\n",
    "        y_preds = model.apply(params, gammas, X)\n",
    "        preds_mean = y_preds[::2]\n",
    "        rmse = jnp.sqrt(jnp.mean((y - preds_mean)**2))\n",
    "        if np.isfinite(y_preds).all():\n",
    "            r2 = r2_score(y, y_preds)\n",
    "        else:\n",
    "            r2 = np.nan\n",
    "\n",
    "    return rmse, r2\n",
    "\n",
    "def eval_per_model_score_bg(model, X, y, params, gammas):\n",
    "    scores = []\n",
    "\n",
    "    for param, gamma in zip(params, gammas):\n",
    "        preds = model.apply(param, gamma, X).ravel()\n",
    "        # preds_mean = preds[::2]\n",
    "        rmse = jnp.sqrt(jnp.mean((y - preds)**2))\n",
    "        scores.append(rmse)\n",
    "\n",
    "\n",
    "\n",
    "    return np.array(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import operator\n",
    "from smac.facade.smac_hpo_facade import SMAC4HPO\n",
    "from smac.scenario.scenario import Scenario\n",
    "from smac.configspace import ConfigurationSpace\n",
    "from ConfigSpace.hyperparameters import (\n",
    "    CategoricalHyperparameter,\n",
    "    UniformFloatHyperparameter,\n",
    "    UniformIntegerHyperparameter,\n",
    ")\n",
    "\n",
    "from ConfigSpace import InCondition, Configuration\n",
    "\n",
    "import math\n",
    "\n",
    "def evaluate_bnn_bg_models(model, X, y, params, gammas):\n",
    "    eval_fn = lambda p, g: model.apply(p, g, X).ravel()\n",
    "    preds = jax.vmap(eval_fn)(params, gammas)\n",
    "    preds = preds.reshape(-1, preds.shape[-1])\n",
    "    losses = jax.vmap(lambda x, z: jnp.sqrt(jnp.mean((x - z)**2)), in_axes=(0, None))(preds, y)\n",
    "    # mean_loss = jnp.sqrt(jnp.mean(losses, axis=-1))\n",
    "    return jnp.mean(losses)\n",
    "\n",
    "\n",
    "def get_feats_dropout_loss(model, params, gammas, X, y):\n",
    "    var_loss_dict = {\"feats_idx\": [], \"num_models\": [] , \"loss_on\": [], \"loss_off\": [], \"loss_diff\": []}\n",
    "    \n",
    "    disc_states = tree_utils.tree_stack(gammas)\n",
    "    contin_states = tree_utils.tree_stack(params)\n",
    "    \n",
    "    \n",
    "    print(disc_states.shape)\n",
    "    eval_fn = jax.jit(lambda X, y, params, gammas: evaluate_bnn_bg_models(model, X, y, params, gammas))\n",
    "    p = X.shape[1]\n",
    "\n",
    "    for idx in range(p):\n",
    "        # loss_diffs = []\n",
    "        # for i, (param, gamma) in enumerate(zip(params, gammas)):\n",
    "        #     if gamma[idx] == 1:\n",
    "        #         loss_on = eval_bg_bnn_model(model, X, y, [param], [gamma])\n",
    "        #         # Turn off the feature\n",
    "        #         gamma_off = gamma.at[idx].set(0)\n",
    "        #         loss_off = eval_bg_bnn_model(model, X, y, [param], [gamma_off])\n",
    "        #\n",
    "        #         loss_diffs.append((loss_on - loss_off))\n",
    "        #     else:\n",
    "        #         loss_diffs.append(0)\n",
    "        # fi_matrix.append(loss_diffs)\n",
    "        # idx = feats_idx[i]\n",
    "        idx_on = np.argwhere(disc_states[:,idx] == 1.).ravel()\n",
    "        loss_on, loss_off = 0., 0.\n",
    "        if idx_on.size == 0: ## irrelevant feature\n",
    "            loss_diff = 1e9\n",
    "        else:\n",
    "            disc_states_on = disc_states[idx_on]\n",
    "            params_on = jax.tree_util.tree_map(lambda x: x[idx_on], contin_states)\n",
    "            loss_on = eval_fn(X, y, params_on, disc_states_on)\n",
    "\n",
    "            # Turn-off the variable, and see how the loss changes\n",
    "            disc_states_off = disc_states_on.at[:,idx].set(0)\n",
    "            loss_off = eval_fn(X, y, params_on, disc_states_off)\n",
    "\n",
    "            # loss_diff = (loss_on - loss_off) * (len(idx_on) / num_models)\n",
    "            loss_diff = (loss_on - loss_off)\n",
    "\n",
    "\n",
    "        var_loss_dict[\"feats_idx\"].append(idx)\n",
    "        var_loss_dict[\"num_models\"].append(idx_on.size)\n",
    "        var_loss_dict[\"loss_on\"].append(loss_on)\n",
    "        var_loss_dict[\"loss_off\"].append(loss_off)\n",
    "        var_loss_dict[\"loss_diff\"].append(loss_diff)\n",
    "\n",
    "\n",
    "    var_loss_df = pd.DataFrame(var_loss_dict).sort_values(by=\"loss_diff\")\n",
    "\n",
    "    return var_loss_df\n",
    "\n",
    "def get_gene_names(gene_cols):\n",
    "    return [gene.split(\"(\")[0].strip() for gene in gene_cols]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Optuna"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "epochs = 1000\n",
    "num_cycles = 10\n",
    "batch_size = 80\n",
    "hidden_sizes = [1000, 500, 300, 100]\n",
    "lr_0 = 1e-3\n",
    "act_fn = jax.nn.swish\n",
    "\n",
    "def objective(trial, seed, x_train, x_val, y_train, y_val, J, epochs, num_cycles,\n",
    "              batch_size, hidden_sizes, lr_0, act_fn):\n",
    "\n",
    "    disc_lr = trial.suggest_float(\"disc_lr\", 0.1, 0.9)\n",
    "    temp = trial.suggest_categorical(\"temp\", [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1.])\n",
    "    beta = trial.suggest_float(\"beta\", 0.6, 0.9)\n",
    "    eta = 1.0\n",
    "    mu = trial.suggest_float(\"mu\", 1.0, 1e2, log=True)\n",
    "    sigma = 1.0\n",
    "    torch.manual_seed(seed)\n",
    "    data_loader = NumpyLoader(NumpyData(x_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    bg_bnn_model, states, disc_states = train_bg_bnn_model(seed, data_loader, epochs, num_cycles, beta, lr_0, disc_lr,\n",
    "                                                           hidden_sizes, temp, sigma, eta, mu, J, act_fn)\n",
    "\n",
    "    rmse = eval_bg_bnn_model(bg_bnn_model, x_val, y_val, states, disc_states)\n",
    "    return rmse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "sampler = optuna.samplers.TPESampler(seed=seed)\n",
    "study = optuna.create_study(sampler=sampler)\n",
    "study.optimize(lambda trial: objective(trial, seed, X_train, X_val, y_train, y_val, J, epochs, num_cycles, batch_size, hidden_sizes, lr_0, act_fn), timeout=300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'disc_lr': 0.2437504314724081, 'temp': 0.01, 'beta': 0.8543741461429453, 'mu': 6.28040232208994}\n"
     ]
    }
   ],
   "source": [
    "bg_config = study.best_params\n",
    "print(bg_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "{'disc_lr': 0.10851902804665374,\n 'temp': 0.01,\n 'beta': 0.750480777514709,\n 'mu': 10.18776343605859}"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "save_dir = f\"{data_dir}/exp_data_5/cancer/gdsc\"\n",
    "bg_config = pickle.load(open(f\"{save_dir}/configs/bnn_config_bg_s_{seed}_v2.pkl\", \"rb\"))\n",
    "bg_config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "data": {
      "text/plain": "580"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng_key = jax.random.PRNGKey(seed)\n",
    "\n",
    "disc_lr_0 = bg_config[\"disc_lr\"]\n",
    "temp, sigma = bg_config[\"temp\"], 1.0\n",
    "eta, mu = 1.0, bg_config[\"mu\"]\n",
    "beta = bg_config[\"beta\"]\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "data_loader = NumpyLoader(NumpyData(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "val_data = NumpyData(X_val, y_val)\n",
    "\n",
    "bg_bnn_model, bnn_bg_states, bg_disc_states = train_bg_bnn_model(seed, data_loader, epochs, num_cycles, beta, lr_0, disc_lr_0,\n",
    "                        hidden_sizes, temp, sigma, eta, mu, J, act_fn=jax.nn.swish)\n",
    "\n",
    "len(bnn_bg_states)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_bg_rmse_val, bnn_bg_r2_val = score_bg_bnn_model(bg_bnn_model, X_val, y_val, bnn_bg_states, bg_disc_states)\n",
    "bnn_bg_rmse_test, bnn_bg_r2_test = score_bg_bnn_model(bg_bnn_model, X_test, y_test, bnn_bg_states, bg_disc_states)\n",
    "print(f\"Val  RMSE: {bnn_bg_rmse_val}, r2_score: {bnn_bg_r2_val}\")\n",
    "print(f\"Test RMSE: {bnn_bg_rmse_test}, r2_score: {bnn_bg_r2_test}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_lst_loss = np.sort(bnn_val_losses)[0]\n",
    "bnn_lst_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_idx = np.argwhere((bnn_bg_val_losses - bnn_lst_loss) < -0.1).squeeze()\n",
    "val_losses_sel = bnn_bg_val_losses[model_idx]\n",
    "plt.hist(val_losses_sel)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_bg_states_sel = list(operator.itemgetter(*model_idx)(bnn_bg_states))\n",
    "bg_disc_states_sel = list(operator.itemgetter(*model_idx)(bg_disc_states))\n",
    "len(bnn_bg_states_sel)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_bg_rmse_val, bnn_bg_r2_val = score_bg_bnn_model(bg_bnn_model, X_val, y_val, bnn_bg_states_sel, bg_disc_states_sel)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gamma_mean = np.mean(np.array(bg_disc_states), axis=0)\n",
    "plt.hist(gamma_mean)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Input Gradients"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_input_gradient(model, X, y, params, gammas):\n",
    "    n, p = X.shape\n",
    "    k = len(params)\n",
    "    mat = np.zeros((k, n, p))\n",
    "\n",
    "    for i in range(n):\n",
    "        x_i, y_i = X[i], y[i]\n",
    "        for j in range(k):\n",
    "            param, gamma = params[j], gammas[j]\n",
    "            gx = jax.grad(model.log_likelihood, argnums=2)(param, gamma, x_i, y_i)\n",
    "            # gx = jax.grad(lambda p, g, x: model.apply(p, g, x).squeeze(), argnums=2)(param, gamma, x_i)\n",
    "            mat[j, i] = gx\n",
    "\n",
    "    return mat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Integrated Gradients"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "def integrated_gradients(model, st, disc_st, x, N):\n",
    "\n",
    "    params = tree_utils.tree_stack(st)\n",
    "    gammas = tree_utils.tree_stack(disc_st)\n",
    "\n",
    "    p = x.shape[-1]\n",
    "    baseline = jnp.zeros((1, p))\n",
    "    t = jnp.linspace(0, 1, N).reshape(-1, 1)\n",
    "    path = baseline * (1 - t) + x * t\n",
    "\n",
    "    def get_grad(pi):\n",
    "        # compute gradient\n",
    "        # add/remove batch axes\n",
    "        return jnp.mean(jax.vmap(jax.grad(lambda param, gamma, p: model.apply(param, gamma, p).squeeze(), argnums=2), in_axes=(0, 0, None))(params, gammas, pi), axis=0)\n",
    "        # return jnp.mean(jax.vmap(jax.grad(lambda param, gamma, p: model.log_likelihood(param, gamma, p, y).squeeze(), argnums=2), in_axes=(0, 0, None,))(params, gammas, pi), axis=0)\n",
    "\n",
    "    gs = jax.vmap(get_grad)(path)\n",
    "    # sum pieces (Riemann sum), multiply by (x - x')\n",
    "    ig = jnp.mean(gs, axis=0, keepdims=True) * (x - baseline)\n",
    "    return ig.squeeze()\n",
    "\n",
    "def plot_grad(g, ax=None):\n",
    "    # g = np.array(g)\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        ax = plt.gca()\n",
    "\n",
    "    s = g.shape[0]\n",
    "    ax.bar(np.arange(s), height=g)\n",
    "    ax.set_xticks(range(s))\n",
    "    # ax.set_xticklabels(range(s))\n",
    "    ax.set_xlabel(\"Feature $x_i$\")\n",
    "    ax.set_ylabel(r\"Gradient $\\frac{\\partial \\hat{f}(\\vec{x})}{\\partial x_i}$\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "igs = jax.vmap(integrated_gradients, in_axes=(None, None, None, 0, None))(bg_bnn_model, bnn_bg_states, bg_disc_states, X_val, 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "igs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "igs = np.zeros_like(X_val)\n",
    "for i in tqdm(range(X_val.shape[0])):\n",
    "    igs[i] = integrated_gradients(bg_bnn_model, bnn_bg_states_sel, bg_disc_states_sel, X_val[i], 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "igs_mean = np.mean(np.abs(igs), axis=0)\n",
    "plot_grad(igs_mean)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_df.iloc[:,np.argsort(igs_mean)[::-1][:20]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Shapley Valeus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def shapley(key, model, i, sm, sampled_x):\n",
    "    m, p = sampled_x.shape\n",
    "    z_choice = jax.random.bernoulli(key, shape=(m, p))*1.\n",
    "    #construct with and w/o ith feature\n",
    "    z_choice = z_choice.at[:, i].set(0.0)\n",
    "    z_choice_i = z_choice.at[:, i].set(1.0)\n",
    "    # select them via multiplication\n",
    "    z = sm*z_choice + sampled_x * (1 - z_choice)\n",
    "    z_i = sm*z_choice_i + sampled_x * (1 - z_choice_i)\n",
    "\n",
    "    v = model(z_i) - model(z)\n",
    "    return jnp.squeeze(jnp.mean(v, axis=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "params, gammas = tree_utils.tree_stack(bnn_bg_states), tree_utils.tree_stack(bg_disc_states)\n",
    "model_apply = lambda x: jnp.mean(jax.vmap(lambda p, g, x: bg_bnn_model.apply(p, g, x), in_axes=(0, 0, None))(params, gammas, x), axis=0)\n",
    "\n",
    "\n",
    "n, p = X_val.shape\n",
    "sl = jnp.arange(p)\n",
    "\n",
    "svs = np.zeros_like(X_val)\n",
    "key, _ = jax.random.split(rng_key, 2)\n",
    "bshapley = jax.vmap(shapley, in_axes=(None, None, 0, None, None, None))\n",
    "M = 20\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    key, _ = jax.random.split(key, 2)\n",
    "    sm = X_val[i]\n",
    "    sampled_idx = jax.random.choice(key, jnp.arange(n), shape=(M,), replace=False).reshape(1, -1)\n",
    "    sampled_x, sampled_y = X_val[sampled_idx].squeeze(), y_val[sampled_idx].squeeze()\n",
    "    sv = bshapley(key, model_apply, sl, sm, sampled_x, sampled_y)\n",
    "    svs[i] = sv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shapley_mean = np.mean(svs, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_apply(X_val[10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random Forest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def train_rf_model(seed, X, y):\n",
    "\n",
    "    cv = KFold(n_splits=5, random_state=seed, shuffle=True)\n",
    "\n",
    "    param_grid = {\n",
    "        'bootstrap': [True],\n",
    "        'max_depth': [80, 90, 100, 110],\n",
    "        'max_features': [2, 3],\n",
    "        'min_samples_leaf': [3, 4, 5],\n",
    "        'min_samples_split': [8, 10, 12],\n",
    "        'n_estimators': [100, 200, 300, 1000]\n",
    "    }\n",
    "\n",
    "    rf_reg = RandomForestRegressor(random_state=seed, max_samples=1.0)\n",
    "    grid_cv = GridSearchCV(estimator = rf_reg, param_grid = param_grid,\n",
    "                            cv = cv, n_jobs = -1, verbose = 0, scoring=\"r2\").fit(X, y)\n",
    "\n",
    "    rf_reg = RandomForestRegressor(random_state=seed, max_samples=1.0,**grid_cv.best_params_)\n",
    "    rf_reg.fit(X, y)\n",
    "\n",
    "    return rf_reg\n",
    "\n",
    "def eval_rf_model(model, X, y):\n",
    "    y_preds = model.predict(X)\n",
    "    rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "    r2 = r2_score(y, y_preds)\n",
    "    return rmse, r2 "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.4816979765892029, r2_score:  0.24659926129908627\n",
      "Test RMSE: 0.5598341226577759, test_score:  0.1362514962482464\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "save_dir = f\"{data_dir}/exp_data_5/cancer/gdsc\"\n",
    "# rf_model = train_rf_model(seed, X_train, y_train)\n",
    "rf_model = pickle.load(open(f\"{save_dir}/checkpoints/rf_model_s_{seed}.pkl\", \"rb\"))\n",
    "rmse_val, r2_val = eval_rf_model(rf_model, X_val, y_val)\n",
    "rmse_test, r2_test = eval_rf_model(rf_model, X_test, y_test)\n",
    "\n",
    "print(f\"Val RMSE: {rmse_val}, r2_score:  {r2_val}\")\n",
    "print(f\"Test RMSE: {rmse_test}, test_score:  {r2_test}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_feats = 20\n",
    "bnn_feat_idx = np.argsort(igs_mean)[::-1][:num_feats]\n",
    "rf_feat_idx = np.argsort(rf_model.feature_importances_)[::-1][:num_feats]\n",
    "# bnn_feat_idx = dropout_df[\"feats_idx\"][:num_feats].to_list()\n",
    "X_train_df.iloc[:,bnn_feat_idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "# epochs = 2000\n",
    "# num_cycles = 10\n",
    "# batch_size = 80\n",
    "# beta = 0.99\n",
    "# lr_0, disc_lr_0 = 1e-3, 0.5\n",
    "# hidden_sizes = [1000, 500, 300, 10]\n",
    "# sigma = 1.0\n",
    "# eta, mu = 1.0, 1.0\n",
    "# temp = 1e-3\n",
    "epochs = 1000\n",
    "num_cycles = 10\n",
    "batch_size = 80\n",
    "beta = 0.80\n",
    "lr_0, disc_lr_0 = 1e-3, 0.5\n",
    "hidden_sizes = [1000, 500, 300, 100]\n",
    "temp, sigma = 1e-2, 1.0\n",
    "eta, mu = 1.0, 1.0\n",
    "\n",
    "\n",
    "save_dir = f\"{data_dir}/exp_data_5/cancer/gdsc\"\n",
    "\n",
    "def cross_val_run(seeds, X, y):\n",
    "\n",
    "    bnn_rf_bg_dict = {\"seed\":[], \"model\": [], \"test_rmse\": [], \"test_r2_score\": []}\n",
    "\n",
    "    for seed in tqdm(seeds):\n",
    "        X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X, y, random_state=seed, shuffle=True, test_size=0.2)\n",
    "        X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X_train_df, y_train_df, shuffle=True, \n",
    "                                                    random_state=seed, test_size=0.2)\n",
    "        # train_transformer = PowerTransformer().fit(X_train_df)\n",
    "        train_transformer = QuantileTransformer(random_state=seed, output_distribution=\"normal\").fit(X_train_df)\n",
    "        train_transformed = train_transformer.transform(X_train_df)\n",
    "        val_transformed = train_transformer.transform(X_val_df)\n",
    "        test_transformed = train_transformer.transform(X_test_df)\n",
    "\n",
    "        X_train_df = pd.DataFrame(train_transformed, columns=X_train_df.columns)\n",
    "        X_val_df = pd.DataFrame(val_transformed, columns=X_val_df.columns)\n",
    "        X_test_df = pd.DataFrame(test_transformed, columns=X_test_df.columns)\n",
    "        X_train, y_train = X_train_df.values, y_train_df.values\n",
    "        X_val, y_val = X_val_df.values, y_val_df.values\n",
    "        X_test, y_test = X_test_df.values, y_test_df.values\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        data_loader = NumpyLoader(NumpyData(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        if os.path.exists(f\"{save_dir}/configs/bnn_bg_config_s_{seed}.pkl\"):\n",
    "            config_bnn_bg = pickle.load(open(f\"{save_dir}/configs/bnn_bg_config_s_{seed}.pkl\", \"rb\"))\n",
    "        else:\n",
    "            config_bnn_bg, _ = optimize_hyper_parameters(seed, X_train, X_val, y_train, y_val, J, total_time=300, bg=True)\n",
    "            pickle.dump(config_bnn_bg, open(f\"{save_dir}/configs/bnn_bg_config_s_{seed}.pkl\", \"wb\"))\n",
    "\n",
    "        beta = config_bnn_bg[\"beta\"]\n",
    "        lr_0, disc_lr_0 = config_bnn_bg[\"lr\"], config_bnn_bg[\"disc_lr\"]\n",
    "        temp, sigma = config_bnn_bg[\"temp\"], 1.0\n",
    "        eta, mu = config_bnn_bg[\"eta\"], config_bnn_bg[\"mu\"]\n",
    "\n",
    "        bg_bnn_model, states, disc_states = train_bg_bnn_model(seed, data_loader, epochs, num_cycles, num_models, beta, lr_0, disc_lr_0,\n",
    "                                                               hidden_sizes, temp, sigma, eta, mu, J, act_fn=jax.nn.relu)\n",
    "        rmse_test_bnn_bg, r2_test_bnn_bg = score_bg_bnn_model(bg_bnn_model, X_test, y_test, states, disc_states)\n",
    "\n",
    "        if os.path.exists(f\"{save_dir}/configs/bnn_config_s_{seed}.pkl\"):\n",
    "            config_bnn = pickle.load(open(f\"{save_dir}/configs/bnn_config_s_{seed}.pkl\", \"rb\"))\n",
    "        else:\n",
    "            config_bnn, _ = optimize_hyper_parameters(seed, X_train, X_val, y_train, y_val, None, total_time=300, bg=False)\n",
    "            pickle.dump(config_bnn ,open(f\"{save_dir}/configs/bnn_config_s_{seed}.pkl\", \"wb\"))\n",
    "\n",
    "        beta = config_bnn[\"beta\"]\n",
    "        lr_0 = config_bnn[\"lr\"]\n",
    "        temp, sigma = config_bnn[\"temp\"], 1.0\n",
    "\n",
    "        bnn_model, states = train_bnn_model(seed, data_loader, epochs, num_cycles, num_models, beta, lr_0,\n",
    "                                            hidden_sizes, temp, sigma, act_fn=jax.nn.relu)\n",
    "\n",
    "        rmse_test_bnn, r2_test_bnn = score_bnn_model(bnn_model, X_test, y_test, states)\n",
    "\n",
    "        if os.path.exists(f\"{save_dir}/checkpoints/rf_model_s_{seed}.pkl\"):\n",
    "            rf_model = pickle.load(open(f\"{save_dir}/checkpoints/rf_model_s_{seed}.pkl\", \"rb\"))\n",
    "        else:\n",
    "            rf_model = train_rf_model(seed, X_train, y_train)\n",
    "            pickle.dump(rf_model, open(f\"{save_dir}/checkpoints/rf_model_s_{seed}.pkl\", \"wb\"))\n",
    "\n",
    "        rmse_test_rf, r2_test_rf = eval_rf_model(rf_model, X_test, y_test)\n",
    "\n",
    "        bnn_rf_bg_dict[\"seed\"].append(seed)\n",
    "        bnn_rf_bg_dict[\"model\"].append(\"RF\")\n",
    "        bnn_rf_bg_dict[\"test_rmse\"].append(rmse_test_rf)\n",
    "        bnn_rf_bg_dict[\"test_r2_score\"].append(r2_test_rf)\n",
    "\n",
    "        bnn_rf_bg_dict[\"seed\"].append(seed)\n",
    "        bnn_rf_bg_dict[\"model\"].append(\"BNN\")\n",
    "        bnn_rf_bg_dict[\"test_rmse\"].append(rmse_test_bnn)\n",
    "        bnn_rf_bg_dict[\"test_r2_score\"].append(r2_test_bnn)\n",
    "\n",
    "        bnn_rf_bg_dict[\"seed\"].append(seed)\n",
    "        bnn_rf_bg_dict[\"model\"].append(\"BNN + BG\")\n",
    "        bnn_rf_bg_dict[\"test_rmse\"].append(rmse_test_bnn_bg)\n",
    "        bnn_rf_bg_dict[\"test_r2_score\"].append(r2_test_bnn_bg)\n",
    "\n",
    "        print(f\"RF scores - {r2_test_rf}\")\n",
    "        print(f\"BNN scores - {r2_test_bnn}\")\n",
    "        print(f\"BNN + BG scores - {r2_test_bnn_bg}\")\n",
    "\n",
    "        pd.DataFrame(bnn_rf_bg_dict).to_csv(f\"{save_dir}/results/bnn_rf_bg_s_{seed}_v6.csv\", index=False)\n",
    "        \n",
    "    return print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# seeds = [422,261,968,282,739,573,220,413,745,775,482,442,210,423,760,57,769,920,226,196]\n",
    "# bnn_rf_bg_df = cross_val_run(seeds, X_selected, target)\n",
    "save_dir = f\"{data_dir}/exp_data_5/cancer/gdsc\"\n",
    "# bnn_rf_bg_df.to_csv(f\"{save_dir}/bnn_rf_bg_df_v5.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_rf_bg_df = pd.read_csv(f\"{save_dir}/bnn_rf_bg_df_v5.csv\")\n",
    "bnn_rf_bg_df.groupby(\"model\").mean().iloc[:,1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_rf_bg_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_rf_bg_df.groupby([\"model\"])[\"val_r2_score\" ,\"test_r2_score\"].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = [\"RF\", \"BNN\", \"BNN + BG\"]\n",
    "\n",
    "rf_val_r2_scores, rf_test_r2_scores = bnn_rf_bg_df[bnn_rf_bg_df[\"model\"] == \"RF\"][\"val_r2_score\"], \\\n",
    "                                               bnn_rf_bg_df[bnn_rf_bg_df[\"model\"] == \"RF\"][\"test_r2_score\"]\n",
    "\n",
    "bnn_val_r2_scores, bnn_test_r2_scores = bnn_rf_bg_df[bnn_rf_bg_df[\"model\"] == \"BNN\"][\"val_r2_score\"], \\\n",
    "                                                  bnn_rf_bg_df[bnn_rf_bg_df[\"model\"] == \"BNN\"][\"test_r2_score\"]\n",
    "\n",
    "bnn_bg_val_r2_scores, bnn_bg_test_r2_scores = bnn_rf_bg_df[bnn_rf_bg_df[\"model\"] == \"BNN + BG\"][\"val_r2_score\"], \\\n",
    "                                                  bnn_rf_bg_df[bnn_rf_bg_df[\"model\"] == \"BNN + BG\"][\"test_r2_score\"]                                              \n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1.grid(color='grey', axis='y', linestyle='-', linewidth=0.25, alpha=0.5)\n",
    "ax2.grid(color='grey', axis='y', linestyle='-', linewidth=0.25, alpha=0.5)\n",
    "\n",
    "bplot1 = ax1.boxplot([rf_val_r2_scores, bnn_val_r2_scores, bnn_bg_val_r2_scores], showmeans=True, patch_artist=True, labels=labels)\n",
    "bplot2 = ax2.boxplot([rf_test_r2_scores, bnn_test_r2_scores, bnn_bg_test_r2_scores], showmeans=True, patch_artist=True, labels=labels)\n",
    "\n",
    "# fill with colors\n",
    "colors = ['lightyellow' ,'lightblue', 'lightgreen']\n",
    "for bplot in (bplot1, bplot2):\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "fig.suptitle(\"Model Comparison\")\n",
    "plt.legend([bplot1['medians'][0], bplot1['means'][0]], ['median', 'mean'])\n",
    "\n",
    "ax1.set_ylabel(\"$R^{2}$\")\n",
    "ax1.set_title(\"Validation Scores\")\n",
    "\n",
    "ax2.set_ylabel(\"$R^{2}$\")\n",
    "ax2.set_title(\"Test Scores\")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_test_r2_scores.std()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_bg_test_r2_scores.std()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stats.ttest_rel(bnn_bg_test_r2_scores, bnn_test_r2_scores, alternative=\"greater\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "stats.ttest_rel(bnn_bg_test_r2_scores, rf_test_r2_scores, alternative=\"greater\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "# epochs = 2000\n",
    "# num_cycles = 10\n",
    "# batch_size = 80\n",
    "# beta = 0.99\n",
    "# lr_0, disc_lr_0 = 1e-3, 0.5\n",
    "# hidden_sizes = [1000, 500, 300, 10]\n",
    "# sigma = 1.0\n",
    "# eta, mu = 1.0, 1.0\n",
    "# temp = 1e-3\n",
    "epochs = 1000\n",
    "num_cycles = 10\n",
    "batch_size = 80\n",
    "beta = 0.80\n",
    "lr_0, disc_lr_0 = 1e-3, 0.5\n",
    "hidden_sizes = [1000, 500, 300, 100]\n",
    "temp, sigma = 1e-2, 1.0\n",
    "eta, mu = 1.0, 1.0\n",
    "\n",
    "\n",
    "save_dir = f\"{data_dir}/exp_data_5/cancer/gdsc\"\n",
    "\n",
    "def cross_val_run(seeds, X, y):\n",
    "\n",
    "    bnn_rf_bg_dict = {\"seed\":[], \"model\": [], \"test_rmse\": [], \"test_r2_score\": []}\n",
    "\n",
    "    for seed in tqdm(seeds):\n",
    "        X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X, y, random_state=seed, shuffle=True, test_size=0.2)\n",
    "        X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X_train_df, y_train_df, shuffle=True, \n",
    "                                                    random_state=seed, test_size=0.2)\n",
    "        # train_transformer = PowerTransformer().fit(X_train_df)\n",
    "        train_transformer = QuantileTransformer(random_state=seed, output_distribution=\"normal\").fit(X_train_df)\n",
    "        train_transformed = train_transformer.transform(X_train_df)\n",
    "        val_transformed = train_transformer.transform(X_val_df)\n",
    "        test_transformed = train_transformer.transform(X_test_df)\n",
    "\n",
    "        X_train_df = pd.DataFrame(train_transformed, columns=X_train_df.columns)\n",
    "        X_val_df = pd.DataFrame(val_transformed, columns=X_val_df.columns)\n",
    "        X_test_df = pd.DataFrame(test_transformed, columns=X_test_df.columns)\n",
    "        X_train, y_train = X_train_df.values, y_train_df.values\n",
    "        X_val, y_val = X_val_df.values, y_val_df.values\n",
    "        X_test, y_test = X_test_df.values, y_test_df.values\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        data_loader = NumpyLoader(NumpyData(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        if os.path.exists(f\"{save_dir}/configs/bnn_bg_config_s_{seed}.pkl\"):\n",
    "            config_bnn_bg = pickle.load(open(f\"{save_dir}/configs/bnn_bg_config_s_{seed}.pkl\", \"rb\"))\n",
    "        else:\n",
    "            config_bnn_bg, _ = optimize_hyper_parameters(seed, X_train, X_val, y_train, y_val, J, total_time=300, bg=True)\n",
    "            pickle.dump(config_bnn_bg, open(f\"{save_dir}/configs/bnn_bg_config_s_{seed}.pkl\", \"wb\"))\n",
    "\n",
    "        beta = config_bnn_bg[\"beta\"]\n",
    "        lr_0, disc_lr_0 = config_bnn_bg[\"lr\"], config_bnn_bg[\"disc_lr\"]\n",
    "        temp, sigma = config_bnn_bg[\"temp\"], 1.0\n",
    "        eta, mu = config_bnn_bg[\"eta\"], config_bnn_bg[\"mu\"]\n",
    "\n",
    "        bg_bnn_model, states, disc_states = train_bg_bnn_model(seed, data_loader, epochs, num_cycles, num_models, beta, lr_0, disc_lr_0,\n",
    "                                                               hidden_sizes, temp, sigma, eta, mu, J, act_fn=jax.nn.relu)\n",
    "        rmse_test_bnn_bg, r2_test_bnn_bg = score_bg_bnn_model(bg_bnn_model, X_test, y_test, states, disc_states)\n",
    "\n",
    "        if os.path.exists(f\"{save_dir}/configs/bnn_config_s_{seed}.pkl\"):\n",
    "            config_bnn = pickle.load(open(f\"{save_dir}/configs/bnn_config_s_{seed}.pkl\", \"rb\"))\n",
    "        else:\n",
    "            config_bnn, _ = optimize_hyper_parameters(seed, X_train, X_val, y_train, y_val, None, total_time=300, bg=False)\n",
    "            pickle.dump(config_bnn ,open(f\"{save_dir}/configs/bnn_config_s_{seed}.pkl\", \"wb\"))\n",
    "\n",
    "        beta = config_bnn[\"beta\"]\n",
    "        lr_0 = config_bnn[\"lr\"]\n",
    "        temp, sigma = config_bnn[\"temp\"], 1.0\n",
    "\n",
    "        bnn_model, states = train_bnn_model(seed, data_loader, epochs, num_cycles, num_models, beta, lr_0,\n",
    "                                            hidden_sizes, temp, sigma, act_fn=jax.nn.relu)\n",
    "\n",
    "        rmse_test_bnn, r2_test_bnn = score_bnn_model(bnn_model, X_test, y_test, states)\n",
    "\n",
    "        if os.path.exists(f\"{save_dir}/checkpoints/rf_model_s_{seed}.pkl\"):\n",
    "            rf_model = pickle.load(open(f\"{save_dir}/checkpoints/rf_model_s_{seed}.pkl\", \"rb\"))\n",
    "        else:\n",
    "            rf_model = train_rf_model(seed, X_train, y_train)\n",
    "            pickle.dump(rf_model, open(f\"{save_dir}/checkpoints/rf_model_s_{seed}.pkl\", \"wb\"))\n",
    "\n",
    "        rmse_test_rf, r2_test_rf = eval_rf_model(rf_model, X_test, y_test)\n",
    "\n",
    "        bnn_rf_bg_dict[\"seed\"].append(seed)\n",
    "        bnn_rf_bg_dict[\"model\"].append(\"RF\")\n",
    "        bnn_rf_bg_dict[\"test_rmse\"].append(rmse_test_rf)\n",
    "        bnn_rf_bg_dict[\"test_r2_score\"].append(r2_test_rf)\n",
    "\n",
    "        bnn_rf_bg_dict[\"seed\"].append(seed)\n",
    "        bnn_rf_bg_dict[\"model\"].append(\"BNN\")\n",
    "        bnn_rf_bg_dict[\"test_rmse\"].append(rmse_test_bnn)\n",
    "        bnn_rf_bg_dict[\"test_r2_score\"].append(r2_test_bnn)\n",
    "\n",
    "        bnn_rf_bg_dict[\"seed\"].append(seed)\n",
    "        bnn_rf_bg_dict[\"model\"].append(\"BNN + BG\")\n",
    "        bnn_rf_bg_dict[\"test_rmse\"].append(rmse_test_bnn_bg)\n",
    "        bnn_rf_bg_dict[\"test_r2_score\"].append(r2_test_bnn_bg)\n",
    "\n",
    "        print(f\"RF scores - {r2_test_rf}\")\n",
    "        print(f\"BNN scores - {r2_test_bnn}\")\n",
    "        print(f\"BNN + BG scores - {r2_test_bnn_bg}\")\n",
    "\n",
    "        pd.DataFrame(bnn_rf_bg_dict).to_csv(f\"{save_dir}/results/bnn_rf_bg_s_{seed}_v6.csv\", index=False)\n",
    "        \n",
    "    return print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# seeds = [422,261,968,282,739,573,220,413,745,775,482,442,210,423,760,57,769,920,226,196]\n",
    "# bnn_rf_bg_df = cross_val_run(seeds, X_selected, target)\n",
    "save_dir = f\"{data_dir}/exp_data_5/cancer/gdsc\"\n",
    "# bnn_rf_bg_df.to_csv(f\"{save_dir}/bnn_rf_bg_df_v5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m bnn_rf_bg_df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msave_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/bnn_rf_bg_df_v5.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m bnn_rf_bg_df\u001B[38;5;241m.\u001B[39mgroupby(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmean()\u001B[38;5;241m.\u001B[39miloc[:,\u001B[38;5;241m1\u001B[39m:]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "bnn_rf_bg_df = pd.read_csv(f\"{save_dir}/bnn_rf_bg_df_v5.csv\")\n",
    "bnn_rf_bg_df.groupby(\"model\").mean().iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "    seed     model  val_rmse  test_rmse  val_r2_score  test_r2_score  temp  \\\n0    422        RF  0.458455   0.407726      0.190245       0.278194  0.00   \n1    422       BNN  0.519543   0.456036     -0.039927       0.097014  0.01   \n2    422  BNN + BG  0.477475   0.417890      0.121665       0.241759  0.01   \n3    261        RF  0.472298   0.478135      0.219840       0.194252  0.00   \n4    261       BNN  0.485724   0.520985      0.174853       0.043360  0.01   \n5    261  BNN + BG  0.472567   0.477572      0.218949       0.196148  0.01   \n6    968        RF  0.545374   0.495877      0.181778       0.232576  0.00   \n7    968       BNN  0.556830   0.514764      0.147042       0.173002  0.01   \n8    968  BNN + BG  0.537401   0.483294      0.205525       0.271028  0.01   \n9    282        RF  0.488759   0.513891      0.250363       0.242852  0.00   \n10   282       BNN  0.521054   0.512086      0.148026       0.248160  0.01   \n11   282  BNN + BG  0.501927   0.502763      0.209428       0.275287  0.01   \n12   739        RF  0.529245   0.431555      0.272428       0.240847  0.00   \n13   739       BNN  0.539864   0.492227      0.242939       0.012385  0.01   \n14   739  BNN + BG  0.517880   0.453139      0.303340       0.163013  0.01   \n15   573        RF  0.509942   0.486312      0.203136       0.270578  0.00   \n16   573       BNN  0.541113   0.497892      0.102739       0.235426  0.01   \n17   573  BNN + BG  0.517498   0.478599      0.179346       0.293531  0.01   \n18   220        RF  0.508020   0.435753      0.166065       0.278702  0.00   \n19   220       BNN  0.535502   0.472123      0.073401       0.153273  0.01   \n20   220  BNN + BG  0.506187   0.436078      0.172074       0.277627  0.01   \n21   413        RF  0.477802   0.450709      0.272186       0.287558  0.00   \n22   413       BNN  0.552448   0.467357      0.027012       0.233955  0.01   \n23   413  BNN + BG  0.466410   0.442242      0.306478       0.314076  0.01   \n24   745        RF  0.481698   0.559834      0.246599       0.136251  0.00   \n25   745       BNN  0.518943   0.567220      0.125588       0.113309  0.01   \n26   745  BNN + BG  0.504003   0.544558      0.175211       0.182747  0.01   \n27   775        RF  0.430964   0.489098      0.276072       0.262239  0.00   \n28   775       BNN  0.459786   0.499087      0.176003       0.231797  0.01   \n29   775  BNN + BG  0.422368   0.487273      0.304661       0.267734  0.01   \n30   482        RF  0.511746   0.443069      0.232807       0.273397  0.00   \n31   482       BNN  0.533622   0.465731      0.165814       0.197165  0.01   \n32   482  BNN + BG  0.503448   0.440120      0.257486       0.283035  0.01   \n33   442        RF  0.463742   0.505373      0.250663       0.234671  0.00   \n34   442       BNN  0.513720   0.536152      0.080445       0.138608  0.01   \n35   442  BNN + BG  0.472688   0.516288      0.221475       0.201255  0.01   \n36   210        RF  0.502025   0.452632      0.172910       0.180289  0.00   \n37   210       BNN  0.525914   0.464739      0.092322       0.135853  0.01   \n38   210  BNN + BG  0.478569   0.450019      0.248391       0.189726  0.01   \n39   423        RF  0.523776   0.428119      0.207780       0.322094  0.00   \n40   423       BNN  0.538059   0.429426      0.163984       0.317949  0.01   \n41   423  BNN + BG  0.509357   0.419169      0.250796       0.350142  0.01   \n42   760        RF  0.487359   0.542658      0.219646       0.247267  0.00   \n43   760       BNN  0.510685   0.562437      0.143157       0.191393  0.01   \n44   760  BNN + BG  0.483187   0.532681      0.232946       0.274690  0.01   \n45    57        RF  0.461133   0.435339      0.311699       0.236654  0.00   \n46    57       BNN  0.527558   0.458087      0.099123       0.154793  0.01   \n47    57  BNN + BG  0.479504   0.429420      0.255765       0.257269  0.01   \n48   769        RF  0.480337   0.522849      0.276238       0.275037  0.00   \n49   769       BNN  0.479620   0.523794      0.278397       0.272415  0.01   \n50   769  BNN + BG  0.461614   0.502757      0.331563       0.329685  0.01   \n51   920        RF  0.520005   0.475550      0.172223       0.308512  0.00   \n52   920       BNN  0.543770   0.478951      0.094833       0.298585  0.01   \n53   920  BNN + BG  0.517457   0.467326      0.180316       0.332221  0.01   \n54   226        RF  0.462030   0.550391      0.265005       0.218335  0.00   \n55   226       BNN  0.502731   0.550973      0.129808       0.216682  0.01   \n56   226  BNN + BG  0.458598   0.537608      0.275883       0.254224  0.01   \n57   196        RF  0.480546   0.489346      0.327424       0.198808  0.00   \n58   196       BNN  0.444827   0.490103      0.423695       0.196328  0.01   \n59   196  BNN + BG  0.450958   0.475407      0.407697       0.243803  0.01   \n\n     mu  \n0   0.0  \n1   0.0  \n2   1.0  \n3   0.0  \n4   0.0  \n5   1.0  \n6   0.0  \n7   0.0  \n8   1.0  \n9   0.0  \n10  0.0  \n11  1.0  \n12  0.0  \n13  0.0  \n14  1.0  \n15  0.0  \n16  0.0  \n17  1.0  \n18  0.0  \n19  0.0  \n20  1.0  \n21  0.0  \n22  0.0  \n23  1.0  \n24  0.0  \n25  0.0  \n26  1.0  \n27  0.0  \n28  0.0  \n29  1.0  \n30  0.0  \n31  0.0  \n32  1.0  \n33  0.0  \n34  0.0  \n35  1.0  \n36  0.0  \n37  0.0  \n38  1.0  \n39  0.0  \n40  0.0  \n41  1.0  \n42  0.0  \n43  0.0  \n44  1.0  \n45  0.0  \n46  0.0  \n47  1.0  \n48  0.0  \n49  0.0  \n50  1.0  \n51  0.0  \n52  0.0  \n53  1.0  \n54  0.0  \n55  0.0  \n56  1.0  \n57  0.0  \n58  0.0  \n59  1.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>seed</th>\n      <th>model</th>\n      <th>val_rmse</th>\n      <th>test_rmse</th>\n      <th>val_r2_score</th>\n      <th>test_r2_score</th>\n      <th>temp</th>\n      <th>mu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>422</td>\n      <td>RF</td>\n      <td>0.458455</td>\n      <td>0.407726</td>\n      <td>0.190245</td>\n      <td>0.278194</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>422</td>\n      <td>BNN</td>\n      <td>0.519543</td>\n      <td>0.456036</td>\n      <td>-0.039927</td>\n      <td>0.097014</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>422</td>\n      <td>BNN + BG</td>\n      <td>0.477475</td>\n      <td>0.417890</td>\n      <td>0.121665</td>\n      <td>0.241759</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>261</td>\n      <td>RF</td>\n      <td>0.472298</td>\n      <td>0.478135</td>\n      <td>0.219840</td>\n      <td>0.194252</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>261</td>\n      <td>BNN</td>\n      <td>0.485724</td>\n      <td>0.520985</td>\n      <td>0.174853</td>\n      <td>0.043360</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>261</td>\n      <td>BNN + BG</td>\n      <td>0.472567</td>\n      <td>0.477572</td>\n      <td>0.218949</td>\n      <td>0.196148</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>968</td>\n      <td>RF</td>\n      <td>0.545374</td>\n      <td>0.495877</td>\n      <td>0.181778</td>\n      <td>0.232576</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>968</td>\n      <td>BNN</td>\n      <td>0.556830</td>\n      <td>0.514764</td>\n      <td>0.147042</td>\n      <td>0.173002</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>968</td>\n      <td>BNN + BG</td>\n      <td>0.537401</td>\n      <td>0.483294</td>\n      <td>0.205525</td>\n      <td>0.271028</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>282</td>\n      <td>RF</td>\n      <td>0.488759</td>\n      <td>0.513891</td>\n      <td>0.250363</td>\n      <td>0.242852</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>282</td>\n      <td>BNN</td>\n      <td>0.521054</td>\n      <td>0.512086</td>\n      <td>0.148026</td>\n      <td>0.248160</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>282</td>\n      <td>BNN + BG</td>\n      <td>0.501927</td>\n      <td>0.502763</td>\n      <td>0.209428</td>\n      <td>0.275287</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>739</td>\n      <td>RF</td>\n      <td>0.529245</td>\n      <td>0.431555</td>\n      <td>0.272428</td>\n      <td>0.240847</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>739</td>\n      <td>BNN</td>\n      <td>0.539864</td>\n      <td>0.492227</td>\n      <td>0.242939</td>\n      <td>0.012385</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>739</td>\n      <td>BNN + BG</td>\n      <td>0.517880</td>\n      <td>0.453139</td>\n      <td>0.303340</td>\n      <td>0.163013</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>573</td>\n      <td>RF</td>\n      <td>0.509942</td>\n      <td>0.486312</td>\n      <td>0.203136</td>\n      <td>0.270578</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>573</td>\n      <td>BNN</td>\n      <td>0.541113</td>\n      <td>0.497892</td>\n      <td>0.102739</td>\n      <td>0.235426</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>573</td>\n      <td>BNN + BG</td>\n      <td>0.517498</td>\n      <td>0.478599</td>\n      <td>0.179346</td>\n      <td>0.293531</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>220</td>\n      <td>RF</td>\n      <td>0.508020</td>\n      <td>0.435753</td>\n      <td>0.166065</td>\n      <td>0.278702</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>220</td>\n      <td>BNN</td>\n      <td>0.535502</td>\n      <td>0.472123</td>\n      <td>0.073401</td>\n      <td>0.153273</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>220</td>\n      <td>BNN + BG</td>\n      <td>0.506187</td>\n      <td>0.436078</td>\n      <td>0.172074</td>\n      <td>0.277627</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>413</td>\n      <td>RF</td>\n      <td>0.477802</td>\n      <td>0.450709</td>\n      <td>0.272186</td>\n      <td>0.287558</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>413</td>\n      <td>BNN</td>\n      <td>0.552448</td>\n      <td>0.467357</td>\n      <td>0.027012</td>\n      <td>0.233955</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>413</td>\n      <td>BNN + BG</td>\n      <td>0.466410</td>\n      <td>0.442242</td>\n      <td>0.306478</td>\n      <td>0.314076</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>745</td>\n      <td>RF</td>\n      <td>0.481698</td>\n      <td>0.559834</td>\n      <td>0.246599</td>\n      <td>0.136251</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>745</td>\n      <td>BNN</td>\n      <td>0.518943</td>\n      <td>0.567220</td>\n      <td>0.125588</td>\n      <td>0.113309</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>745</td>\n      <td>BNN + BG</td>\n      <td>0.504003</td>\n      <td>0.544558</td>\n      <td>0.175211</td>\n      <td>0.182747</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>775</td>\n      <td>RF</td>\n      <td>0.430964</td>\n      <td>0.489098</td>\n      <td>0.276072</td>\n      <td>0.262239</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>775</td>\n      <td>BNN</td>\n      <td>0.459786</td>\n      <td>0.499087</td>\n      <td>0.176003</td>\n      <td>0.231797</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>775</td>\n      <td>BNN + BG</td>\n      <td>0.422368</td>\n      <td>0.487273</td>\n      <td>0.304661</td>\n      <td>0.267734</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>482</td>\n      <td>RF</td>\n      <td>0.511746</td>\n      <td>0.443069</td>\n      <td>0.232807</td>\n      <td>0.273397</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>482</td>\n      <td>BNN</td>\n      <td>0.533622</td>\n      <td>0.465731</td>\n      <td>0.165814</td>\n      <td>0.197165</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>482</td>\n      <td>BNN + BG</td>\n      <td>0.503448</td>\n      <td>0.440120</td>\n      <td>0.257486</td>\n      <td>0.283035</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>442</td>\n      <td>RF</td>\n      <td>0.463742</td>\n      <td>0.505373</td>\n      <td>0.250663</td>\n      <td>0.234671</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>442</td>\n      <td>BNN</td>\n      <td>0.513720</td>\n      <td>0.536152</td>\n      <td>0.080445</td>\n      <td>0.138608</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>442</td>\n      <td>BNN + BG</td>\n      <td>0.472688</td>\n      <td>0.516288</td>\n      <td>0.221475</td>\n      <td>0.201255</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>210</td>\n      <td>RF</td>\n      <td>0.502025</td>\n      <td>0.452632</td>\n      <td>0.172910</td>\n      <td>0.180289</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>210</td>\n      <td>BNN</td>\n      <td>0.525914</td>\n      <td>0.464739</td>\n      <td>0.092322</td>\n      <td>0.135853</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>210</td>\n      <td>BNN + BG</td>\n      <td>0.478569</td>\n      <td>0.450019</td>\n      <td>0.248391</td>\n      <td>0.189726</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>423</td>\n      <td>RF</td>\n      <td>0.523776</td>\n      <td>0.428119</td>\n      <td>0.207780</td>\n      <td>0.322094</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>423</td>\n      <td>BNN</td>\n      <td>0.538059</td>\n      <td>0.429426</td>\n      <td>0.163984</td>\n      <td>0.317949</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>423</td>\n      <td>BNN + BG</td>\n      <td>0.509357</td>\n      <td>0.419169</td>\n      <td>0.250796</td>\n      <td>0.350142</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>760</td>\n      <td>RF</td>\n      <td>0.487359</td>\n      <td>0.542658</td>\n      <td>0.219646</td>\n      <td>0.247267</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>760</td>\n      <td>BNN</td>\n      <td>0.510685</td>\n      <td>0.562437</td>\n      <td>0.143157</td>\n      <td>0.191393</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>760</td>\n      <td>BNN + BG</td>\n      <td>0.483187</td>\n      <td>0.532681</td>\n      <td>0.232946</td>\n      <td>0.274690</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>57</td>\n      <td>RF</td>\n      <td>0.461133</td>\n      <td>0.435339</td>\n      <td>0.311699</td>\n      <td>0.236654</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>57</td>\n      <td>BNN</td>\n      <td>0.527558</td>\n      <td>0.458087</td>\n      <td>0.099123</td>\n      <td>0.154793</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>57</td>\n      <td>BNN + BG</td>\n      <td>0.479504</td>\n      <td>0.429420</td>\n      <td>0.255765</td>\n      <td>0.257269</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>769</td>\n      <td>RF</td>\n      <td>0.480337</td>\n      <td>0.522849</td>\n      <td>0.276238</td>\n      <td>0.275037</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>769</td>\n      <td>BNN</td>\n      <td>0.479620</td>\n      <td>0.523794</td>\n      <td>0.278397</td>\n      <td>0.272415</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>769</td>\n      <td>BNN + BG</td>\n      <td>0.461614</td>\n      <td>0.502757</td>\n      <td>0.331563</td>\n      <td>0.329685</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>920</td>\n      <td>RF</td>\n      <td>0.520005</td>\n      <td>0.475550</td>\n      <td>0.172223</td>\n      <td>0.308512</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>920</td>\n      <td>BNN</td>\n      <td>0.543770</td>\n      <td>0.478951</td>\n      <td>0.094833</td>\n      <td>0.298585</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>920</td>\n      <td>BNN + BG</td>\n      <td>0.517457</td>\n      <td>0.467326</td>\n      <td>0.180316</td>\n      <td>0.332221</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>226</td>\n      <td>RF</td>\n      <td>0.462030</td>\n      <td>0.550391</td>\n      <td>0.265005</td>\n      <td>0.218335</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>226</td>\n      <td>BNN</td>\n      <td>0.502731</td>\n      <td>0.550973</td>\n      <td>0.129808</td>\n      <td>0.216682</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>226</td>\n      <td>BNN + BG</td>\n      <td>0.458598</td>\n      <td>0.537608</td>\n      <td>0.275883</td>\n      <td>0.254224</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>196</td>\n      <td>RF</td>\n      <td>0.480546</td>\n      <td>0.489346</td>\n      <td>0.327424</td>\n      <td>0.198808</td>\n      <td>0.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>196</td>\n      <td>BNN</td>\n      <td>0.444827</td>\n      <td>0.490103</td>\n      <td>0.423695</td>\n      <td>0.196328</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>196</td>\n      <td>BNN + BG</td>\n      <td>0.450958</td>\n      <td>0.475407</td>\n      <td>0.407697</td>\n      <td>0.243803</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_rf_bg_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bnn_rf_bg_df.groupby([\"model\"])[\"val_r2_score\" ,\"test_r2_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 864x432 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [\"RF\", \"BNN\", \"BNN + BG\"]\n",
    "\n",
    "rf_val_r2_scores, rf_test_r2_scores = bnn_rf_bg_df[bnn_rf_bg_df[\"model\"] == \"RF\"][\"val_r2_score\"], \\\n",
    "                                               bnn_rf_bg_df[bnn_rf_bg_df[\"model\"] == \"RF\"][\"test_r2_score\"]\n",
    "\n",
    "bnn_val_r2_scores, bnn_test_r2_scores = bnn_rf_bg_df[bnn_rf_bg_df[\"model\"] == \"BNN\"][\"val_r2_score\"], \\\n",
    "                                                  bnn_rf_bg_df[bnn_rf_bg_df[\"model\"] == \"BNN\"][\"test_r2_score\"]\n",
    "\n",
    "bnn_bg_val_r2_scores, bnn_bg_test_r2_scores = bnn_rf_bg_df[bnn_rf_bg_df[\"model\"] == \"BNN + BG\"][\"val_r2_score\"], \\\n",
    "                                                  bnn_rf_bg_df[bnn_rf_bg_df[\"model\"] == \"BNN + BG\"][\"test_r2_score\"]                                              \n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1.grid(color='grey', axis='y', linestyle='-', linewidth=0.25, alpha=0.5)\n",
    "ax2.grid(color='grey', axis='y', linestyle='-', linewidth=0.25, alpha=0.5)\n",
    "\n",
    "bplot1 = ax1.boxplot([rf_val_r2_scores, bnn_val_r2_scores, bnn_bg_val_r2_scores], showmeans=True, patch_artist=True, labels=labels)\n",
    "bplot2 = ax2.boxplot([rf_test_r2_scores, bnn_test_r2_scores, bnn_bg_test_r2_scores], showmeans=True, patch_artist=True, labels=labels)\n",
    "\n",
    "# fill with colors\n",
    "colors = ['lightyellow' ,'lightblue', 'lightgreen']\n",
    "for bplot in (bplot1, bplot2):\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "fig.suptitle(\"Model Comparison\")\n",
    "plt.legend([bplot1['medians'][0], bplot1['means'][0]], ['median', 'mean'])\n",
    "\n",
    "ax1.set_ylabel(\"$R^{2}$\")\n",
    "ax1.set_title(\"Validation Scores\")\n",
    "\n",
    "ax2.set_ylabel(\"$R^{2}$\")\n",
    "ax2.set_title(\"Test Scores\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.04499079968112092"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_test_r2_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.05241289956379923"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_bg_test_r2_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Ttest_relResult(statistic=8.477441507332248, pvalue=3.505409890772487e-08)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_rel(bnn_bg_test_r2_scores, bnn_test_r2_scores, alternative=\"greater\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Ttest_relResult(statistic=1.9510907439469571, pvalue=0.032975061812330744)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "stats.ttest_rel(bnn_bg_test_r2_scores, rf_test_r2_scores, alternative=\"greater\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### GP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gplearn.genetic import SymbolicTransformer, SymbolicClassifier, SymbolicRegressor\n",
    "from gplearn.functions import make_function\n",
    "import operator\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_best_programs(gp, num_models, classifier=True, ascending=True, sort_fit=\"OOB_fitness\"):\n",
    "    gp_dict = {'Gen': [], \"Ind\": [], \"Fitness\": [], 'OOB_fitness': [], \"Equation\": []}\n",
    "\n",
    "    if classifier:\n",
    "        for idGen in range(len(gp._programs)):\n",
    "            for idPopulation in range(gp.population_size):\n",
    "                gp_dict[\"Gen\"].append(idGen)\n",
    "                gp_dict[\"Ind\"].append(idPopulation)\n",
    "                gp_dict[\"Fitness\"].append(gp._programs[idGen][idPopulation].fitness_)\n",
    "                gp_dict[\"OOB_fitness\"].append(gp._programs[idGen][idPopulation].oob_fitness_)\n",
    "                gp_dict[\"Equation\"].append(str(gp._programs[idGen][idPopulation]))\n",
    "    else:\n",
    "        for idx, prog in enumerate(gp._programs[-1]):\n",
    "                gp_dict[\"Gen\"].append(-1)\n",
    "                gp_dict[\"Ind\"].append(idx)\n",
    "                gp_dict[\"Fitness\"].append(prog.fitness_)\n",
    "                gp_dict[\"OOB_fitness\"].append(prog.oob_fitness_)\n",
    "                gp_dict[\"Equation\"].append(str(prog))\n",
    "\n",
    "    gp_df = pd.DataFrame(gp_dict).sort_values(sort_fit, ascending=ascending)[:num_models]\n",
    "    programs = []\n",
    "    for i in range(num_models):\n",
    "        gen, ind = int(gp_df.iloc[i][\"Gen\"]), int(gp_df.iloc[i][\"Ind\"])\n",
    "        programs.append(gp._programs[gen][ind])\n",
    "\n",
    "    return programs, gp_df\n",
    "\n",
    "\n",
    "def gp_transform(est, X, classifier=False, num_models=100, sort_fit=\"Fitness\"):\n",
    "    if classifier or (sort_fit == \"OOB_fitness\"):\n",
    "        programs, gp_df = get_best_programs(est, num_models, classifier, sort_fit=sort_fit, ascending=classifier)\n",
    "        out = np.zeros((X.shape[0], len(programs)))\n",
    "        for i, prog in enumerate(programs):\n",
    "            out[:, i] = prog.execute(X)\n",
    "\n",
    "        return out, gp_df\n",
    "    else:\n",
    "        return est.transform(X), None\n",
    "\n",
    "function_set = ['add', 'sub', 'mul', 'div', 'sqrt', 'log',\n",
    "                'abs', 'neg', 'inv', 'max', 'min', 'sin', 'cos']\n",
    "\n",
    "def train_linear_model(seed, X_train, X_test, y_train, y_test):\n",
    "    cv = KFold(n_splits=3, random_state=seed, shuffle=True)\n",
    "    param_grid = {\"alpha\": np.logspace(-2, 2, 20)}\n",
    "    grid_cv = GridSearchCV(estimator=Ridge(max_iter=10000), param_grid=param_grid, \n",
    "                                            verbose=0, scoring=\"r2\", cv=cv).fit(X_train, y_train)\n",
    "    lin_model = Ridge(max_iter=10000, **grid_cv.best_params_)\n",
    "    lin_model.fit(X_train, y_train)\n",
    "    y_test_pred = lin_model.predict(X_test)\n",
    "\n",
    "    test_rmse_score = np.sqrt(np.mean((y_test - y_test_pred)**2))\n",
    "    test_r2_score = r2_score(y_test, y_test_pred)\n",
    "    test_pearson, test_pval = stats.pearsonr(y_test, y_test_pred)\n",
    "\n",
    "    return test_rmse_score, test_r2_score, test_pearson, test_pval\n",
    "\n",
    "def train_gp(seed, X_train, X_train_2, X_test, y_train, y_train_2, y_test, num_models=5, sort_fit=\"OOB_fitness\", verbose=0, num_gen=100, \n",
    "                        p_cxvr=0.8, p_subt_mut=0.1, p_hmut=0.05, p_pmut=0.1, subsample=0.8, complexity_coef=0.05):\n",
    "    gp_est = SymbolicTransformer(population_size=1000, hall_of_fame=200, n_components=50, generations=num_gen,\n",
    "                                 function_set=function_set,\n",
    "                           p_crossover=p_cxvr, p_subtree_mutation=p_subt_mut,\n",
    "                           p_hoist_mutation=p_hmut, p_point_mutation=p_pmut,\n",
    "                           max_samples=subsample, verbose=verbose,\n",
    "                           parsimony_coefficient=complexity_coef, random_state=seed)\n",
    "\n",
    "    gp_est.fit(X_train, y_train)\n",
    "\n",
    "    gp_features_train, gp_train_df = gp_transform(gp_est, X_train_2, classifier=False, sort_fit=sort_fit, num_models=num_models)\n",
    "    gp_features_test, gp_test_df = gp_transform(gp_est, X_test, classifier=False, sort_fit=sort_fit, num_models=num_models)\n",
    "\n",
    "    X_train_comb = np.concatenate([X_train_2, gp_features_train], axis=1)\n",
    "    X_test_comb = np.concatenate([X_test, gp_features_test], axis=1)\n",
    "\n",
    "\n",
    "    test_rmse_score, test_r2_score, test_pearson, test_pval = train_linear_model(seed, X_train_comb, X_test_comb, y_train_2, y_test)\n",
    "\n",
    "    return test_rmse_score, test_r2_score, test_pearson, test_pval, gp_test_df\n",
    "\n",
    "def train_gp_v2(seed, X_train, X_test, y_train,y_test, verbose=0, num_gen=100,\n",
    "             p_cxvr=0.8, p_subt_mut=0.1, p_hmut=0.05, p_pmut=0.1, subsample=0.8, complexity_coef=0.05):\n",
    "    gp_est = SymbolicRegressor(population_size=1000, generations=num_gen,\n",
    "                                 function_set=function_set,\n",
    "                                 p_crossover=p_cxvr, p_subtree_mutation=p_subt_mut,\n",
    "                                 p_hoist_mutation=p_hmut, p_point_mutation=p_pmut,\n",
    "                                 max_samples=subsample, verbose=verbose,\n",
    "                                 parsimony_coefficient=complexity_coef, random_state=seed)\n",
    "\n",
    "    gp_est.fit(X_train, y_train)\n",
    "\n",
    "    y_test_pred = gp_est.predict(X_test)\n",
    "    test_rmse_score = np.sqrt(np.mean((y_test - y_test_pred)**2))\n",
    "    test_r2_score = r2_score(y_test, y_test_pred)\n",
    "    test_pearson, test_pval = stats.pearsonr(y_test, y_test_pred)\n",
    "\n",
    "    return test_rmse_score, test_r2_score, test_pearson, test_pval, gp_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "import optuna\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "epochs = 200\n",
    "hidden_sizes = [1000, 500, 300, 100]\n",
    "num_cycles = 10\n",
    "batch_size = 80\n",
    "lr_0 = 1e-3\n",
    "sigma = 1.0\n",
    "eta = 1.0\n",
    "num_feats = [10, 20, 30, 40, 50]\n",
    "\n",
    "num_gp_models = 5\n",
    "save_dir = f\"{data_dir}/exp_data_5/cancer/gdsc\"\n",
    "\n",
    "\n",
    "def cross_val_gp_train(seeds, X, y):\n",
    "    for seed in tqdm(seeds):\n",
    "        rng_key = jax.random.PRNGKey(seed)\n",
    "        bnn_rf_gp_dict = {\"seed\": [], \"model\": [], \"num_feats\": [], \"test_rmse_score\": [],\n",
    "                          \"test_r2_score\": [], \"test_pcc\": [], \"test_pcc_pval\": []}\n",
    "\n",
    "        bnn_rf_dict = {\"seed\": [], \"model\": [], \"test_rmse_score\": [],\n",
    "                       \"test_r2_score\": []}\n",
    "        X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X, y, random_state=seed, shuffle=True,\n",
    "                                                                        test_size=0.2)\n",
    "        X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X_train_df, y_train_df, shuffle=True,\n",
    "                                                                      random_state=seed, test_size=0.2)\n",
    "        train_transformer = QuantileTransformer(random_state=seed, output_distribution=\"normal\").fit(X_train_df)\n",
    "        train_transformed = train_transformer.transform(X_train_df)\n",
    "        val_transformed = train_transformer.transform(X_val_df)\n",
    "        test_transformed = train_transformer.transform(X_test_df)\n",
    "\n",
    "        X_train_df = pd.DataFrame(train_transformed, columns=X_train_df.columns)\n",
    "        X_val_df = pd.DataFrame(val_transformed, columns=X_val_df.columns)\n",
    "        X_test_df = pd.DataFrame(test_transformed, columns=X_test_df.columns)\n",
    "        X_train, y_train = X_train_df.values, y_train_df.values\n",
    "        X_val, y_val = X_val_df.values, y_val_df.values\n",
    "        X_test, y_test = X_test_df.values, y_test_df.values\n",
    "\n",
    "        # Train BNN and RF models and select features\n",
    "\n",
    "        # if os.path.exists(f\"{save_dir}/dropout/bnn_dropout_loss_s_{seed}_v2.csv\"):\n",
    "        #     bnn_dropout_loss_df = pd.read_csv(f\"{save_dir}/dropout/bnn_dropout_loss_s_{seed}_v2.csv\")\n",
    "        # else:\n",
    "        #     bg_bnn_model, states, disc_states = train_bg_bnn_model(seed, X_train, y_train, epochs, num_cycles, beta, lr_0,\n",
    "        #                                                            disc_lr_0, batch_size, hidden_sizes, temp, sigma, eta, mu, J)\n",
    "        #     bnn_dropout_loss_df = get_feats_dropout_loss(bg_bnn_model, states, disc_states, X_train, y_train)\n",
    "        #     bnn_dropout_loss_df.to_csv(f\"{save_dir}/dropout/bnn_dropout_loss_s_{seed}_v2.csv\", index=False)\n",
    "\n",
    "        # if os.path.exists(f\"{save_dir}/configs/bnn_config_ft_sel_s_{seed}.pkl\"):\n",
    "        #     bnn_config = pickle.load(open(f\"{save_dir}/configs/bnn_config_ft_sel_s_{seed}.pkl\", \"rb\"))\n",
    "        # else:\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        sampler = optuna.samplers.TPESampler(seed=seed)\n",
    "        study = optuna.create_study(sampler=sampler)\n",
    "        study.optimize(lambda trial: objective(trial, seed, X_train, X_val, y_train, y_val, J, epochs, num_cycles,\n",
    "                                            batch_size, hidden_sizes, lr_0), timeout=300)\n",
    "        bnn_config = study.best_params\n",
    "        pickle.dump(bnn_config, open(f\"{save_dir}/configs/bnn_config_bg_s_{seed}_v2.pkl\", \"wb\"))\n",
    "\n",
    "        disc_lr_0 = bnn_config[\"disc_lr\"]\n",
    "        temp  = bnn_config[\"temp\"]\n",
    "        mu = bnn_config[\"mu\"]\n",
    "        beta = bnn_config[\"beta\"]\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        data_loader = NumpyLoader(NumpyData(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        bg_bnn_model, states, disc_states = train_bg_bnn_model(seed, data_loader, epochs, num_cycles, beta, lr_0, disc_lr_0,\n",
    "                                                               hidden_sizes, temp, sigma, eta, mu, J, act_fn=jax.nn.swish)\n",
    "\n",
    "        bnn_rmse_test, bnn_r2_test = score_bg_bnn_model(bg_bnn_model, X_test, y_test, states, disc_states)\n",
    "\n",
    "        # # gamma_mean = np.mean(np.array(disc_states), axis=0)\n",
    "        # igs = np.zeros_like(X_val)\n",
    "        # for i in range(X_val.shape[0]):\n",
    "        #     igs[i] = integrated_gradients(bg_bnn_model, states, disc_states, X_val[i], y_val[i], 100)\n",
    "        #\n",
    "        # igs = jax.vmap(integrated_gradients, in_axes=(None, None, None, 0, 0, None))(bg_bnn_model, states, disc_states, X_val, y_val, 100)\n",
    "        # igs_mean = np.mean(np.abs(igs), axis=0)\n",
    "        # bnn_feat_idx = np.argsort(gamma_mean)[::-1][:num_feats]\n",
    "\n",
    "        if os.path.exists(f\"{save_dir}/checkpoints/rf_model_s_{seed}.pkl\"):\n",
    "            rf_model = pickle.load(open(f\"{save_dir}/checkpoints/rf_model_s_{seed}.pkl\", \"rb\"))\n",
    "        else:\n",
    "            rf_model = train_rf_model(seed, X_train, y_train)\n",
    "            pickle.dump(rf_model, open(f\"{save_dir}/checkpoints/rf_model_s_{seed}.pkl\", \"wb\"))\n",
    "\n",
    "        rf_rmse_test, rf_r2_test = eval_rf_model(rf_model, X_test, y_test)\n",
    "\n",
    "\n",
    "        bnn_rf_dict[\"seed\"].append(seed)\n",
    "        bnn_rf_dict[\"model\"].append(\"RF\")\n",
    "        bnn_rf_dict[\"test_rmse_score\"].append(rf_rmse_test)\n",
    "        bnn_rf_dict[\"test_r2_score\"].append(rf_r2_test)\n",
    "\n",
    "\n",
    "        bnn_rf_dict[\"seed\"].append(seed)\n",
    "        bnn_rf_dict[\"model\"].append(\"BNN + BG\")\n",
    "        bnn_rf_dict[\"test_rmse_score\"].append(bnn_rmse_test)\n",
    "        bnn_rf_dict[\"test_r2_score\"].append(bnn_r2_test)\n",
    "\n",
    "        # for num_feat in num_feats:\n",
    "        #     bnn_feat_idx = np.argsort(gamma_mean)[::-1][:num_feat]\n",
    "        #     rf_feat_idx = np.argsort(rf_model.feature_importances_)[::-1][:num_feat]\n",
    "        #     X_train_2, y_train_2 = jax.random.choice(rng_key, X_train, shape=(X_val.shape[0],), replace=False), jax.random.choice(rng_key, y_train, shape=(X_val.shape[0],), replace=False)\n",
    "        #\n",
    "        #     # Create new validation for training Linear Regression models same size as outer validation\n",
    "        #     X_gp_train_rf, X_gp_train_2_rf, X_gp_val_rf, X_gp_test_rf = X_val[:,rf_feat_idx], X_train_2[:,rf_feat_idx], X_train[:,rf_feat_idx], X_test[:,rf_feat_idx]\n",
    "        #     X_gp_train_bnn, X_gp_train_2_bnn, X_gp_val_bnn, X_gp_test_bnn = X_val[:,bnn_feat_idx], X_train_2[:,bnn_feat_idx], X_train[:,bnn_feat_idx], X_test[:,bnn_feat_idx]\n",
    "        #\n",
    "        #     y_train_gp, y_train_gp_2, y_val_gp, y_test_gp = y_val, y_train_2, y_train, y_test\n",
    "        #\n",
    "        #     ## Run GP\n",
    "        #     bnn_test_rmse_score, bnn_test_r2_score, bnn_test_pcc, bnn_test_pval = train_linear_model(seed, X_gp_train_2_bnn, X_gp_test_bnn, y_train_gp_2, y_test_gp)\n",
    "        #     rf_test_rmse_score, rf_test_r2_score, rf_test_pcc, rf_test_pval = train_linear_model(seed, X_gp_train_2_rf, X_gp_test_rf, y_train_gp_2, y_test_gp)\n",
    "        #\n",
    "        #\n",
    "        #     bnn_gp_test_rmse_score, bnn_gp_test_r2_score, bnn_gp_test_pcc, bnn_gp_test_pval, _ = train_gp(seed, X_gp_train_bnn, X_gp_train_2_bnn, X_gp_test_bnn,\n",
    "        #                                                                                                   y_train_gp, y_train_gp_2, y_test_gp, num_models=1, verbose=0, p_cxvr=0.8, p_subt_mut=0.05,\n",
    "        #                                                                                                   p_hmut=0.05, p_pmut=0.05, complexity_coef=0.001)\n",
    "        #\n",
    "        #\n",
    "        #     rf_gp_test_rmse_score, rf_gp_test_r2_score, rf_gp_test_pcc, rf_gp_test_pval, _ = train_gp(seed, X_gp_train_rf, X_gp_train_2_rf, X_gp_test_rf,\n",
    "        #                                                                                               y_train_gp, y_train_gp_2, y_test_gp, num_models=1, verbose=0, p_cxvr=0.8, p_subt_mut=0.05,\n",
    "        #                                                                                               p_hmut=0.05, p_pmut=0.05, complexity_coef=0.001)\n",
    "        #\n",
    "        #\n",
    "        #     # ======================== Save scores for compressed models =========================\n",
    "        #\n",
    "        #     bnn_rf_gp_dict[\"seed\"].append(seed)\n",
    "        #     bnn_rf_gp_dict[\"model\"].append(\"RF + LR\")\n",
    "        #     bnn_rf_gp_dict[\"num_feats\"].append(num_feat)\n",
    "        #     bnn_rf_gp_dict[\"test_rmse_score\"].append(rf_test_rmse_score)\n",
    "        #     bnn_rf_gp_dict[\"test_r2_score\"].append(rf_test_r2_score)\n",
    "        #     bnn_rf_gp_dict[\"test_pcc\"].append(rf_test_pcc)\n",
    "        #     bnn_rf_gp_dict[\"test_pcc_pval\"].append(rf_test_pval)\n",
    "        #\n",
    "        #     bnn_rf_gp_dict[\"seed\"].append(seed)\n",
    "        #     bnn_rf_gp_dict[\"model\"].append(\"RF + LR + GP\")\n",
    "        #     bnn_rf_gp_dict[\"num_feats\"].append(num_feat)\n",
    "        #     bnn_rf_gp_dict[\"test_rmse_score\"].append(rf_gp_test_rmse_score)\n",
    "        #     bnn_rf_gp_dict[\"test_r2_score\"].append(rf_gp_test_r2_score)\n",
    "        #     bnn_rf_gp_dict[\"test_pcc\"].append(rf_gp_test_pcc)\n",
    "        #     bnn_rf_gp_dict[\"test_pcc_pval\"].append(rf_gp_test_pval)\n",
    "        #\n",
    "        #     bnn_rf_gp_dict[\"seed\"].append(seed)\n",
    "        #     bnn_rf_gp_dict[\"model\"].append(\"BNN + LR\")\n",
    "        #     bnn_rf_gp_dict[\"num_feats\"].append(num_feat)\n",
    "        #     bnn_rf_gp_dict[\"test_rmse_score\"].append(bnn_test_rmse_score)\n",
    "        #     bnn_rf_gp_dict[\"test_r2_score\"].append(bnn_test_r2_score)\n",
    "        #     bnn_rf_gp_dict[\"test_pcc\"].append(bnn_test_pcc)\n",
    "        #     bnn_rf_gp_dict[\"test_pcc_pval\"].append(bnn_test_pval)\n",
    "        #\n",
    "        #\n",
    "        #     bnn_rf_gp_dict[\"seed\"].append(seed)\n",
    "        #     bnn_rf_gp_dict[\"model\"].append(\"BNN + LR + GP\")\n",
    "        #     bnn_rf_gp_dict[\"num_feats\"].append(num_feat)\n",
    "        #     bnn_rf_gp_dict[\"test_rmse_score\"].append(bnn_gp_test_rmse_score)\n",
    "        #     bnn_rf_gp_dict[\"test_r2_score\"].append(bnn_gp_test_r2_score)\n",
    "        #     bnn_rf_gp_dict[\"test_pcc\"].append(bnn_gp_test_pcc)\n",
    "        #     bnn_rf_gp_dict[\"test_pcc_pval\"].append(bnn_gp_test_pval)\n",
    "        #\n",
    "        #\n",
    "        #     # print(f\"Num Feats {num_feat} - RF scores - LR: {rf_test_r2_score} GP: {rf_gp_test_r2_score}\")\n",
    "        #     # print(f\"Num Feats {num_feat} - BNN scores - LR: {bnn_test_r2_score} GP: {bnn_gp_test_r2_score}\")\n",
    "        #\n",
    "        # pd.DataFrame(bnn_rf_gp_dict).to_csv(f\"{save_dir}/results/bnn_rf_gp_s_{seed}_gamma_mean_v7.csv\", index=False)\n",
    "        pd.DataFrame(bnn_rf_dict).to_csv(f\"{save_dir}/results/bnn_bg_rf_s_{seed}_v5.csv\", index=False)\n",
    "\n",
    "        print(f\"RF - test_rmse: {rf_rmse_test}, test_r2_score: {rf_r2_test}\")\n",
    "        print(f\"BNN - test_rmse: {bnn_rmse_test}, test_r2_score: {bnn_r2_test}\")\n",
    "\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "seeds = [422,261,968,282,739,573,220,413,745,775,482,442,210,423,760,57,769,920,226,196]\n",
    "# seeds = [769,920,226,196]\n",
    "bnn_rf_gp_dict = cross_val_gp_train(seeds, X_selected, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                         test_rmse_score  test_r2_score  test_pcc  \\\nmodel         num_feats                                             \nBNN + LR      10                0.537268       0.047827  0.268009   \n              20                0.537873       0.046022  0.312778   \n              30                0.545694       0.014559  0.314402   \n              40                0.548733       0.005249  0.325595   \n              50                0.562054      -0.041952  0.303852   \nBNN + LR + GP 10                0.596980      -0.262101  0.217735   \n              20                0.573259      -0.098103  0.275400   \n              30                0.566625      -0.070660  0.304064   \n              40                0.563732      -0.056073  0.311494   \n              50                0.574822      -0.094130  0.299497   \nRF + LR       10                0.494575       0.197003  0.458374   \n              20                0.493651       0.197615  0.465853   \n              30                0.488648       0.215022  0.480978   \n              40                0.492449       0.202834  0.474456   \n              50                0.494120       0.197730  0.473012   \nRF + LR + GP  10                0.544896      -0.109444  0.429179   \n              20                0.497218       0.185374  0.458679   \n              30                0.494995       0.194069  0.471333   \n              40                0.500833       0.174388  0.460211   \n              50                0.501782       0.171145  0.459229   \n\n                         test_pcc_pval  \nmodel         num_feats                 \nBNN + LR      10              0.119289  \n              20              0.027440  \n              30              0.031976  \n              40              0.036093  \n              50              0.043030  \nBNN + LR + GP 10              0.199679  \n              20              0.056152  \n              30              0.041436  \n              40              0.049129  \n              50              0.058287  \nRF + LR       10              0.000611  \n              20              0.001160  \n              30              0.000446  \n              40              0.000963  \n              50              0.001095  \nRF + LR + GP  10              0.037417  \n              20              0.001031  \n              30              0.000402  \n              40              0.001183  \n              50              0.001432  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>test_rmse_score</th>\n      <th>test_r2_score</th>\n      <th>test_pcc</th>\n      <th>test_pcc_pval</th>\n    </tr>\n    <tr>\n      <th>model</th>\n      <th>num_feats</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">BNN + LR</th>\n      <th>10</th>\n      <td>0.537268</td>\n      <td>0.047827</td>\n      <td>0.268009</td>\n      <td>0.119289</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.537873</td>\n      <td>0.046022</td>\n      <td>0.312778</td>\n      <td>0.027440</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.545694</td>\n      <td>0.014559</td>\n      <td>0.314402</td>\n      <td>0.031976</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.548733</td>\n      <td>0.005249</td>\n      <td>0.325595</td>\n      <td>0.036093</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>0.562054</td>\n      <td>-0.041952</td>\n      <td>0.303852</td>\n      <td>0.043030</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">BNN + LR + GP</th>\n      <th>10</th>\n      <td>0.596980</td>\n      <td>-0.262101</td>\n      <td>0.217735</td>\n      <td>0.199679</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.573259</td>\n      <td>-0.098103</td>\n      <td>0.275400</td>\n      <td>0.056152</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.566625</td>\n      <td>-0.070660</td>\n      <td>0.304064</td>\n      <td>0.041436</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.563732</td>\n      <td>-0.056073</td>\n      <td>0.311494</td>\n      <td>0.049129</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>0.574822</td>\n      <td>-0.094130</td>\n      <td>0.299497</td>\n      <td>0.058287</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">RF + LR</th>\n      <th>10</th>\n      <td>0.494575</td>\n      <td>0.197003</td>\n      <td>0.458374</td>\n      <td>0.000611</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.493651</td>\n      <td>0.197615</td>\n      <td>0.465853</td>\n      <td>0.001160</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.488648</td>\n      <td>0.215022</td>\n      <td>0.480978</td>\n      <td>0.000446</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.492449</td>\n      <td>0.202834</td>\n      <td>0.474456</td>\n      <td>0.000963</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>0.494120</td>\n      <td>0.197730</td>\n      <td>0.473012</td>\n      <td>0.001095</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">RF + LR + GP</th>\n      <th>10</th>\n      <td>0.544896</td>\n      <td>-0.109444</td>\n      <td>0.429179</td>\n      <td>0.037417</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.497218</td>\n      <td>0.185374</td>\n      <td>0.458679</td>\n      <td>0.001031</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.494995</td>\n      <td>0.194069</td>\n      <td>0.471333</td>\n      <td>0.000402</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.500833</td>\n      <td>0.174388</td>\n      <td>0.460211</td>\n      <td>0.001183</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>0.501782</td>\n      <td>0.171145</td>\n      <td>0.459229</td>\n      <td>0.001432</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = f\"{data_dir}/exp_data_5/cancer/gdsc\"\n",
    "seeds = [422,261,968,282,739,573,220,413,745,775,482,442,210,423,760,57,769,920,226,196]\n",
    "# seeds = [422,261,968,282,739,573,220,413,745,775,482,442,210,423,760,57]\n",
    "res_dfs = []\n",
    "for seed in seeds:\n",
    "    df = pd.read_csv(f\"{save_dir}/results/bnn_rf_gp_s_{seed}_ig_abs_v6.csv\")\n",
    "    res_dfs.append(df)\n",
    "\n",
    "bnn_rf_df = pd.concat(res_dfs, axis=0)\n",
    "bnn_rf_df.groupby([\"model\", \"num_feats\"]).mean().iloc[:,1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "          test_rmse_score  test_r2_score\nmodel                                   \nBNN + BG         0.481284       0.238046\nRF               0.479711       0.245956",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>test_rmse_score</th>\n      <th>test_r2_score</th>\n    </tr>\n    <tr>\n      <th>model</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>BNN + BG</th>\n      <td>0.481284</td>\n      <td>0.238046</td>\n    </tr>\n    <tr>\n      <th>RF</th>\n      <td>0.479711</td>\n      <td>0.245956</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = f\"{data_dir}/exp_data_5/cancer/gdsc\"\n",
    "seeds = [422,261,968,282,739,573,220,413,745,775,482,442,210,423,760,57,769,920,226,196]\n",
    "# seeds = [422,261,968,282,739,573,220,413,745,775,482,442,210,423,760,57]\n",
    "res_dfs = []\n",
    "for seed in seeds:\n",
    "    df = pd.read_csv(f\"{save_dir}/results/bnn_bg_rf_s_{seed}_v5.csv\")\n",
    "    res_dfs.append(df)\n",
    "\n",
    "bnn_rf_df = pd.concat(res_dfs, axis=0)\n",
    "bnn_rf_df.groupby([\"model\"]).mean().iloc[:,1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_rf_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_rf_df = bnn_rf_df.reset_index().drop(index=[8, 9], axis=0).groupby([\"model\"]).mean()\n",
    "bnn_rf_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                         test_rmse_score  test_r2_score  test_pcc  \\\nmodel         num_feats                                             \nBNN + LR      20                0.549379       0.003239  0.271716   \n              40                0.546482       0.013174  0.337297   \n              60                0.554629      -0.018310  0.335159   \nBNN + LR + GP 20                0.555483      -0.019855  0.263980   \n              40                0.550449      -0.001111  0.332696   \n              60                0.561666      -0.044314  0.326638   \nRF + LR       20                0.495686       0.191597  0.460010   \n              40                0.503463       0.165533  0.452178   \n              60                0.509864       0.141250  0.431336   \nRF + LR + GP  20                0.498977       0.180815  0.456942   \n              40                0.503225       0.165837  0.453768   \n              60                0.511880       0.133692  0.432519   \n\n                         test_pcc_pval  \nmodel         num_feats                 \nBNN + LR      20              0.103979  \n              40              0.019269  \n              60              0.026847  \nBNN + LR + GP 20              0.093821  \n              40              0.024275  \n              60              0.030761  \nRF + LR       20              0.000520  \n              40              0.000141  \n              60              0.000862  \nRF + LR + GP  20              0.000486  \n              40              0.000171  \n              60              0.000989  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>test_rmse_score</th>\n      <th>test_r2_score</th>\n      <th>test_pcc</th>\n      <th>test_pcc_pval</th>\n    </tr>\n    <tr>\n      <th>model</th>\n      <th>num_feats</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">BNN + LR</th>\n      <th>20</th>\n      <td>0.549379</td>\n      <td>0.003239</td>\n      <td>0.271716</td>\n      <td>0.103979</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.546482</td>\n      <td>0.013174</td>\n      <td>0.337297</td>\n      <td>0.019269</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>0.554629</td>\n      <td>-0.018310</td>\n      <td>0.335159</td>\n      <td>0.026847</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">BNN + LR + GP</th>\n      <th>20</th>\n      <td>0.555483</td>\n      <td>-0.019855</td>\n      <td>0.263980</td>\n      <td>0.093821</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.550449</td>\n      <td>-0.001111</td>\n      <td>0.332696</td>\n      <td>0.024275</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>0.561666</td>\n      <td>-0.044314</td>\n      <td>0.326638</td>\n      <td>0.030761</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">RF + LR</th>\n      <th>20</th>\n      <td>0.495686</td>\n      <td>0.191597</td>\n      <td>0.460010</td>\n      <td>0.000520</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.503463</td>\n      <td>0.165533</td>\n      <td>0.452178</td>\n      <td>0.000141</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>0.509864</td>\n      <td>0.141250</td>\n      <td>0.431336</td>\n      <td>0.000862</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">RF + LR + GP</th>\n      <th>20</th>\n      <td>0.498977</td>\n      <td>0.180815</td>\n      <td>0.456942</td>\n      <td>0.000486</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.503225</td>\n      <td>0.165837</td>\n      <td>0.453768</td>\n      <td>0.000171</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>0.511880</td>\n      <td>0.133692</td>\n      <td>0.432519</td>\n      <td>0.000989</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_rf_gp_lsw = pd.read_csv(f\"{save_dir}/bnn_rf_gp_feature_sel_summary.csv\")\n",
    "bnn_rf_gp_lsw = bnn_rf_gp_lsw.drop_duplicates().drop(\"Unnamed: 0\", axis=1)\n",
    "bnn_rf_gp_lsw.groupby([\"model\", \"num_feats\"]).mean().iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "                         test_rmse_score  test_r2_score  test_pcc  \\\nmodel         num_feats                                             \nBNN + LR      20                0.535148       0.059113  0.310668   \nBNN + LR + GP 20                0.553210      -0.009103  0.280440   \nRF + LR       20                0.495686       0.191597  0.460010   \nRF + LR + GP  20                0.499409       0.179732  0.457535   \n\n                         test_pcc_pval  \nmodel         num_feats                 \nBNN + LR      20              0.050418  \nBNN + LR + GP 20              0.094543  \nRF + LR       20              0.000520  \nRF + LR + GP  20              0.000252  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>test_rmse_score</th>\n      <th>test_r2_score</th>\n      <th>test_pcc</th>\n      <th>test_pcc_pval</th>\n    </tr>\n    <tr>\n      <th>model</th>\n      <th>num_feats</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>BNN + LR</th>\n      <th>20</th>\n      <td>0.535148</td>\n      <td>0.059113</td>\n      <td>0.310668</td>\n      <td>0.050418</td>\n    </tr>\n    <tr>\n      <th>BNN + LR + GP</th>\n      <th>20</th>\n      <td>0.553210</td>\n      <td>-0.009103</td>\n      <td>0.280440</td>\n      <td>0.094543</td>\n    </tr>\n    <tr>\n      <th>RF + LR</th>\n      <th>20</th>\n      <td>0.495686</td>\n      <td>0.191597</td>\n      <td>0.460010</td>\n      <td>0.000520</td>\n    </tr>\n    <tr>\n      <th>RF + LR + GP</th>\n      <th>20</th>\n      <td>0.499409</td>\n      <td>0.179732</td>\n      <td>0.457535</td>\n      <td>0.000252</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = f\"{data_dir}/exp_data_5/cancer/gdsc\"\n",
    "seeds = [422,261,968,282,739,573,220,413,745,775,482,442,210,423,760,57,769,920,226,196]\n",
    "res_dfs = []\n",
    "for seed in seeds:\n",
    "    df = pd.read_csv(f\"{save_dir}/results/bnn_rf_gp_s_{seed}_v4.csv\")\n",
    "    res_dfs.append(df)\n",
    "\n",
    "bnn_rf_gp_df = pd.concat(res_dfs, axis=0)\n",
    "bnn_rf_gp_df.groupby([\"model\", \"num_feats\"]).mean().iloc[:,1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "    seed          model  num_feats  test_rmse_score  test_r2_score  test_pcc  \\\n0    422        RF + LR         20         0.445575       0.137967  0.441134   \n1    422   RF + LR + GP         20         0.440351       0.158062  0.437110   \n2    422       BNN + LR         20         0.446160       0.135702  0.378847   \n3    422  BNN + LR + GP         20         0.432252       0.188745  0.438863   \n0    261        RF + LR         20         0.515131       0.064735  0.299998   \n..   ...            ...        ...              ...            ...       ...   \n3    226  BNN + LR + GP         20         0.580038       0.131860  0.382773   \n0    196        RF + LR         20         0.497757       0.171029  0.438416   \n1    196   RF + LR + GP         20         0.497990       0.170254  0.439247   \n2    196       BNN + LR         20         0.535433       0.040789  0.296835   \n3    196  BNN + LR + GP         20         0.541201       0.020010  0.301244   \n\n    test_pcc_pval  \n0        0.000034  \n1        0.000040  \n2        0.000449  \n3        0.000037  \n0        0.006176  \n..            ...  \n3        0.000387  \n0        0.000038  \n1        0.000037  \n2        0.006769  \n3        0.005956  \n\n[80 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>seed</th>\n      <th>model</th>\n      <th>num_feats</th>\n      <th>test_rmse_score</th>\n      <th>test_r2_score</th>\n      <th>test_pcc</th>\n      <th>test_pcc_pval</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>422</td>\n      <td>RF + LR</td>\n      <td>20</td>\n      <td>0.445575</td>\n      <td>0.137967</td>\n      <td>0.441134</td>\n      <td>0.000034</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>422</td>\n      <td>RF + LR + GP</td>\n      <td>20</td>\n      <td>0.440351</td>\n      <td>0.158062</td>\n      <td>0.437110</td>\n      <td>0.000040</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>422</td>\n      <td>BNN + LR</td>\n      <td>20</td>\n      <td>0.446160</td>\n      <td>0.135702</td>\n      <td>0.378847</td>\n      <td>0.000449</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>422</td>\n      <td>BNN + LR + GP</td>\n      <td>20</td>\n      <td>0.432252</td>\n      <td>0.188745</td>\n      <td>0.438863</td>\n      <td>0.000037</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>261</td>\n      <td>RF + LR</td>\n      <td>20</td>\n      <td>0.515131</td>\n      <td>0.064735</td>\n      <td>0.299998</td>\n      <td>0.006176</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>226</td>\n      <td>BNN + LR + GP</td>\n      <td>20</td>\n      <td>0.580038</td>\n      <td>0.131860</td>\n      <td>0.382773</td>\n      <td>0.000387</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>196</td>\n      <td>RF + LR</td>\n      <td>20</td>\n      <td>0.497757</td>\n      <td>0.171029</td>\n      <td>0.438416</td>\n      <td>0.000038</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>196</td>\n      <td>RF + LR + GP</td>\n      <td>20</td>\n      <td>0.497990</td>\n      <td>0.170254</td>\n      <td>0.439247</td>\n      <td>0.000037</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>196</td>\n      <td>BNN + LR</td>\n      <td>20</td>\n      <td>0.535433</td>\n      <td>0.040789</td>\n      <td>0.296835</td>\n      <td>0.006769</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>196</td>\n      <td>BNN + LR + GP</td>\n      <td>20</td>\n      <td>0.541201</td>\n      <td>0.020010</td>\n      <td>0.301244</td>\n      <td>0.005956</td>\n    </tr>\n  </tbody>\n</table>\n<p>80 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_rf_gp_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save_dir = f\"{data_dir}/exp_data_5/cancer/gdsc\"\n",
    "# seeds = [422,261,968,282,739,573,220,413,745,775,482,442,210,423,760,57,769,920,226,196]\n",
    "# res_dfs = []\n",
    "# for seed in seeds:\n",
    "#     df = pd.read_csv(f\"{save_dir}/results/bnn_rf_gp_s_{seed}_v2.csv\")\n",
    "#     res_dfs.append(df)\n",
    "#\n",
    "# bnn_rf_gp_df_v2 = pd.concat(res_dfs, axis=0)\n",
    "# bnn_rf_gp_df_v2.groupby([\"model\", \"num_feats\"]).mean().iloc[:,1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}