{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\r\n",
      "Collecting jaxlib[cuda112]==0.3.15\r\n",
      "  Downloading https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.15%2Bcuda11.cudnn82-cp39-none-manylinux2014_x86_64.whl (162.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m162.7/162.7 MB\u001B[0m \u001B[31m12.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25h\u001B[33mWARNING: jaxlib 0.3.15+cuda11.cudnn82 does not provide the extra 'cuda112'\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from jaxlib[cuda112]==0.3.15) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.9/dist-packages (from jaxlib[cuda112]==0.3.15) (1.23.1)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jaxlib[cuda112]==0.3.15) (1.8.1)\r\n",
      "Installing collected packages: jaxlib\r\n",
      "  Attempting uninstall: jaxlib\r\n",
      "    Found existing installation: jaxlib 0.3.8+cuda11.cudnn82\r\n",
      "    Uninstalling jaxlib-0.3.8+cuda11.cudnn82:\r\n",
      "      Successfully uninstalled jaxlib-0.3.8+cuda11.cudnn82\r\n",
      "Successfully installed jaxlib-0.3.15+cuda11.cudnn82\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mLooking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\r\n",
      "Collecting jax[cuda112]==0.3.17\r\n",
      "  Downloading jax-0.3.17.tar.gz (1.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m55.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[33mWARNING: jax 0.3.17 does not provide the extra 'cuda112'\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (1.23.1)\r\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (3.3.0)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (1.8.1)\r\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (4.3.0)\r\n",
      "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (0.6.0)\r\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax[cuda112]==0.3.17) (3.8.1)\r\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax[cuda112]==0.3.17) (5.8.0)\r\n",
      "Building wheels for collected packages: jax\r\n",
      "  Building wheel for jax (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for jax: filename=jax-0.3.17-py3-none-any.whl size=1217849 sha256=da767a95a5655593d68fbb139ae97c44651becfd3bb832dd6baafc92db68da9c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/36/cd/88/2d90379f7549c27d5654e893f74210f30f0c645c23a71e6f56\r\n",
      "Successfully built jax\r\n",
      "Installing collected packages: jax\r\n",
      "  Attempting uninstall: jax\r\n",
      "    Found existing installation: jax 0.3.14\r\n",
      "    Uninstalling jax-0.3.14:\r\n",
      "      Successfully uninstalled jax-0.3.14\r\n",
      "Successfully installed jax-0.3.17\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting optax\r\n",
      "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m154.9/154.9 kB\u001B[0m \u001B[31m24.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.9/dist-packages (from optax) (0.3.17)\r\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from optax) (1.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.9/dist-packages (from optax) (4.3.0)\r\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.9/dist-packages (from optax) (0.3.15+cuda11.cudnn82)\r\n",
      "Collecting chex>=0.1.5\r\n",
      "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m85.3/85.3 kB\u001B[0m \u001B[31m24.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from optax) (1.23.1)\r\n",
      "Collecting dm-tree>=0.1.5\r\n",
      "  Downloading dm_tree-0.1.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (142 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m142.7/142.7 kB\u001B[0m \u001B[31m36.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting toolz>=0.9.0\r\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m55.8/55.8 kB\u001B[0m \u001B[31m17.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: etils[epath] in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (0.6.0)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (1.8.1)\r\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (3.3.0)\r\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax>=0.1.55->optax) (3.8.1)\r\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax>=0.1.55->optax) (5.8.0)\r\n",
      "Installing collected packages: dm-tree, toolz, chex, optax\r\n",
      "Successfully installed chex-0.1.5 dm-tree-0.1.7 optax-0.1.4 toolz-0.12.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting dm-haiku\r\n",
      "  Downloading dm_haiku-0.0.9-py3-none-any.whl (352 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m352.1/352.1 kB\u001B[0m \u001B[31m36.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (1.1.0)\r\n",
      "Collecting jmp>=0.0.2\r\n",
      "  Downloading jmp-0.0.2-py3-none-any.whl (16 kB)\r\n",
      "Collecting tabulate>=0.8.9\r\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\r\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (1.23.1)\r\n",
      "Installing collected packages: tabulate, jmp, dm-haiku\r\n",
      "Successfully installed dm-haiku-0.0.9 jmp-0.0.2 tabulate-0.9.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting tensorflow-probability==0.17\r\n",
      "  Downloading tensorflow_probability-0.17.0-py2.py3-none-any.whl (6.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.5/6.5 MB\u001B[0m \u001B[31m77.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m0:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (0.4.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (2.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (1.23.1)\r\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (0.1.7)\r\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow-probability==0.17) (1.14.0)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (1.1.0)\r\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (5.1.1)\r\n",
      "Installing collected packages: tensorflow-probability\r\n",
      "Successfully installed tensorflow-probability-0.17.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting git+https://github.com/blackjax-devs/blackjax.git\r\n",
      "  Cloning https://github.com/blackjax-devs/blackjax.git to /tmp/pip-req-build-463dksq6\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/blackjax-devs/blackjax.git /tmp/pip-req-build-463dksq6\r\n",
      "  Resolved https://github.com/blackjax-devs/blackjax.git to commit eb2ee2680fa0754a4ad49c9dcbd8ed9ba68cf494\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting fastprogress>=0.2.0\r\n",
      "  Downloading fastprogress-1.0.3-py3-none-any.whl (12 kB)\r\n",
      "Requirement already satisfied: jax>=0.3.13 in /usr/local/lib/python3.9/dist-packages (from blackjax==0.9.6+78.geb2ee26) (0.3.17)\r\n",
      "Requirement already satisfied: jaxlib>=0.3.10 in /usr/local/lib/python3.9/dist-packages (from blackjax==0.9.6+78.geb2ee26) (0.3.15+cuda11.cudnn82)\r\n",
      "Collecting jaxopt>=0.5.5\r\n",
      "  Downloading jaxopt-0.5.5-py3-none-any.whl (132 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m132.3/132.3 kB\u001B[0m \u001B[31m23.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+78.geb2ee26) (1.1.0)\r\n",
      "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+78.geb2ee26) (0.6.0)\r\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+78.geb2ee26) (3.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+78.geb2ee26) (1.23.1)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+78.geb2ee26) (1.8.1)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+78.geb2ee26) (4.3.0)\r\n",
      "Requirement already satisfied: matplotlib>=2.0.1 in /usr/local/lib/python3.9/dist-packages (from jaxopt>=0.5.5->blackjax==0.9.6+78.geb2ee26) (3.5.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+78.geb2ee26) (0.11.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+78.geb2ee26) (1.4.3)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+78.geb2ee26) (9.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+78.geb2ee26) (2.8.2)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+78.geb2ee26) (4.34.4)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+78.geb2ee26) (3.0.9)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+78.geb2ee26) (21.3)\r\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax>=0.3.13->blackjax==0.9.6+78.geb2ee26) (5.8.0)\r\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax>=0.3.13->blackjax==0.9.6+78.geb2ee26) (3.8.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+78.geb2ee26) (1.14.0)\r\n",
      "Building wheels for collected packages: blackjax\r\n",
      "  Building wheel for blackjax (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for blackjax: filename=blackjax-0.9.6+78.geb2ee26-py3-none-any.whl size=126440 sha256=e1e902bf2031ff936df3f5a1d98998c34d48cd896c2e315e719f1c5d5c11cefc\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-lecrxtsi/wheels/e6/1e/f6/a6e0408a4e374b9cdb789b1769716b4ed61eef520a2dd702b1\r\n",
      "Successfully built blackjax\r\n",
      "Installing collected packages: fastprogress, jaxopt, blackjax\r\n",
      "Successfully installed blackjax-0.9.6+78.geb2ee26 fastprogress-1.0.3 jaxopt-0.5.5\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\r\n",
      "Get:2 https://deb.nodesource.com/node_16.x focal InRelease [4583 B]\r\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]            \u001B[0m\u001B[0m\u001B[33m\r\n",
      "Get:4 https://deb.nodesource.com/node_16.x focal/main amd64 Packages [774 B]\r\n",
      "Get:5 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease [18.1 kB] \u001B[0m\u001B[33m\r\n",
      "Get:6 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [27.7 kB]\r\n",
      "Get:7 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1779 kB]\r\n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [972 kB]\r\n",
      "Get:9 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2358 kB][33m\r\n",
      "Get:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 Packages [29.5 kB][33m\u001B[33m\r\n",
      "                                                                               \r[0mGet:11 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\r\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]     \u001B[0m\u001B[33m\u001B[33m\r\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\r\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\r\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\r\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\r\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [1894 kB]\r\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2827 kB]\r\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1275 kB]\r\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [30.4 kB]\r\n",
      "Get:21 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\r\n",
      "Get:22 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\r\n",
      "Fetched 24.7 MB in 2s (11.9 MB/s)[33m                   \u001B[0m\u001B[33m\u001B[33m\u001B[33m\u001B[33m\u001B[33m\r\n",
      "Reading package lists... Done\r\n",
      "Building dependency tree       \r\n",
      "Reading state information... Done\r\n",
      "112 packages can be upgraded. Run 'apt list --upgradable' to see them.\r\n",
      "Reading package lists... Done\r\n",
      "Building dependency tree       \r\n",
      "Reading state information... Done\r\n",
      "The following additional packages will be installed:\r\n",
      "  fonts-liberation libann0 libcdt5 libcgraph6 libgts-0.7-5 libgts-bin libgvc6\r\n",
      "  libgvpr2 liblab-gamut1 libpathplan4\r\n",
      "Suggested packages:\r\n",
      "  gsfonts graphviz-doc\r\n",
      "The following NEW packages will be installed:\r\n",
      "  fonts-liberation graphviz libann0 libcdt5 libcgraph6 libgts-0.7-5 libgts-bin\r\n",
      "  libgvc6 libgvpr2 liblab-gamut1 libpathplan4\r\n",
      "0 upgraded, 11 newly installed, 0 to remove and 112 not upgraded.\r\n",
      "Need to get 2701 kB of archives.\r\n",
      "After this operation, 11.3 MB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 fonts-liberation all 1:1.07.4-11 [822 kB]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libann0 amd64 1.1.2+doc-7build1 [26.0 kB]\r\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 libcdt5 amd64 2.42.2-3build2 [18.7 kB]\r\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 libcgraph6 amd64 2.42.2-3build2 [41.3 kB]\r\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgts-0.7-5 amd64 0.7.6+darcs121130-4 [150 kB]\r\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/universe amd64 libpathplan4 amd64 2.42.2-3build2 [21.9 kB]\r\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgvc6 amd64 2.42.2-3build2 [647 kB]\r\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgvpr2 amd64 2.42.2-3build2 [167 kB]\r\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/universe amd64 liblab-gamut1 amd64 2.42.2-3build2 [177 kB]\r\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/universe amd64 graphviz amd64 2.42.2-3build2 [590 kB]\r\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgts-bin amd64 0.7.6+darcs121130-4 [41.3 kB]\r\n",
      "Fetched 2701 kB in 0s (14.8 MB/s)      \u001B[0m\u001B[33m\r\n",
      "\n",
      "\u001B7\u001B[0;23r\u001B8\u001B[1ASelecting previously unselected package fonts-liberation.\r\n",
      "(Reading database ... 78556 files and directories currently installed.)\r\n",
      "Preparing to unpack .../00-fonts-liberation_1%3a1.07.4-11_all.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  0%]\u001B[49m\u001B[39m [..........................................................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  2%]\u001B[49m\u001B[39m [#.........................................................] \u001B8Unpacking fonts-liberation (1:1.07.4-11) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  4%]\u001B[49m\u001B[39m [##........................................................] \u001B8Selecting previously unselected package libann0.\r\n",
      "Preparing to unpack .../01-libann0_1.1.2+doc-7build1_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  7%]\u001B[49m\u001B[39m [###.......................................................] \u001B8Unpacking libann0 (1.1.2+doc-7build1) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  9%]\u001B[49m\u001B[39m [#####.....................................................] \u001B8Selecting previously unselected package libcdt5:amd64.\r\n",
      "Preparing to unpack .../02-libcdt5_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 11%]\u001B[49m\u001B[39m [######....................................................] \u001B8Unpacking libcdt5:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 13%]\u001B[49m\u001B[39m [#######...................................................] \u001B8Selecting previously unselected package libcgraph6:amd64.\r\n",
      "Preparing to unpack .../03-libcgraph6_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 16%]\u001B[49m\u001B[39m [#########.................................................] \u001B8Unpacking libcgraph6:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 18%]\u001B[49m\u001B[39m [##########................................................] \u001B8Selecting previously unselected package libgts-0.7-5:amd64.\r\n",
      "Preparing to unpack .../04-libgts-0.7-5_0.7.6+darcs121130-4_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 20%]\u001B[49m\u001B[39m [###########...............................................] \u001B8Unpacking libgts-0.7-5:amd64 (0.7.6+darcs121130-4) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 22%]\u001B[49m\u001B[39m [############..............................................] \u001B8Selecting previously unselected package libpathplan4:amd64.\r\n",
      "Preparing to unpack .../05-libpathplan4_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 24%]\u001B[49m\u001B[39m [##############............................................] \u001B8Unpacking libpathplan4:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 27%]\u001B[49m\u001B[39m [###############...........................................] \u001B8Selecting previously unselected package libgvc6.\r\n",
      "Preparing to unpack .../06-libgvc6_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 29%]\u001B[49m\u001B[39m [################..........................................] \u001B8Unpacking libgvc6 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 31%]\u001B[49m\u001B[39m [##################........................................] \u001B8Selecting previously unselected package libgvpr2:amd64.\r\n",
      "Preparing to unpack .../07-libgvpr2_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 33%]\u001B[49m\u001B[39m [###################.......................................] \u001B8Unpacking libgvpr2:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 36%]\u001B[49m\u001B[39m [####################......................................] \u001B8Selecting previously unselected package liblab-gamut1:amd64.\r\n",
      "Preparing to unpack .../08-liblab-gamut1_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 38%]\u001B[49m\u001B[39m [#####################.....................................] \u001B8Unpacking liblab-gamut1:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 40%]\u001B[49m\u001B[39m [#######################...................................] \u001B8Selecting previously unselected package graphviz.\r\n",
      "Preparing to unpack .../09-graphviz_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 42%]\u001B[49m\u001B[39m [########################..................................] \u001B8Unpacking graphviz (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 44%]\u001B[49m\u001B[39m [#########################.................................] \u001B8Selecting previously unselected package libgts-bin.\r\n",
      "Preparing to unpack .../10-libgts-bin_0.7.6+darcs121130-4_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 47%]\u001B[49m\u001B[39m [###########################...............................] \u001B8Unpacking libgts-bin (0.7.6+darcs121130-4) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 49%]\u001B[49m\u001B[39m [############################..............................] \u001B8Setting up liblab-gamut1:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 51%]\u001B[49m\u001B[39m [#############################.............................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 53%]\u001B[49m\u001B[39m [##############################............................] \u001B8Setting up libgts-0.7-5:amd64 (0.7.6+darcs121130-4) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 56%]\u001B[49m\u001B[39m [################################..........................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 58%]\u001B[49m\u001B[39m [#################################.........................] \u001B8Setting up libpathplan4:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 60%]\u001B[49m\u001B[39m [##################################........................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 62%]\u001B[49m\u001B[39m [####################################......................] \u001B8Setting up libann0 (1.1.2+doc-7build1) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 64%]\u001B[49m\u001B[39m [#####################################.....................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 67%]\u001B[49m\u001B[39m [######################################....................] \u001B8Setting up fonts-liberation (1:1.07.4-11) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 69%]\u001B[49m\u001B[39m [#######################################...................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 71%]\u001B[49m\u001B[39m [#########################################.................] \u001B8Setting up libcdt5:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 73%]\u001B[49m\u001B[39m [##########################################................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 76%]\u001B[49m\u001B[39m [###########################################...............] \u001B8Setting up libcgraph6:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 78%]\u001B[49m\u001B[39m [#############################################.............] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 80%]\u001B[49m\u001B[39m [##############################################............] \u001B8Setting up libgts-bin (0.7.6+darcs121130-4) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 82%]\u001B[49m\u001B[39m [###############################################...........] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 84%]\u001B[49m\u001B[39m [################################################..........] \u001B8Setting up libgvc6 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 87%]\u001B[49m\u001B[39m [##################################################........] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 89%]\u001B[49m\u001B[39m [###################################################.......] \u001B8Setting up libgvpr2:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 91%]\u001B[49m\u001B[39m [####################################################......] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 93%]\u001B[49m\u001B[39m [######################################################....] \u001B8Setting up graphviz (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 96%]\u001B[49m\u001B[39m [#######################################################...] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 98%]\u001B[49m\u001B[39m [########################################################..] \u001B8Processing triggers for libc-bin (2.31-0ubuntu9.7) ...\r\n",
      "Processing triggers for man-db (2.9.1-1) ...\r\n",
      "Processing triggers for fontconfig (2.13.1-2ubuntu3) ...\r\n",
      "\r\n",
      "Reading package lists... Done\r\n",
      "Building dependency tree       \r\n",
      "Reading state information... Done\r\n",
      "The following additional packages will be installed:\r\n",
      "  swig4.0\r\n",
      "Suggested packages:\r\n",
      "  swig-doc swig-examples swig4.0-examples swig4.0-doc\r\n",
      "The following NEW packages will be installed:\r\n",
      "  swig swig4.0\r\n",
      "0 upgraded, 2 newly installed, 0 to remove and 112 not upgraded.\r\n",
      "Need to get 1086 kB of archives.\r\n",
      "After this operation, 5413 kB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig4.0 amd64 4.0.1-5build1 [1081 kB]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig all 4.0.1-5build1 [5528 B]\r\n",
      "Fetched 1086 kB in 1s (1532 kB/s)\r\n",
      "Selecting previously unselected package swig4.0.\r\n",
      "(Reading database ... 78769 files and directories currently installed.)\r\n",
      "Preparing to unpack .../swig4.0_4.0.1-5build1_amd64.deb ...\r\n",
      "Unpacking swig4.0 (4.0.1-5build1) ...\r\n",
      "Selecting previously unselected package swig.\r\n",
      "Preparing to unpack .../swig_4.0.1-5build1_all.deb ...\r\n",
      "Unpacking swig (4.0.1-5build1) ...\r\n",
      "Setting up swig4.0 (4.0.1-5build1) ...\r\n",
      "Setting up swig (4.0.1-5build1) ...\r\n",
      "Processing triggers for man-db (2.9.1-1) ...\r\n",
      "Collecting smac\r\n",
      "  Downloading smac-1.4.0.tar.gz (202 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m202.9/202.9 kB\u001B[0m \u001B[31m26.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.9/dist-packages (from smac) (1.23.1)\r\n",
      "Collecting distributed\r\n",
      "  Downloading distributed-2022.12.0-py3-none-any.whl (925 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m925.3/925.3 kB\u001B[0m \u001B[31m83.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting pynisher<1.0.0\r\n",
      "  Downloading pynisher-0.6.4.tar.gz (11 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting dask\r\n",
      "  Downloading dask-2022.12.0-py3-none-any.whl (1.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m85.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from smac) (2022.7.9)\r\n",
      "Collecting ConfigSpace>=0.5.0\r\n",
      "  Downloading ConfigSpace-0.6.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.6/5.6 MB\u001B[0m \u001B[31m10.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0mm\r\n",
      "\u001B[?25hRequirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from smac) (1.8.1)\r\n",
      "Collecting pyrfr>=0.8.3\r\n",
      "  Downloading pyrfr-0.8.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.5/4.5 MB\u001B[0m \u001B[31m13.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting emcee>=3.0.0\r\n",
      "  Downloading emcee-3.1.3-py2.py3-none-any.whl (46 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.2/46.2 kB\u001B[0m \u001B[31m15.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from smac) (5.9.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.9/dist-packages (from smac) (1.1.1)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from smac) (1.1.0)\r\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from ConfigSpace>=0.5.0->smac) (0.29.30)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from ConfigSpace>=0.5.0->smac) (4.3.0)\r\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from ConfigSpace>=0.5.0->smac) (3.0.9)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from pynisher<1.0.0->smac) (63.1.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22.0->smac) (3.1.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (5.4.1)\r\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (0.12.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (21.3)\r\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (2022.5.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (2.1.0)\r\n",
      "Collecting partd>=0.3.10\r\n",
      "  Downloading partd-1.3.0-py3-none-any.whl (18 kB)\r\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (8.1.3)\r\n",
      "Collecting tblib>=1.6.0\r\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\r\n",
      "Collecting zict>=0.1.3\r\n",
      "  Downloading zict-2.2.0-py2.py3-none-any.whl (23 kB)\r\n",
      "Collecting sortedcontainers!=2.0.0,!=2.0.1\r\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\r\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from distributed->smac) (1.26.10)\r\n",
      "Collecting msgpack>=0.6.0\r\n",
      "  Downloading msgpack-1.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m322.4/322.4 kB\u001B[0m \u001B[31m52.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from distributed->smac) (3.1.2)\r\n",
      "Collecting locket>=1.0.0\r\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\r\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.9/dist-packages (from distributed->smac) (6.2)\r\n",
      "Collecting heapdict\r\n",
      "  Downloading HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->distributed->smac) (2.1.1)\r\n",
      "Building wheels for collected packages: smac, pynisher\r\n",
      "  Building wheel for smac (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for smac: filename=smac-1.4.0-py3-none-any.whl size=262348 sha256=b2dab8d617d7e902a388f556524c5a7090d2d322cc30b210ff09b6aafe2bb32c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/cc/e7/d683d9404760c4701ea2f64faaf689a8de718f701de63e71ea\r\n",
      "  Building wheel for pynisher (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for pynisher: filename=pynisher-0.6.4-py3-none-any.whl size=7026 sha256=a18fedd2c1c630b260973779fd1bada5741087d6efa5c35c4532d77436a91b84\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/1d/de/5e/d4947b76b76ba27581d1e09f395eca1583a802203a41c04873\r\n",
      "Successfully built smac pynisher\r\n",
      "Installing collected packages: sortedcontainers, msgpack, heapdict, zict, tblib, pyrfr, pynisher, locket, emcee, partd, ConfigSpace, dask, distributed, smac\r\n",
      "Successfully installed ConfigSpace-0.6.0 dask-2022.12.0 distributed-2022.12.0 emcee-3.1.3 heapdict-1.0.1 locket-1.0.0 msgpack-1.0.4 partd-1.3.0 pynisher-0.6.4 pyrfr-0.8.3 smac-1.4.0 sortedcontainers-2.4.0 tblib-1.7.0 zict-2.2.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting graphviz\r\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m47.0/47.0 kB\u001B[0m \u001B[31m12.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: graphviz\r\n",
      "Successfully installed graphviz-0.20.1\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting gplearn\r\n",
      "  Downloading gplearn-0.4.2-py3-none-any.whl (25 kB)\r\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from gplearn) (1.1.1)\r\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from gplearn) (1.1.0)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.8.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (3.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.23.1)\r\n",
      "Installing collected packages: gplearn\r\n",
      "Successfully installed gplearn-0.4.2\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U jaxlib[cuda112]==0.3.15 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install -U jax[cuda112]==0.3.17 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install optax\n",
    "!pip install dm-haiku\n",
    "!pip install tensorflow-probability==0.17\n",
    "!pip install git+https://github.com/blackjax-devs/blackjax.git\n",
    "!apt update\n",
    "!apt install -y graphviz\n",
    "!apt-get -y install swig\n",
    "!pip install smac\n",
    "!pip install graphviz\n",
    "!pip install gplearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"False\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "os.environ[\"JAX_ENABLE_X64\"] = \"True\"\n",
    "\n",
    "SERVER = 1\n",
    "\n",
    "if not SERVER:\n",
    "    %cd /home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "tfd = tfp.distributions\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import numpy as np\n",
    "import optax\n",
    "from nn_util import *\n",
    "plt.style.use('ggplot')\n",
    "%load_ext autoreload"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "if SERVER:\n",
    "    data_dir = \".\"\n",
    "else:\n",
    "    data_dir = \"/home/xabush/code/snet/moses-incons-pen-xp/data\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GDSC Cell Line"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tamoxifen"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(406, 37265)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdsc_dir = f\"{data_dir}/cell_line/gdsc2\"\n",
    "gdsc_exp_tamox_data = pd.read_csv(f\"{gdsc_dir}/tamoxifen_response_gene_expr.csv\")\n",
    "gdsc_exp_tamox_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "X, target = gdsc_exp_tamox_data.iloc[:,:-1], gdsc_exp_tamox_data.iloc[:,-1]\n",
    "# change to -log10(IC_50) to make it comparable\n",
    "target = -np.log10(np.exp(target)) # exp b/c the values are natural logs of raw IC_50"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "     symbol method_of_action        cosmic_moa intogen_moa    gene_id\n0     ABCB1              Act               NaN         Act  SIDG00064\n1      ABI1        ambiguous       TSG, fusion   ambiguous  SIDG00145\n2      ABL1              Act  oncogene, fusion         Act  SIDG00150\n3      ABL2              Act  oncogene, fusion         Act  SIDG00151\n4     ACKR3              Act  oncogene, fusion         Act  SIDG00205\n..      ...              ...               ...         ...        ...\n778  ZNF814              Act               NaN         Act  SIDG42334\n779   ZNF93              LoF               NaN         LoF  SIDG41755\n780   ZNRF3              LoF               NaN         LoF  SIDG42403\n781   ZRSR2              LoF               TSG         LoF  SIDG42422\n782    ZXDB              LoF               NaN         LoF  SIDG42467\n\n[783 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>method_of_action</th>\n      <th>cosmic_moa</th>\n      <th>intogen_moa</th>\n      <th>gene_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ABCB1</td>\n      <td>Act</td>\n      <td>NaN</td>\n      <td>Act</td>\n      <td>SIDG00064</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ABI1</td>\n      <td>ambiguous</td>\n      <td>TSG, fusion</td>\n      <td>ambiguous</td>\n      <td>SIDG00145</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ABL1</td>\n      <td>Act</td>\n      <td>oncogene, fusion</td>\n      <td>Act</td>\n      <td>SIDG00150</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ABL2</td>\n      <td>Act</td>\n      <td>oncogene, fusion</td>\n      <td>Act</td>\n      <td>SIDG00151</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ACKR3</td>\n      <td>Act</td>\n      <td>oncogene, fusion</td>\n      <td>Act</td>\n      <td>SIDG00205</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>778</th>\n      <td>ZNF814</td>\n      <td>Act</td>\n      <td>NaN</td>\n      <td>Act</td>\n      <td>SIDG42334</td>\n    </tr>\n    <tr>\n      <th>779</th>\n      <td>ZNF93</td>\n      <td>LoF</td>\n      <td>NaN</td>\n      <td>LoF</td>\n      <td>SIDG41755</td>\n    </tr>\n    <tr>\n      <th>780</th>\n      <td>ZNRF3</td>\n      <td>LoF</td>\n      <td>NaN</td>\n      <td>LoF</td>\n      <td>SIDG42403</td>\n    </tr>\n    <tr>\n      <th>781</th>\n      <td>ZRSR2</td>\n      <td>LoF</td>\n      <td>TSG</td>\n      <td>LoF</td>\n      <td>SIDG42422</td>\n    </tr>\n    <tr>\n      <th>782</th>\n      <td>ZXDB</td>\n      <td>LoF</td>\n      <td>NaN</td>\n      <td>LoF</td>\n      <td>SIDG42467</td>\n    </tr>\n  </tbody>\n</table>\n<p>783 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_driver_genes_df = pd.read_csv(f\"{data_dir}/cell_line/driver_genes_20221018.csv\")\n",
    "cancer_driver_genes_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "cols = X.columns.to_list()\n",
    "driver_syms = cancer_driver_genes_df[\"symbol\"].to_list()\n",
    "sym_list = [sym.strip() for sym in cols if sym in driver_syms]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(406, 768)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_selected = X[sym_list]\n",
    "X_selected.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "seed = 261\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_selected, target, random_state=seed, shuffle=True, test_size=0.2)\n",
    "X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X_train_df, y_train_df, shuffle=True,\n",
    "                                                              random_state=seed, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:2583: UserWarning: n_quantiles (1000) is greater than the total number of samples (259). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer, PowerTransformer, RobustScaler, MinMaxScaler, Normalizer, StandardScaler\n",
    "\n",
    "train_transformer = QuantileTransformer(random_state=seed, output_distribution=\"normal\").fit(X_train_df)\n",
    "# train_transformer = PowerTransformer().fit(X_train_df)\n",
    "train_transformed = train_transformer.transform(X_train_df)\n",
    "val_transformed = train_transformer.transform(X_val_df)\n",
    "test_transformed = train_transformer.transform(X_test_df)\n",
    "\n",
    "X_train_df = pd.DataFrame(train_transformed, columns=X_train_df.columns)\n",
    "X_val_df = pd.DataFrame(val_transformed, columns=X_val_df.columns)\n",
    "X_test_df = pd.DataFrame(test_transformed, columns=X_test_df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_df.values, y_train_df.values\n",
    "X_val, y_val = X_val_df.values, y_val_df.values\n",
    "X_test, y_test = X_test_df.values, y_test_df.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class TrainingState(NamedTuple):\n",
    "    params: hk.Params\n",
    "    avg_params: hk.Params\n",
    "    opt_state: optax.OptState\n",
    "    net_state: hk.State"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "outputs": [],
   "source": [
    "class Conv1DNet:\n",
    "    def __init__(self, optim, hidden_size=4096, cha_input=16, cha_hidden=32,\n",
    "                 K=2, dropout_input=0.1, dropout_hidden=0.2, dropout_output=0.2, l2_coef=1e-5, act_fn=jax.nn.relu):\n",
    "\n",
    "        self.act_fn = act_fn\n",
    "        self.optimiser = optim\n",
    "        self.loss = jax.jit(self.loss)\n",
    "        self.update = jax.jit(self.update)\n",
    "\n",
    "        # hidden_size = sign_size*cha_input\n",
    "        # hidden_size = sign_size\n",
    "        # sign_size1 = sign_size\n",
    "        # sign_size2 = sign_size//2\n",
    "        # output_size = (sign_size//4) * cha_hidden\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cha_input = cha_input\n",
    "        self.cha_input_reshape = int(hidden_size/cha_input)\n",
    "        self.cha_hidden = cha_hidden\n",
    "        # self.K = K\n",
    "        # self.sign_size1 = sign_size1\n",
    "        # self.sign_size2 = sign_size2\n",
    "        # self.output_size = output_size\n",
    "        self.dropout_input = dropout_input\n",
    "        self.dropout_hidden = dropout_hidden\n",
    "        self.dropout_output = dropout_output\n",
    "        self.l2_coef = l2_coef\n",
    "\n",
    "        self.batch_norm1 = lambda x, is_training: hk.BatchNorm(True, True, 0.999)(x, is_training)\n",
    "        self.dense1 = lambda x: hk.Linear(hidden_size, with_bias=False)(x)\n",
    "\n",
    "        # 1st conv layer\n",
    "        self.batch_norm_c1 = lambda x, is_training: hk.BatchNorm(True, True, 0.999)(x, is_training)\n",
    "        self.conv1 = lambda x: hk.Conv1D(\n",
    "            cha_input,\n",
    "            kernel_shape=5,\n",
    "            stride = 1,\n",
    "            padding = \"VALID\",\n",
    "            # feature_group_count=cha_input,\n",
    "            with_bias=False)(x)\n",
    "\n",
    "        self.ave_po_c1 = lambda x: hk.AvgPool(window_shape=4, strides=1, padding=\"VALID\")(x)\n",
    "\n",
    "        #2nd conv layer\n",
    "        self.batch_norm_c2 = lambda x, is_training: hk.BatchNorm(True, True, 0.999)(x, is_training)\n",
    "        self.conv2 = lambda x: hk.Conv1D(\n",
    "            cha_hidden,\n",
    "            kernel_shape = 3,\n",
    "            stride = 1,\n",
    "            with_bias=False)(x)\n",
    "\n",
    "        # 3rd conv layer\n",
    "        self.batch_norm_c3 = lambda x, is_training: hk.BatchNorm(True, True, 0.999)(x, is_training)\n",
    "        self.conv3 = lambda x: hk.Conv1D(\n",
    "            cha_hidden,\n",
    "            kernel_shape = 3,\n",
    "            stride = 1,\n",
    "            with_bias = False)(x)\n",
    "\n",
    "        # 4th conv layer\n",
    "        self.batch_norm_c4 = lambda x, is_training: hk.BatchNorm(True, True, 0.999)(x, is_training)\n",
    "        self.conv4 = lambda x: hk.Conv1D(\n",
    "            cha_hidden,\n",
    "            kernel_shape=5,\n",
    "            stride=1,\n",
    "            # feature_group_count=cha_hidden,\n",
    "            with_bias=False)(x)\n",
    "\n",
    "        self.avg_po_c4 = lambda x: hk.AvgPool(window_shape=4, strides=2, padding=\"VALID\")(x)\n",
    "\n",
    "        self.flt = lambda x: hk.Flatten()(x)\n",
    "        self.batch_norm2 = lambda x, is_training: hk.BatchNorm(True, True, 0.999)(x, is_training)\n",
    "        self.dense2 = lambda x: hk.Linear(1, with_bias=False)(x)\n",
    "\n",
    "        self._forward = hk.transform_with_state(self._forward_fn)\n",
    "        self.loss = jax.jit(self.loss)\n",
    "        self.update = jax.jit(self.update)\n",
    "\n",
    "    def dropout(self, rng, rate, x, is_training):\n",
    "        if is_training:\n",
    "            return hk.dropout(rng, rate, x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _forward_fn(self, x, is_training):\n",
    "        key = hk.next_rng_key()\n",
    "        keys = jax.random.split(key, 5)\n",
    "\n",
    "        # x = self.batch_norm1(x, is_training)\n",
    "        x = self.dropout(keys[0], self.dropout_input, x, is_training)\n",
    "        x = self.dense1(x)\n",
    "        x = jax.nn.celu(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_input, self.cha_input_reshape)\n",
    "\n",
    "        x = self.batch_norm_c1(x, is_training)\n",
    "        x = self.conv1(x)\n",
    "        x = self.act_fn(x)\n",
    "\n",
    "        x = self.ave_po_c1(x)\n",
    "\n",
    "        x = self.batch_norm_c2(x, is_training)\n",
    "        x = self.dropout(keys[1], self.dropout_hidden, x, is_training)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act_fn(x)\n",
    "        x_s = x\n",
    "\n",
    "        x = self.batch_norm_c3(x, is_training)\n",
    "        x = self.dropout(keys[2], self.dropout_hidden, x, is_training)\n",
    "        x = self.conv3(x)\n",
    "        x = self.act_fn(x)\n",
    "\n",
    "        x = self.batch_norm_c4(x, is_training)\n",
    "        x = self.conv4(x)\n",
    "        x = x + x_s # skip connection\n",
    "        x = self.act_fn(x)\n",
    "\n",
    "        x = self.avg_po_c4(x)\n",
    "\n",
    "        x = self.flt(x)\n",
    "\n",
    "        x = self.batch_norm2(x, is_training)\n",
    "        x = self.dropout(keys[4], self.dropout_output, x, is_training)\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def init(self, rng, x):\n",
    "        params, net_state = self._forward.init(rng, x, is_training=True)\n",
    "        opt_state = self.optimiser.init(params)\n",
    "        return TrainingState(params, params, opt_state, net_state)\n",
    "\n",
    "    def apply(self, params, net_state, x, key, is_training=True):\n",
    "        return self._forward.apply(params, net_state, key, x, is_training)\n",
    "\n",
    "\n",
    "    def update(self, key, train_state, x, y):\n",
    "        params, avg_params, opt_state, net_state = train_state\n",
    "        grads, net_state = jax.grad(self.loss, has_aux=True)(params, net_state, key, x, y)\n",
    "        updates, opt_state = self.optimiser.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        avg_params = optax.incremental_update(params, avg_params, step_size=0.8)\n",
    "        return TrainingState(params, avg_params, opt_state, net_state)\n",
    "\n",
    "\n",
    "\n",
    "    def loss(self, params, net_state, key, x, y):\n",
    "        preds_mean, state = self.apply(params, net_state, x, key, is_training=True)\n",
    "        # preds_mean, preds_std = jnp.split(preds, [1], axis=-1)\n",
    "        # preds_std = jax.nn.softplus(preds_std)\n",
    "        preds_mean = preds_mean.squeeze()\n",
    "        l2_loss = jnp.mean(optax.l2_loss(y, preds_mean)) / x.shape[0]\n",
    "        l2_reg = 0.5 * sum(\n",
    "                jnp.sum(jnp.square(p)) for p in jax.tree_util.tree_leaves(params))\n",
    "\n",
    "        return l2_loss + self.l2_coef*l2_reg, state\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import torch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_nn_model(rng_key, train_loader, val_data, epochs, lr_0, max_lr, l2_coef,\n",
    "                   dropout_input=0.1, dropout_hidden=0.1, dropout_output=0.1,\n",
    "                   act_fn=jax.nn.relu, patience=20):\n",
    "\n",
    "\n",
    "\n",
    "    num_batches = len(train_loader)\n",
    "    total_steps = num_batches*epochs\n",
    "\n",
    "    print(f\"Num batches: {num_batches}, Total steps: {total_steps}\")\n",
    "\n",
    "    # step_size_fn = lambda count: lr_0\n",
    "\n",
    "    step_size_fn = optax.linear_onecycle_schedule(transition_steps=total_steps, peak_value=max_lr, div_factor=1e3, pct_start=0.1)\n",
    "    optim = optax.chain(optax.scale_by_schedule(step_size_fn), optax.adam(lr_0))\n",
    "\n",
    "    model = Conv1DNet(optim, dropout_input=dropout_input, dropout_hidden=dropout_hidden, dropout_output=dropout_output, act_fn=act_fn, l2_coef=l2_coef)\n",
    "\n",
    "    train_state = model.init(rng_key ,next(iter(train_loader))[0])\n",
    "    # states = []\n",
    "    val_losses = []\n",
    "    step = 0\n",
    "    key = rng_key\n",
    "    early_stopping = 0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            _, key = jax.random.split(key, 2)\n",
    "            train_state = model.update(key, train_state, batch_x, batch_y)\n",
    "            # states.append(train_state)\n",
    "            step += 1\n",
    "        val_loss  = eval_nn_model(key, model, val_data.data, val_data.target, train_state)\n",
    "        if epoch != 0 and (val_loss > val_losses[-1]):\n",
    "            early_stopping += 1\n",
    "            if early_stopping > patience:\n",
    "                print(f\"Early stopping at epoch: {epoch}, Total train steps: {step}\")\n",
    "                break\n",
    "        else:\n",
    "            early_stopping = 0\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Total number of steps trained: {step}\")\n",
    "    return model, train_state, val_losses\n",
    "\n",
    "def eval_nn_model(rng_key, model, X, y, state):\n",
    "    preds_mean, _ = model.apply(state.avg_params, state.net_state, X, rng_key, is_training=False)\n",
    "    preds_mean = preds_mean.squeeze()\n",
    "    # print(preds_mean)\n",
    "\n",
    "    rmse = jnp.sqrt(jnp.mean((y - preds_mean)**2))\n",
    "\n",
    "    return rmse\n",
    "\n",
    "def score_nn_model(rng_key, model, X, y, state):\n",
    "    preds_mean, _ = model.apply(state.avg_params, state.net_state, X, rng_key, is_training=False)\n",
    "    preds_mean = preds_mean.squeeze()\n",
    "    # print(preds_mean)\n",
    "\n",
    "    rmse = jnp.sqrt(jnp.mean((y - preds_mean)**2))\n",
    "    r2 = r2_score(y, preds_mean)\n",
    "\n",
    "    return rmse, r2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "outputs": [],
   "source": [
    "from smac.facade.smac_hpo_facade import SMAC4HPO\n",
    "from smac.scenario.scenario import Scenario\n",
    "from smac.configspace import ConfigurationSpace\n",
    "from ConfigSpace.hyperparameters import (\n",
    "    CategoricalHyperparameter,\n",
    "    UniformFloatHyperparameter,\n",
    "    UniformIntegerHyperparameter,\n",
    ")\n",
    "\n",
    "from ConfigSpace import InCondition, Configuration\n",
    "\n",
    "import math\n",
    "\n",
    "def get_configspace_bnn_bg()-> ConfigurationSpace:\n",
    "\n",
    "    # Build Configuration Space which defines all parameters and their ranges.\n",
    "    cs  = ConfigurationSpace()\n",
    "    lr = CategoricalHyperparameter(\"lr\", [1e-4, 1e-3, 1e-2], default_value=1e-3)\n",
    "    disc_lr = CategoricalHyperparameter(\"disc_lr\", [0.1, 0.3, 0.5, 0.6, 0.8, 0.9], default_value=0.5)\n",
    "    max_lr = UniformFloatHyperparameter(\"max_lr\", 0.1, 2.0)\n",
    "    batch_size = CategoricalHyperparameter(\"batch_size\", [32, 64, 80], default_value=32)\n",
    "\n",
    "    eta = UniformFloatHyperparameter(\"eta\", 1.0, 10)\n",
    "    mu = UniformFloatHyperparameter(\"mu\", 1e-5, 1.0)\n",
    "\n",
    "    dropout_hidden = CategoricalHyperparameter(\"dropout_hidden\", [0.0, 0.1, 0.2, 0.3, 0.4, 0.5], default_value=0.2)\n",
    "    dropout_output = CategoricalHyperparameter(\"dropout_output\", [0.0, 0.1, 0.2, 0.3, 0.4, 0.5], default_value=0.2)\n",
    "\n",
    "    l2_coef = UniformFloatHyperparameter(\"l2_coef\", 1e-10, 1e-4)\n",
    "\n",
    "    cs.add_hyperparameters([lr, disc_lr, max_lr, batch_size, eta, mu, dropout_hidden, dropout_output, l2_coef])\n",
    "\n",
    "    return cs\n",
    "\n",
    "def get_configspace_nn()-> ConfigurationSpace:\n",
    "\n",
    "    # Build Configuration Space which defines all parameters and their ranges.\n",
    "    cs  = ConfigurationSpace()\n",
    "    lr = CategoricalHyperparameter(\"lr\", [1e-4, 1e-3, 1e-2], default_value=1e-3)\n",
    "    max_lr = UniformFloatHyperparameter(\"max_lr\", 0.1, 2.0)\n",
    "    batch_size = CategoricalHyperparameter(\"batch_size\", [32, 64, 80], default_value=32)\n",
    "\n",
    "    dropout_input = CategoricalHyperparameter(\"dropout_input\", [0.0, 0.1, 0.2, 0.3, 0.4, 0.5], default_value=0.2)\n",
    "    dropout_hidden = CategoricalHyperparameter(\"dropout_hidden\", [0.0, 0.1, 0.2, 0.3, 0.4, 0.5], default_value=0.2)\n",
    "    dropout_output = CategoricalHyperparameter(\"dropout_output\", [0.0, 0.1, 0.2, 0.3, 0.4, 0.5], default_value=0.2)\n",
    "\n",
    "    l2_coef = UniformFloatHyperparameter(\"l2_coef\", 1e-10, 1e-4)\n",
    "\n",
    "    cs.add_hyperparameters([lr, max_lr, batch_size, dropout_input, dropout_hidden, dropout_output, l2_coef])\n",
    "\n",
    "    return cs\n",
    "\n",
    "def generate_train_nn_cs(seed, X_train, X_val, y_train, y_val):\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    def train_cs(config: Configuration)-> float:\n",
    "        epochs = 1000\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        max_lr = config[\"max_lr\"]\n",
    "        lr_0 = config[\"lr\"]\n",
    "\n",
    "        dropout_input = config[\"dropout_input\"]\n",
    "        dropout_hidden = config[\"dropout_hidden\"]\n",
    "        dropout_output = config[\"dropout_output\"]\n",
    "\n",
    "        l2_coef = config[\"l2_coef\"]\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        data_loader = NumpyLoader(NumpyData(X_train, y_train), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        val_data = NumpyData(X_val, y_val)\n",
    "        nn_model, state, _ = train_nn_model(rng_key, data_loader, val_data, epochs, lr_0, max_lr, l2_coef,\n",
    "                                                     dropout_input=dropout_input, dropout_hidden=dropout_hidden, dropout_output=dropout_output,\n",
    "                                                     act_fn=jax.nn.swish, patience=10)\n",
    "\n",
    "\n",
    "        val_loss  = eval_nn_model(rng_key, nn_model, val_data.data, val_data.target, state)\n",
    "        if math.isnan(val_loss):\n",
    "            return 1e9\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "\n",
    "    return train_cs\n",
    "\n",
    "def generate_train_bg_nn_cs(seed, X_train, X_val, y_train, y_val, J):\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    def train_cs(config: Configuration)-> float:\n",
    "        epochs = 1000\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        max_lr = config[\"max_lr\"]\n",
    "        lr_0 = config[\"lr\"]\n",
    "\n",
    "        disc_lr_0 = config[\"disc_lr\"]\n",
    "        eta = config[\"eta\"]\n",
    "        mu = config[\"mu\"]\n",
    "\n",
    "        dropout_hidden = config[\"dropout_hidden\"]\n",
    "        dropout_output = config[\"dropout_output\"]\n",
    "\n",
    "        l2_coef = config[\"l2_coef\"]\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        data_loader = NumpyLoader(NumpyData(X_train, y_train), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        val_data = NumpyData(X_val, y_val)\n",
    "        nn_model, state, _ = train_bg_nn_model(rng_key, data_loader, val_data, epochs, lr_0, disc_lr_0, max_lr, l2_coef,\n",
    "                                            J, eta, mu, dropout_hidden=dropout_hidden, dropout_output=dropout_output,\n",
    "                                            act_fn=jax.nn.swish, patience=10)\n",
    "\n",
    "\n",
    "        val_loss  = eval_bg_nn_model(rng_key, nn_model, val_data.data, val_data.target, state)\n",
    "        if math.isnan(val_loss):\n",
    "            return 1e9\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "\n",
    "    return train_cs\n",
    "\n",
    "def optimize_hyper_parameters(seed, X_train, X_val, y_train, y_val, total_time=60, bg=False, J=None):\n",
    "\n",
    "    if bg:\n",
    "        cs = get_configspace_bnn_bg()\n",
    "        train_cs = generate_train_bg_nn_cs(seed, X_train, X_val, y_train, y_val, J)\n",
    "    else:\n",
    "        cs = get_configspace_nn()\n",
    "        train_cs = generate_train_nn_cs(seed, X_train, X_val, y_train, y_val)\n",
    "\n",
    "    scenario = Scenario({\n",
    "        \"run_obj\": \"quality\",\n",
    "        \"wallclock-limit\": total_time,\n",
    "        \"cs\": cs,\n",
    "        \"deterministic\": True,\n",
    "        \"cutoff\": 10,  # runtime limit for the target algorithm\n",
    "        \"seed\": seed\n",
    "    })\n",
    "\n",
    "    smac = SMAC4HPO(scenario=scenario, rng=np.random.RandomState(seed), tae_runner=train_cs)\n",
    "\n",
    "    tae = smac.get_tae_runner()\n",
    "\n",
    "    try:\n",
    "        incumbent = smac.optimize()\n",
    "\n",
    "    finally:\n",
    "        incumbent = smac.solver.incumbent\n",
    "\n",
    "\n",
    "    inc_val = tae.run(config=incumbent, seed=seed)\n",
    "\n",
    "    return incumbent, inc_val[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration(values={\n",
      "  'batch_size': 32,\n",
      "  'dropout_hidden': 0.5,\n",
      "  'dropout_input': 0.2,\n",
      "  'dropout_output': 0.1,\n",
      "  'l2_coef': 4.928562866785006e-06,\n",
      "  'lr': 0.001,\n",
      "  'max_lr': 1.0662692129611968,\n",
      "})\n",
      "\n",
      "0.5059036754755423\n"
     ]
    }
   ],
   "source": [
    "# config, score = optimize_hyper_parameters(seed, X_train, X_val, y_train, y_val, total_time=300)\n",
    "print(config)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num batches: 8, Total steps: 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 414/1000 [00:45<01:04,  9.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch: 414, Total train steps: 3320\n",
      "Total number of steps trained: 3320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "rng_key = jax.random.PRNGKey(seed)\n",
    "epochs = 1000\n",
    "batch_size = config[\"batch_size\"]\n",
    "lr_0, max_lr = config[\"lr\"], config[\"max_lr\"]\n",
    "hidden_size = 4096\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "data_loader = NumpyLoader(NumpyData(X_train, y_train), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_data = NumpyData(X_val, y_val)\n",
    "\n",
    "nn_model, state, val_losses = train_nn_model(rng_key, data_loader, val_data, epochs, lr_0, max_lr,\n",
    "                                             dropout_input=config[\"dropout_input\"], dropout_hidden=config[\"dropout_hidden\"],\n",
    "                                             dropout_output=config[\"dropout_output\"], l2_coef=config[\"l2_coef\"],\n",
    "                                             act_fn=jax.nn.swish, patience=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.5059036754755423, r2_score: 0.1048661095859238\n",
      "Test RMSE: 0.4792797933885309, r2_score: 0.19038852881809532\n"
     ]
    }
   ],
   "source": [
    "rmse_val, r2_val = score_nn_model(rng_key, nn_model, X_val, y_val, state)\n",
    "rmse_test, r2_test = score_nn_model(rng_key, nn_model, X_test, y_test, state)\n",
    "print(f\"Val RMSE: {rmse_val}, r2_score: {r2_val}\")\n",
    "print(f\"Test RMSE: {rmse_test}, r2_score: {r2_test}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "11862"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J = np.load(f\"{data_dir}/cell_line/cancer_genes_net.npy\")\n",
    "np.count_nonzero(J)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [],
   "source": [
    "from optim_util import *\n",
    "class BgTrainingState(NamedTuple):\n",
    "    params: hk.Params\n",
    "    avg_params: hk.Params\n",
    "    opt_state: optax.OptState\n",
    "    net_state: hk.State\n",
    "    disc_state: Pytree\n",
    "    avg_disc_state: Pytree\n",
    "    disc_opt_state: OptaxSGLDState"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "outputs": [],
   "source": [
    "class BGConv1DNet:\n",
    "    def __init__(self, optim, disc_optim, J, eta, mu, hidden_size=4096, cha_input=16, cha_hidden=32,\n",
    "                 dropout_hidden=0.2, dropout_output=0.2, l2_coef=1e-5, act_fn=jax.nn.relu):\n",
    "\n",
    "        self.act_fn = act_fn\n",
    "        self.optimiser = optim\n",
    "        self.disc_optimiser = disc_optim\n",
    "        self.loss = jax.jit(self.loss)\n",
    "        self.update = jax.jit(self.update)\n",
    "\n",
    "        self.J = J\n",
    "        self.eta = eta\n",
    "        self.mu = mu\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cha_input = cha_input\n",
    "        self.cha_input_reshape = int(hidden_size/cha_input)\n",
    "        self.cha_hidden = cha_hidden\n",
    "        self.dropout_hidden = dropout_hidden\n",
    "        self.dropout_output = dropout_output\n",
    "        self.l2_coef = l2_coef\n",
    "\n",
    "        self.batch_norm1 = lambda x, is_training: hk.BatchNorm(True, True, 0.999)(x, is_training)\n",
    "        self.dense1 = lambda x: hk.Linear(hidden_size, with_bias=False)(x)\n",
    "\n",
    "        # 1st conv layer\n",
    "        self.batch_norm_c1 = lambda x, is_training: hk.BatchNorm(True, True, 0.999)(x, is_training)\n",
    "        self.conv1 = lambda x: hk.Conv1D(\n",
    "            cha_input,\n",
    "            kernel_shape=5,\n",
    "            stride = 1,\n",
    "            padding = \"VALID\",\n",
    "            # feature_group_count=cha_input,\n",
    "            with_bias=False)(x)\n",
    "\n",
    "        self.ave_po_c1 = lambda x: hk.AvgPool(window_shape=4, strides=1, padding=\"VALID\")(x)\n",
    "\n",
    "        #2nd conv layer\n",
    "        self.batch_norm_c2 = lambda x, is_training: hk.BatchNorm(True, True, 0.999)(x, is_training)\n",
    "        self.conv2 = lambda x: hk.Conv1D(\n",
    "            cha_hidden,\n",
    "            kernel_shape = 3,\n",
    "            stride = 1,\n",
    "            with_bias=False)(x)\n",
    "\n",
    "        # 3rd conv layer\n",
    "        self.batch_norm_c3 = lambda x, is_training: hk.BatchNorm(True, True, 0.999)(x, is_training)\n",
    "        self.conv3 = lambda x: hk.Conv1D(\n",
    "            cha_hidden,\n",
    "            kernel_shape = 3,\n",
    "            stride = 1,\n",
    "            with_bias = False)(x)\n",
    "\n",
    "        # 4th conv layer\n",
    "        self.batch_norm_c4 = lambda x, is_training: hk.BatchNorm(True, True, 0.999)(x, is_training)\n",
    "        self.conv4 = lambda x: hk.Conv1D(\n",
    "            cha_hidden,\n",
    "            kernel_shape=5,\n",
    "            stride=1,\n",
    "            # feature_group_count=cha_hidden,\n",
    "            with_bias=False)(x)\n",
    "\n",
    "        self.avg_po_c4 = lambda x: hk.AvgPool(window_shape=4, strides=2, padding=\"VALID\")(x)\n",
    "\n",
    "        self.flt = lambda x: hk.Flatten()(x)\n",
    "        self.batch_norm2 = lambda x, is_training: hk.BatchNorm(True, True, 0.999)(x, is_training)\n",
    "        self.dense2 = lambda x: hk.Linear(1, with_bias=False)(x)\n",
    "\n",
    "        self._forward = hk.transform_with_state(self._forward_fn)\n",
    "        self.loss = jax.jit(self.loss)\n",
    "        self.update = jax.jit(self.update)\n",
    "\n",
    "    def dropout(self, rng, rate, x, is_training):\n",
    "        if is_training:\n",
    "            return hk.dropout(rng, rate, x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _forward_fn(self, x, gamma, is_training):\n",
    "\n",
    "        if is_training:\n",
    "            x = x @ jnp.diag(gamma)\n",
    "        key = hk.next_rng_key()\n",
    "        keys = jax.random.split(key, 3)\n",
    "\n",
    "        # x = self.batch_norm1(x, is_training)\n",
    "        x = self.dense1(x)\n",
    "        # x = jax.nn.celu(x)\n",
    "        x = self.act_fn(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_input, self.cha_input_reshape)\n",
    "\n",
    "        x = self.batch_norm_c1(x, is_training)\n",
    "        x = self.conv1(x)\n",
    "        x = self.act_fn(x)\n",
    "\n",
    "        x = self.ave_po_c1(x)\n",
    "\n",
    "        x = self.batch_norm_c2(x, is_training)\n",
    "        x = self.dropout(keys[0], self.dropout_hidden, x, is_training)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act_fn(x)\n",
    "        x_s = x\n",
    "\n",
    "        x = self.batch_norm_c3(x, is_training)\n",
    "        x = self.dropout(keys[1], self.dropout_hidden, x, is_training)\n",
    "        x = self.conv3(x)\n",
    "        x = self.act_fn(x)\n",
    "\n",
    "        x = self.batch_norm_c4(x, is_training)\n",
    "        x = self.conv4(x)\n",
    "        x = x + x_s # skip connection\n",
    "        x = self.act_fn(x)\n",
    "\n",
    "        x = self.avg_po_c4(x)\n",
    "\n",
    "        x = self.flt(x)\n",
    "\n",
    "        x = self.batch_norm2(x, is_training)\n",
    "        x = self.dropout(keys[2], self.dropout_output, x, is_training)\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def init(self, rng, x):\n",
    "        gamma = tfd.Bernoulli(0.5*jnp.ones(x.shape[-1])).sample(seed=rng)*1.\n",
    "        params, net_state = self._forward.init(rng, x, gamma, is_training=True)\n",
    "        opt_state = self.optimiser.init(params)\n",
    "        disc_opt_state = self.disc_optimiser.init(gamma)\n",
    "        return BgTrainingState(params, params, opt_state, net_state, gamma, gamma, disc_opt_state)\n",
    "\n",
    "    def apply(self, params, net_state, key, gamma, x, is_training):\n",
    "        return self._forward.apply(params, net_state, key, x, gamma, is_training)\n",
    "\n",
    "    def update(self, key, train_state, x, y):\n",
    "        params, avg_params, opt_state, net_state, gamma, avg_gamma, disc_opt_state = train_state\n",
    "        grads, net_state = jax.grad(self.loss, has_aux=True)(params, net_state, key, gamma, x, y)\n",
    "        updates, opt_state = self.optimiser.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        avg_params = optax.incremental_update(params, avg_params, step_size=0.001)\n",
    "\n",
    "        disc_grads, net_state = jax.grad(self.disc_loss, has_aux=True)(gamma, params, net_state, key, x, y)\n",
    "        gamma, disc_opt_state = self.disc_optimiser.update(key, gamma, disc_grads, disc_opt_state)\n",
    "        avg_gamma = optax.incremental_update(gamma, avg_gamma, step_size=0.01)\n",
    "        return BgTrainingState(params, avg_params, opt_state, net_state, gamma, avg_gamma, disc_opt_state)\n",
    "\n",
    "    def loss(self, params, net_state, key, gamma, x, y):\n",
    "        preds_mean, state = self.apply(params, net_state, key, gamma, x, is_training=True)\n",
    "        preds_mean = preds_mean.squeeze()\n",
    "        l2_loss = jnp.mean(optax.l2_loss(y, preds_mean)) / x.shape[0]\n",
    "        l2_reg = 0.5 * sum(\n",
    "            jnp.sum(jnp.square(p)) for p in jax.tree_util.tree_leaves(params))\n",
    "\n",
    "        return l2_loss + self.l2_coef*l2_reg, state\n",
    "\n",
    "    def disc_loss(self, gamma, params, net_state, key, x, y):\n",
    "\n",
    "        net_loss, state = self.loss(params, net_state, key, gamma, x, y)\n",
    "        log_prior = self.ising_prior(gamma)\n",
    "\n",
    "        return log_prior - net_loss, state\n",
    "\n",
    "    # def ising_prior(self, gamma):\n",
    "    #     \"\"\"Log probability of the Ising model - prior over the discrete variables\"\"\"\n",
    "    #     return self.eta*(gamma.T @ self.J @ gamma) + self.mu*jnp.sum(gamma)\n",
    "\n",
    "    def ising_prior(self, gamma):\n",
    "        x = (2 * gamma) - 1\n",
    "\n",
    "        xg = x @ self.J\n",
    "        xgx = (xg * x).sum(-1)\n",
    "        return self.eta*xgx - self.mu*jnp.sum(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import torch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_bg_nn_model(rng_key, train_loader, val_data, epochs, lr_0, disc_lr_0, max_lr, l2_coef,\n",
    "                   J, eta, mu, dropout_hidden=0.1, dropout_output=0.1,\n",
    "                   act_fn=jax.nn.relu, patience=10):\n",
    "\n",
    "\n",
    "\n",
    "    num_batches = len(train_loader)\n",
    "    total_steps = num_batches*epochs\n",
    "\n",
    "    print(f\"Num batches: {num_batches}, Total steps: {total_steps}\")\n",
    "\n",
    "    disc_step_size_fn = lambda count: disc_lr_0\n",
    "    disc_optim = disc_sgld_gradient_update(disc_step_size_fn, momentum_decay=0, preconditioner=get_identity_preconditioner())\n",
    "\n",
    "    step_size_fn = optax.linear_onecycle_schedule(transition_steps=total_steps, peak_value=max_lr, div_factor=1e3, pct_start=0.1)\n",
    "    optim = optax.chain(optax.scale_by_schedule(step_size_fn), optax.adam(lr_0))\n",
    "\n",
    "    model = BGConv1DNet(optim, disc_optim, J, eta, mu, dropout_hidden=dropout_hidden, dropout_output=dropout_output, act_fn=act_fn, l2_coef=l2_coef)\n",
    "\n",
    "    train_state = model.init(rng_key ,next(iter(train_loader))[0])\n",
    "    # disc_states = []\n",
    "    val_losses = []\n",
    "    step = 0\n",
    "    key = rng_key\n",
    "    early_stopping = 0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            _, key = jax.random.split(key, 2)\n",
    "            train_state = model.update(key, train_state, batch_x, batch_y)\n",
    "            # disc_states.append(train_state.disc_state)\n",
    "            step += 1\n",
    "        val_loss  = eval_bg_nn_model(key, model, val_data.data, val_data.target, train_state)\n",
    "        if epoch != 0 and (val_loss > val_losses[-1]):\n",
    "            early_stopping += 1\n",
    "            if early_stopping > patience:\n",
    "                print(f\"Early stopping at epoch: {epoch}, Total train steps: {step}\")\n",
    "                break\n",
    "        else:\n",
    "            early_stopping = 0\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Total number of steps trained: {step}\")\n",
    "    return model, train_state, val_losses\n",
    "\n",
    "def eval_bg_nn_model(rng_key, model, X, y, state):\n",
    "    # disc_state = jax.random.bernoulli(rng_key, state.avg_disc_state)\n",
    "    preds_mean, _ = model.apply(state.avg_params, state.net_state, rng_key, state.disc_state, X, is_training=False)\n",
    "    preds_mean = preds_mean.squeeze()\n",
    "    # print(preds_mean)\n",
    "\n",
    "    rmse = jnp.sqrt(jnp.mean((y - preds_mean)**2))\n",
    "\n",
    "    return rmse\n",
    "\n",
    "def score_bg_nn_model(rng_key, model, X, y, state):\n",
    "    # disc_state = jax.random.bernoulli(rng_key, state.avg_disc_state)\n",
    "    preds_mean, _ = model.apply(state.avg_params, state.net_state, rng_key, state.disc_state, X, is_training=False)\n",
    "    preds_mean = preds_mean.squeeze()\n",
    "    # print(preds_mean)\n",
    "\n",
    "    rmse = jnp.sqrt(jnp.mean((y - preds_mean)**2))\n",
    "    r2 = r2_score(y, preds_mean)\n",
    "\n",
    "    return rmse, r2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'seed': 261}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num batches: 3, Total steps: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:42<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps trained: 3000\n",
      "Num batches: 8, Total steps: 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 653/1000 [01:41<00:53,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch: 653, Total train steps: 5232\n",
      "Total number of steps trained: 5232\n",
      "Num batches: 8, Total steps: 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 522/1000 [01:22<01:15,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch: 522, Total train steps: 4184\n",
      "Total number of steps trained: 4184\n",
      "Num batches: 4, Total steps: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:53<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps trained: 4000\n",
      "Num batches: 8, Total steps: 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 522/1000 [01:20<01:13,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch: 522, Total train steps: 4184\n",
      "Total number of steps trained: 4184\n",
      "Configuration(values={\n",
      "  'batch_size': 32,\n",
      "  'disc_lr': 0.5,\n",
      "  'dropout_hidden': 0.2,\n",
      "  'dropout_output': 0.2,\n",
      "  'eta': 5.5,\n",
      "  'l2_coef': 5e-05,\n",
      "  'lr': 0.001,\n",
      "  'max_lr': 1.05,\n",
      "  'mu': 0.500005,\n",
      "})\n",
      "\n",
      "0.5294856395751633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bg_config, bg_score = optimize_hyper_parameters(seed, X_train, X_val, y_train, y_val, total_time=300, bg=True, J=J)\n",
    "print(bg_config)\n",
    "print(bg_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num batches: 8, Total steps: 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 522/1000 [01:21<01:14,  6.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch: 522, Total train steps: 4184\n",
      "Total number of steps trained: 4184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "rng_key = jax.random.PRNGKey(seed)\n",
    "epochs = 1000\n",
    "batch_size = bg_config[\"batch_size\"]\n",
    "disc_lr_0 = bg_config[\"disc_lr\"]\n",
    "lr_0, max_lr = bg_config[\"lr\"], bg_config[\"max_lr\"]\n",
    "eta, mu = bg_config[\"eta\"], bg_config[\"mu\"]\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "data_loader = NumpyLoader(NumpyData(X_train, y_train), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_data = NumpyData(X_val, y_val)\n",
    "\n",
    "bg_nn_model, bg_state, val_losses = train_bg_nn_model(rng_key, data_loader, val_data, epochs, lr_0, disc_lr_0, max_lr,bg_config[\"l2_coef\"],\n",
    "                                             J, eta, mu, dropout_hidden=bg_config[\"dropout_hidden\"],\n",
    "                                             dropout_output=bg_config[\"dropout_output\"],\n",
    "                                             act_fn=jax.nn.swish, patience=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x7fcd48301df0>]"
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.5294856395751633, r2_score: 0.01947041337241684\n",
      "Test RMSE: 0.539304877400437, r2_score: -0.02510206938640458\n"
     ]
    }
   ],
   "source": [
    "rmse_val, r2_val = score_bg_nn_model(rng_key, bg_nn_model, X_val, y_val, bg_state)\n",
    "rmse_test, r2_test = score_bg_nn_model(rng_key, bg_nn_model, X_test, y_test, bg_state)\n",
    "print(f\"Val RMSE: {rmse_val}, r2_score: {r2_val}\")\n",
    "print(f\"Test RMSE: {rmse_test}, r2_score: {r2_test}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([  2.,  49.,  23.,   8.,   0.,   0.,   0.,   0.,   0., 686.]),\n array([0.1639665 , 0.24756985, 0.3311732 , 0.41477655, 0.4983799 ,\n        0.58198325, 0.6655866 , 0.74918995, 0.8327933 , 0.91639665,\n        1.        ]),\n <BarContainer object of 10 artists>)"
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAASQklEQVR4nO3df5CdVX3H8fdtNv4YpaRwbSabxMEOUUuZAkqBjm1HiTiEUsJM9au0asBU6giODrYl9pe2/SfUGWn+sIw/UJKOA3xLa8nUVOtEGMeOoQpV20ptIw35QSAshGjKaBLn9o97ojvM7j53d+/eu/fk/ZrZuc9znnP3nPnO5pMnZ8990up0OkiS6vJTw56AJKn/DHdJqpDhLkkVMtwlqUKGuyRVaGzYEyjcsiNJc9OaqnGxhDuPPfbYwMZqt9tMTEwMbLxRZI2aWaNm1qjZfGo0Pj4+7TWXZSSpQo137hHxCuDuSU0/B/wpsK20nwXsASIzD0dEC9gCXAE8C1ybmQ/1d9qSpJk03rln5ncy8/zMPB94Nd3A/iywCdiZmWuAneUcYB2wpnxdD9y2APOWJM1gtssya4HvZuajwHpga2nfClxdjtcD2zKzk5m7gGURsaIfk5Uk9Wa2v1B9C3BnOV6emQfL8ePA8nK8Etg36T37S9tBZtBut2c5lbkbGxsb6HijyBo1s0bNrFGzhapRz+EeEc8DrgI+8NxrmdmJiHltZxzkb9T9DX4za9TMGjWzRs0Ww26ZdcBDmflEOX/i5HJLeT1U2g8Aqye9b1VpkyQNyGyWZa7hJ0syANuBDcDm8nrvpPYbI+Iu4GLgyKTlG0nSAPQU7hHxIuAy4HcnNW8GMiI2Ao8CUdp30N0GuZvuzprr+jZbSVJPWovkP+vo+AnVxcUaNbNGzUalRksOT8DTTw5l7BeMr+b/XvjiOb23rLkv7scPSNLQPP0kxzbfPJShl/7JR+ClZ/f9+/r4AUmqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKjfXSKSKWAZ8EzgU6wDuA7wB3A2cBe4DIzMMR0QK2AFcAzwLXZuZD/Z64JGl6vd65bwE+n5mvBM4DHgY2ATszcw2ws5wDrAPWlK/rgdv6OmNJUqPGcI+I04FfA24HyMxjmfkMsB7YWrptBa4ux+uBbZnZycxdwLKIWNHneUuSZtDLsszLgCeBT0fEecCDwHuB5Zl5sPR5HFhejlcC+ya9f39pO8gM2u32LKY9P2NjYwMdbxRZo2bWqNmo1Ojo3qUcG9LYrVZrQWrUS7iPAa8C3pOZD0TEFn6yBANAZnYiojOfiUxMTMzn7bPSbrcHOt4oskbNrFGzUanRkuPHhzZ2p9OZc43Gx8envdbLmvt+YH9mPlDO76Eb9k+cXG4pr4fK9QPA6knvX1XaJEkD0hjumfk4sC8iXlGa1gLfBrYDG0rbBuDecrwdeHtEtCLiEuDIpOUbSdIA9LQVEngP8JmIeB7wCHAd3b8YMiI2Ao8CUfruoLsNcjfdrZDX9XXGkqRGPYV7Zn4DuHCKS2un6NsBbpjftCRJ8+EnVCWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqNNZLp4jYA3wf+BFwIjMvjIgzgLuBs4A9QGTm4YhoAVuAK4BngWsz86H+T12SNJ3Z3Lm/LjPPz8wLy/kmYGdmrgF2lnOAdcCa8nU9cFu/JitJ6s18lmXWA1vL8Vbg6knt2zKzk5m7gGURsWIe40iSZqmnZRmgA/xzRHSAj2Xmx4HlmXmwXH8cWF6OVwL7Jr13f2k7yAza7XbPk56vsbGxgY43iqxRM2vUbFRqdHTvUo4NaexWq7UgNeo13H8lMw9ExM8CX4yI/5p8MTM7JfjnbGJiYj5vn5V2uz3Q8UaRNWpmjZqNSo2WHD8+tLE7nc6cazQ+Pj7ttZ6WZTLzQHk9BHwWuAh44uRyS3k9VLofAFZPevuq0iZJGpDGcI+IF0XEaSePgTcA/wFsBzaUbhuAe8vxduDtEdGKiEuAI5OWbyRJA9DLnfty4CsR8U3gX4HPZebngc3AZRHxP8DryznADuARYDfwCeDdfZ+1JGlGjWvumfkIcN4U7U8Ba6do7wA39GV2kqQ58ROqklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUa67VjRCwBvg4cyMwrI+JlwF3AmcCDwNsy81hEPB/YBrwaeAp4c2bu6fvMJUnTms2d+3uBhyed3wLcmplnA4eBjaV9I3C4tN9a+kmSBqincI+IVcCvA58s5y3gUuCe0mUrcHU5Xl/OKdfXlv6SpAHpdVnmr4A/AE4r52cCz2TmiXK+H1hZjlcC+wAy80REHCn9J2YaoN1u9z7reRobGxvoeKPIGjWzRs1GpUZH9y7l2JDGbrVaC1KjxnCPiCuBQ5n5YES8tu8zKCYmZsz+vmq32wMdbxRZo2bWqNmo1GjJ8eNDG7vT6cy5RuPj49Ne62VZ5jXAVRGxh+4vUC8FtgDLIuLkXw6rgAPl+ACwGqBcP53uL1YlSQPSGO6Z+YHMXJWZZwFvAb6Umb8N3Ae8sXTbANxbjreXc8r1L2Vmp6+zliTNaD773G8GboqI3XTX1G8v7bcDZ5b2m4BN85uiJGm2et7nDpCZ9wP3l+NHgIum6PMD4E19mJskaY78hKokVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShcaaOkTEC4AvA88v/e/JzA9GxMuAu4AzgQeBt2XmsYh4PrANeDXwFPDmzNyzQPOXJE2hlzv3HwKXZuZ5wPnA5RFxCXALcGtmng0cBjaW/huBw6X91tJPkjRAjeGemZ3MPFpOl5avDnApcE9p3wpcXY7Xl3PK9bUR0erXhCVJzRqXZQAiYgndpZezgY8C3wWeycwTpct+YGU5XgnsA8jMExFxhO7SzcRMY7Tb7VlPfq7GxsYGOt4oskbNrFGzUanR0b1LOTaksVut1oLUqKdwz8wfAedHxDLgs8Ar+z2RiYkZs7+v2u32QMcbRdaomTVqNio1WnL8+NDG7nQ6c67R+Pj4tNdmtVsmM58B7gN+GVgWESf/clgFHCjHB4DVAOX66XR/sSpJGpDGcI+Il5Q7diLihcBlwMN0Q/6NpdsG4N5yvL2cU65/KTM7fZyzJKlBL3fuK4D7IuJbwNeAL2bmPwI3AzdFxG66a+q3l/63A2eW9puATf2ftiRpJo1r7pn5LeCCKdofAS6aov0HwJv6MjtJ0pz4CVVJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekCo01dYiI1cA2YDnQAT6emVsi4gzgbuAsYA8QmXk4IlrAFuAK4Fng2sx8aGGmL0maSi937ieA92fmOcAlwA0RcQ6wCdiZmWuAneUcYB2wpnxdD9zW91lLkmbUGO6ZefDknXdmfh94GFgJrAe2lm5bgavL8XpgW2Z2MnMXsCwiVvR74pKk6TUuy0wWEWcBFwAPAMsz82C59DjdZRvoBv++SW/bX9oOMoN2uz2bqczL2NjYQMcbRdaomTVqNio1Orp3KceGNHar1VqQGvUc7hHxYuDvgPdl5vci4sfXMrMTEZ35TGRiYmI+b5+Vdrs90PFGkTVqZo2ajUqNlhw/PrSxO53OnGs0Pj4+7bWedstExFK6wf6ZzPz70vzEyeWW8nqotB8AVk96+6rSJkkakF52y7SA24GHM/Mjky5tBzYAm8vrvZPab4yIu4CLgSOTlm8kSQPQy7LMa4C3Af8eEd8obX9IN9QzIjYCjwIn12l20N0GuZvuVsjr+jlhSVKzxnDPzK8ArWkur52ifwe4YZ7zkiTNg59QlaQKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SarQWFOHiPgUcCVwKDPPLW1nAHcDZwF7gMjMwxHRArYAVwDPAtdm5kMLM3VJ0nR6uXO/A7j8OW2bgJ2ZuQbYWc4B1gFrytf1wG39maYkaTYawz0zvww8/Zzm9cDWcrwVuHpS+7bM7GTmLmBZRKzo01wlST2a65r78sw8WI4fB5aX45XAvkn99pc2SdIANa65N8nMTkR05vt92u32fL9Fz8bGxgY63iiyRs2sUbNRqdHRvUs5NqSxW63WgtRoruH+RESsyMyDZdnlUGk/AKye1G9VaWs0MTExx6nMXrvdHuh4o8gaNbNGzUalRkuOHx/a2J1OZ841Gh8fn/baXMN9O7AB2Fxe753UfmNE3AVcDByZtHwjSRqQXrZC3gm8FmhHxH7gg3RDPSNiI/AoEKX7DrrbIHfT3Qp53QLMWZLUoDHcM/OaaS6tnaJvB7hhvpOSJM2Pn1CVpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVKF5P/L3VLbk8AQ8/eTgBz7jJfzoZxb/Y1QlDY/hPh9PP8mxzTcPfNjnbboFDHdJM3BZRpIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIrZAjqDU2xpLvPrygYxzdu3Tq/xHePfbSSDDcR9H3v8exLX+2oEMcm6bdPfbSaHBZRpIqZLhLUoUWZFkmIi4HtgBLgE9m5uaFGEeDN4j1/im51i/NSt/DPSKWAB8FLgP2A1+LiO2Z+e1+j6UhGMB6/1Rc65dmZyHu3C8CdmfmIwARcRewHliQcJ/Lkxmn3QkyS60T8/8ekrQQFiLcVwL7Jp3vBy5uetP4+PjcRpvr+/rldZcPZ9w3/MZwxh322IvMnH9uTyEjUaPxcfjVrw97Fn21WLZCtoY9AUmqyULsljkArJ50vqq0SZIGZCHu3L8GrImIl9EN9bcAv7UA40iSptH3O/fMPAHcCHwBeLjblP/Z73EkSdNrdTqdYc9BktRnfkJVkipkuEtShRbLVsgF0fQYhIi4Cfgd4ATwJPCOzHx04BMdol4fFRERvwncA/xSZta1IbhBLzWKiAA+BHSAb2bmKbWJoIc/ay8FtgLLSp9Nmblj0PMcloj4FHAlcCgzz53ieotu/a4AngWuzcyH5jNmtXfukx6DsA44B7gmIs55Trd/Ay7MzF+kG1x/OdhZDlePNSIiTgPeCzww2BkOXy81iog1wAeA12TmLwDvG/Q8h6nHn6M/pru54gK6O+j+erCzHLo7gJk+8bgOWFO+rgdum++A1YY7kx6DkJnHgJOPQfixzLwvM58tp7vo7sk/lTTWqPgL4BbgB4Oc3CLRS43eCXw0Mw8DZOahAc9x2HqpUQf46XJ8OvDYAOc3dJn5ZeDpGbqsB7ZlZiczdwHLImLFfMasOdynegzCyhn6bwT+aUFntPg01igiXgWszszPDXJii0gvP0cvB14eEf8SEbvKEsWppJcafQh4a0TsB3YA7xnM1EbGbPOqUc3h3rOIeCtwIfDhYc9lMYmInwI+Arx/2HNZ5Mbo/nP6tcA1wCciYtkwJ7QIXQPckZmr6K4r/035+dICqbm4PT0GISJeD/wRcFVm/nBAc1ssmmp0GnAucH9E7AEuAbZHxIUDm+Hw9fJztB/YnpnHM/N/gf+mG/anil5qtBFIgMz8KvACwGc4/0TfH9tS826ZxscgRMQFwMeAy0/BdVJoqFFmHmHSH8CIuB/4vVNst0wvj9P4B7p3pp+OiDbdZZpHBjnJIeulRnuBtcAdEfHzdMN9ds/qrtt24MbyiPSLgSOZeXA+37DaO/fpHoMQEX8eEVeVbh8GXgz8bUR8IyK2D2m6Q9FjjU5pPdboC8BTEfFt4D7g9zPzqeHMePB6rNH7gXdGxDeBO+lu9TtlPh4fEXcCXwVeERH7I2JjRLwrIt5Vuuyge0OwG/gE8O75junjBySpQtXeuUvSqcxwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRX6f4faVoaANUfRAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(bg_state.avg_disc_state)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "outputs": [
    {
     "data": {
      "text/plain": "684"
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(bg_state.disc_state)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "outputs": [],
   "source": [
    "# bnn_feat_idx = np.argsort(bg_state.avg_disc_state)[::-1][:20]\n",
    "bnn_feat_idx = np.argsort(np.mean(np.array(disc_states), axis=0))[::-1][:20]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "outputs": [
    {
     "data": {
      "text/plain": "array([448, 589, 736, 180, 337, 306, 508, 181, 476, 243, 640, 587, 458,\n       468, 369,  81, 668, 120, 660, 580])"
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_feat_idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "outputs": [
    {
     "data": {
      "text/plain": "       NKX2-1     RPL10       WRN      EHD2       KDR     HTRA2     PLCB4  \\\n0    1.990720 -0.495631  0.574113 -1.421957 -5.199338  1.641107  0.409503   \n1    0.205464 -1.193795 -0.680601  1.118003  0.528865 -1.100033  0.357216   \n2   -5.199338  1.255486 -1.255486 -2.156973 -5.199338 -0.245329 -0.983053   \n3   -5.199338  1.765264 -0.077803 -0.680601 -5.199338  2.269206 -0.808844   \n4    2.662687 -1.213847  2.662687 -1.370462  0.398960  0.730448  1.015059   \n..        ...       ...       ...       ...       ...       ...       ...   \n254  5.199338 -1.449182  0.225352 -1.031459 -5.199338 -1.193795  0.907124   \n255  0.484675  0.644302  1.174212 -0.769055  0.255354 -1.322365  1.031459   \n256  0.822394  1.395747  0.048597 -0.336579 -5.199338  0.245329 -0.009716   \n257  0.305888 -0.305888 -1.213847  1.193795 -5.199338 -1.174212  1.604849   \n258 -5.199338  0.484675  0.409503 -0.692900  1.255486 -0.692900  0.430727   \n\n       EIF1AX    PABPC1     FGFR3    SOHLH2      ROS1       NRK     NUP98  \\\n0    0.585607 -0.936847 -0.316085  0.849957 -5.199338 -5.199338  0.409503   \n1   -0.205464 -0.597179  0.205464  0.205464  0.126641  1.213847  0.863984   \n2   -1.570585  1.118003 -0.175782 -5.199338 -5.199338 -5.199338  0.357216   \n3   -1.507108 -0.107076 -0.967422  1.118003 -5.199338 -5.199338 -0.346879   \n4    0.705305 -1.065120 -0.597179 -0.388462 -5.199338 -5.199338 -0.265405   \n..        ...       ...       ...       ...       ...       ...       ...   \n254  1.346025 -2.420390 -0.195551  0.305888 -5.199338 -5.199338  1.065120   \n255  0.265405  2.156973  0.983053  1.866185 -5.199338  1.234399 -1.924727   \n256 -0.068062 -1.604849  0.680601 -0.585607 -5.199338 -5.199338 -1.048141   \n257 -0.398960 -1.449182  0.029151  0.398960  0.822394  5.199338 -0.822394   \n258 -1.277147  0.000000  0.107076  0.146254  0.346879  1.720887 -0.717820   \n\n          LPP      CALR       SYK     CHEK2    STAT5B      RHOH  \n0    0.126641 -0.126641  1.299424 -0.452147 -0.087552  0.346879  \n1   -0.398960  0.705305 -0.136441  0.430727 -1.346025 -5.199338  \n2   -1.346025 -0.907124  2.066729  0.185658  0.029151  1.213847  \n3   -1.720887  0.107076  1.395747  0.644302  1.395747  1.031459  \n4   -1.370462 -1.155069  0.367591  0.680601  0.068062  0.495631  \n..        ...       ...       ...       ...       ...       ...  \n254 -0.255354  0.517724  1.604849  1.604849  0.849957  0.420092  \n255 -0.863984  0.136441  0.185658  0.998928 -1.048141  0.692900  \n256 -0.878182  0.921884 -0.205464 -0.769055 -0.632390 -5.199338  \n257  0.608832  1.136341 -0.097309 -0.585607 -1.924727 -0.585607  \n258 -0.574113  1.924727  0.087552  0.863984  1.015059 -0.388462  \n\n[259 rows x 20 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NKX2-1</th>\n      <th>RPL10</th>\n      <th>WRN</th>\n      <th>EHD2</th>\n      <th>KDR</th>\n      <th>HTRA2</th>\n      <th>PLCB4</th>\n      <th>EIF1AX</th>\n      <th>PABPC1</th>\n      <th>FGFR3</th>\n      <th>SOHLH2</th>\n      <th>ROS1</th>\n      <th>NRK</th>\n      <th>NUP98</th>\n      <th>LPP</th>\n      <th>CALR</th>\n      <th>SYK</th>\n      <th>CHEK2</th>\n      <th>STAT5B</th>\n      <th>RHOH</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.990720</td>\n      <td>-0.495631</td>\n      <td>0.574113</td>\n      <td>-1.421957</td>\n      <td>-5.199338</td>\n      <td>1.641107</td>\n      <td>0.409503</td>\n      <td>0.585607</td>\n      <td>-0.936847</td>\n      <td>-0.316085</td>\n      <td>0.849957</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.409503</td>\n      <td>0.126641</td>\n      <td>-0.126641</td>\n      <td>1.299424</td>\n      <td>-0.452147</td>\n      <td>-0.087552</td>\n      <td>0.346879</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.205464</td>\n      <td>-1.193795</td>\n      <td>-0.680601</td>\n      <td>1.118003</td>\n      <td>0.528865</td>\n      <td>-1.100033</td>\n      <td>0.357216</td>\n      <td>-0.205464</td>\n      <td>-0.597179</td>\n      <td>0.205464</td>\n      <td>0.205464</td>\n      <td>0.126641</td>\n      <td>1.213847</td>\n      <td>0.863984</td>\n      <td>-0.398960</td>\n      <td>0.705305</td>\n      <td>-0.136441</td>\n      <td>0.430727</td>\n      <td>-1.346025</td>\n      <td>-5.199338</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-5.199338</td>\n      <td>1.255486</td>\n      <td>-1.255486</td>\n      <td>-2.156973</td>\n      <td>-5.199338</td>\n      <td>-0.245329</td>\n      <td>-0.983053</td>\n      <td>-1.570585</td>\n      <td>1.118003</td>\n      <td>-0.175782</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.357216</td>\n      <td>-1.346025</td>\n      <td>-0.907124</td>\n      <td>2.066729</td>\n      <td>0.185658</td>\n      <td>0.029151</td>\n      <td>1.213847</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-5.199338</td>\n      <td>1.765264</td>\n      <td>-0.077803</td>\n      <td>-0.680601</td>\n      <td>-5.199338</td>\n      <td>2.269206</td>\n      <td>-0.808844</td>\n      <td>-1.507108</td>\n      <td>-0.107076</td>\n      <td>-0.967422</td>\n      <td>1.118003</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-0.346879</td>\n      <td>-1.720887</td>\n      <td>0.107076</td>\n      <td>1.395747</td>\n      <td>0.644302</td>\n      <td>1.395747</td>\n      <td>1.031459</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.662687</td>\n      <td>-1.213847</td>\n      <td>2.662687</td>\n      <td>-1.370462</td>\n      <td>0.398960</td>\n      <td>0.730448</td>\n      <td>1.015059</td>\n      <td>0.705305</td>\n      <td>-1.065120</td>\n      <td>-0.597179</td>\n      <td>-0.388462</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-0.265405</td>\n      <td>-1.370462</td>\n      <td>-1.155069</td>\n      <td>0.367591</td>\n      <td>0.680601</td>\n      <td>0.068062</td>\n      <td>0.495631</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>254</th>\n      <td>5.199338</td>\n      <td>-1.449182</td>\n      <td>0.225352</td>\n      <td>-1.031459</td>\n      <td>-5.199338</td>\n      <td>-1.193795</td>\n      <td>0.907124</td>\n      <td>1.346025</td>\n      <td>-2.420390</td>\n      <td>-0.195551</td>\n      <td>0.305888</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>1.065120</td>\n      <td>-0.255354</td>\n      <td>0.517724</td>\n      <td>1.604849</td>\n      <td>1.604849</td>\n      <td>0.849957</td>\n      <td>0.420092</td>\n    </tr>\n    <tr>\n      <th>255</th>\n      <td>0.484675</td>\n      <td>0.644302</td>\n      <td>1.174212</td>\n      <td>-0.769055</td>\n      <td>0.255354</td>\n      <td>-1.322365</td>\n      <td>1.031459</td>\n      <td>0.265405</td>\n      <td>2.156973</td>\n      <td>0.983053</td>\n      <td>1.866185</td>\n      <td>-5.199338</td>\n      <td>1.234399</td>\n      <td>-1.924727</td>\n      <td>-0.863984</td>\n      <td>0.136441</td>\n      <td>0.185658</td>\n      <td>0.998928</td>\n      <td>-1.048141</td>\n      <td>0.692900</td>\n    </tr>\n    <tr>\n      <th>256</th>\n      <td>0.822394</td>\n      <td>1.395747</td>\n      <td>0.048597</td>\n      <td>-0.336579</td>\n      <td>-5.199338</td>\n      <td>0.245329</td>\n      <td>-0.009716</td>\n      <td>-0.068062</td>\n      <td>-1.604849</td>\n      <td>0.680601</td>\n      <td>-0.585607</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-1.048141</td>\n      <td>-0.878182</td>\n      <td>0.921884</td>\n      <td>-0.205464</td>\n      <td>-0.769055</td>\n      <td>-0.632390</td>\n      <td>-5.199338</td>\n    </tr>\n    <tr>\n      <th>257</th>\n      <td>0.305888</td>\n      <td>-0.305888</td>\n      <td>-1.213847</td>\n      <td>1.193795</td>\n      <td>-5.199338</td>\n      <td>-1.174212</td>\n      <td>1.604849</td>\n      <td>-0.398960</td>\n      <td>-1.449182</td>\n      <td>0.029151</td>\n      <td>0.398960</td>\n      <td>0.822394</td>\n      <td>5.199338</td>\n      <td>-0.822394</td>\n      <td>0.608832</td>\n      <td>1.136341</td>\n      <td>-0.097309</td>\n      <td>-0.585607</td>\n      <td>-1.924727</td>\n      <td>-0.585607</td>\n    </tr>\n    <tr>\n      <th>258</th>\n      <td>-5.199338</td>\n      <td>0.484675</td>\n      <td>0.409503</td>\n      <td>-0.692900</td>\n      <td>1.255486</td>\n      <td>-0.692900</td>\n      <td>0.430727</td>\n      <td>-1.277147</td>\n      <td>0.000000</td>\n      <td>0.107076</td>\n      <td>0.146254</td>\n      <td>0.346879</td>\n      <td>1.720887</td>\n      <td>-0.717820</td>\n      <td>-0.574113</td>\n      <td>1.924727</td>\n      <td>0.087552</td>\n      <td>0.863984</td>\n      <td>1.015059</td>\n      <td>-0.388462</td>\n    </tr>\n  </tbody>\n</table>\n<p>259 rows × 20 columns</p>\n</div>"
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.iloc[:,bnn_feat_idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "outputs": [],
   "source": [
    "from gplearn.genetic import SymbolicTransformer, SymbolicClassifier, SymbolicRegressor\n",
    "from gplearn.functions import make_function\n",
    "import operator\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_best_programs(gp, num_models, classifier=True, ascending=True, sort_fit=\"OOB_fitness\"):\n",
    "    gp_dict = {'Gen': [], \"Ind\": [], \"Fitness\": [], 'OOB_fitness': [], \"Equation\": []}\n",
    "\n",
    "    if classifier:\n",
    "        for idGen in range(len(gp._programs)):\n",
    "            for idPopulation in range(gp.population_size):\n",
    "                gp_dict[\"Gen\"].append(idGen)\n",
    "                gp_dict[\"Ind\"].append(idPopulation)\n",
    "                gp_dict[\"Fitness\"].append(gp._programs[idGen][idPopulation].fitness_)\n",
    "                gp_dict[\"OOB_fitness\"].append(gp._programs[idGen][idPopulation].oob_fitness_)\n",
    "                gp_dict[\"Equation\"].append(str(gp._programs[idGen][idPopulation]))\n",
    "    else:\n",
    "        for idx, prog in enumerate(gp._programs[-1]):\n",
    "            gp_dict[\"Gen\"].append(-1)\n",
    "            gp_dict[\"Ind\"].append(idx)\n",
    "            gp_dict[\"Fitness\"].append(prog.fitness_)\n",
    "            gp_dict[\"OOB_fitness\"].append(prog.oob_fitness_)\n",
    "            gp_dict[\"Equation\"].append(str(prog))\n",
    "\n",
    "    gp_df = pd.DataFrame(gp_dict).sort_values(sort_fit, ascending=ascending)[:num_models]\n",
    "    programs = []\n",
    "    for i in range(num_models):\n",
    "        gen, ind = int(gp_df.iloc[i][\"Gen\"]), int(gp_df.iloc[i][\"Ind\"])\n",
    "        programs.append(gp._programs[gen][ind])\n",
    "\n",
    "    return programs, gp_df\n",
    "\n",
    "\n",
    "def gp_transform(est, X, classifier=False, num_models=100, sort_fit=\"Fitness\"):\n",
    "    if classifier or (sort_fit == \"OOB_fitness\"):\n",
    "        programs, gp_df = get_best_programs(est, num_models, classifier, sort_fit=sort_fit, ascending=classifier)\n",
    "        out = np.zeros((X.shape[0], len(programs)))\n",
    "        for i, prog in enumerate(programs):\n",
    "            out[:, i] = prog.execute(X)\n",
    "\n",
    "        return out, gp_df\n",
    "    else:\n",
    "        return est.transform(X), None\n",
    "\n",
    "function_set = ['add', 'sub', 'mul', 'div', 'sqrt', 'log',\n",
    "                'abs', 'neg', 'inv', 'max', 'min', 'exp']\n",
    "\n",
    "def train_linear_model(seed, X_train, X_test, y_train, y_test):\n",
    "    cv = KFold(n_splits=3, random_state=seed, shuffle=True)\n",
    "    param_grid = {\"alpha\": np.logspace(-2, 2, 20)}\n",
    "    grid_cv = GridSearchCV(estimator=Ridge(max_iter=10000), param_grid=param_grid,\n",
    "                           verbose=0, scoring=\"r2\", cv=cv).fit(X_train, y_train)\n",
    "    lin_model = Ridge(max_iter=10000, **grid_cv.best_params_)\n",
    "    lin_model.fit(X_train, y_train)\n",
    "    y_test_pred = lin_model.predict(X_test)\n",
    "\n",
    "    test_rmse_score = np.sqrt(np.mean((y_test - y_test_pred)**2))\n",
    "    test_r2_score = r2_score(y_test, y_test_pred)\n",
    "    test_pearson, test_pval = stats.pearsonr(y_test, y_test_pred)\n",
    "\n",
    "    return test_rmse_score, test_r2_score, test_pearson, test_pval\n",
    "\n",
    "def train_gp(seed, X_train, X_train_2, X_test, y_train, y_train_2, y_test, num_models=5, sort_fit=\"OOB_fitness\", verbose=0, num_gen=100,\n",
    "             p_cxvr=0.8, p_subt_mut=0.1, p_hmut=0.05, p_pmut=0.1, subsample=0.8, complexity_coef=0.05):\n",
    "    gp_est = SymbolicTransformer(population_size=1000, hall_of_fame=200, n_components=50, generations=num_gen,\n",
    "                                 p_crossover=p_cxvr, p_subtree_mutation=p_subt_mut,\n",
    "                                 p_hoist_mutation=p_hmut, p_point_mutation=p_pmut,\n",
    "                                 max_samples=subsample, verbose=verbose,\n",
    "                                 parsimony_coefficient=complexity_coef, random_state=seed)\n",
    "\n",
    "    gp_est.fit(X_train, y_train)\n",
    "\n",
    "    gp_features_train, gp_train_df = gp_transform(gp_est, X_train_2, classifier=False, sort_fit=sort_fit, num_models=num_models)\n",
    "    gp_features_test, gp_test_df = gp_transform(gp_est, X_test, classifier=False, sort_fit=sort_fit, num_models=num_models)\n",
    "\n",
    "    X_train_comb = np.concatenate([X_train_2, gp_features_train], axis=1)\n",
    "    X_test_comb = np.concatenate([X_test, gp_features_test], axis=1)\n",
    "\n",
    "\n",
    "    test_rmse_score, test_r2_score, test_pearson, test_pval = train_linear_model(seed, X_train_comb, X_test_comb, y_train_2, y_test)\n",
    "\n",
    "    return test_rmse_score, test_r2_score, test_pearson, test_pval, gp_test_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "outputs": [],
   "source": [
    "X_train_2, y_train_2 = jax.random.choice(rng_key, X_train, shape=(X_val.shape[0],), replace=False), jax.random.choice(rng_key, y_train, shape=(X_val.shape[0],), replace=False)\n",
    "X_gp_train_bnn, X_gp_train_2_bnn, X_gp_val_bnn, X_gp_test_bnn = X_val[:,bnn_feat_idx], X_train_2[:,bnn_feat_idx], X_train[:,bnn_feat_idx], X_test[:,bnn_feat_idx]\n",
    "y_train_gp, y_train_gp_2, y_val_gp, y_test_gp = y_val, y_train_2, y_train, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test rmse: 0.5148490096871814, r2_score: 0.06576060288625452, pcc: 0.2890981258051059, pcc_pval: 0.008433354355688273\n"
     ]
    }
   ],
   "source": [
    "bnn_test_rmse_score, bnn_test_r2_score, bnn_test_pcc, bnn_test_pval = train_linear_model(seed, X_gp_train_2_bnn, X_gp_test_bnn, y_train_gp_2, y_test_gp)\n",
    "print(f\"Test rmse: {bnn_test_rmse_score}, r2_score: {bnn_test_r2_score}, pcc: {bnn_test_pcc}, pcc_pval: {bnn_test_pval}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test rmse: 0.5439077870952783, r2_score: -0.04267501632493409, pcc: 0.2635821998026937, pcc_pval: 0.01672615922491233\n"
     ]
    }
   ],
   "source": [
    "bnn_gp_test_rmse_score, bnn_gp_test_r2_score, bnn_gp_test_pcc, bnn_gp_test_pval, gp_df = train_gp(seed, X_gp_train_bnn, X_gp_train_2_bnn, X_gp_test_bnn,\n",
    "                                                                                              y_train_gp, y_train_gp_2, y_test_gp, num_models=5, verbose=0, p_cxvr=0.7, p_subt_mut=0.1,\n",
    "                                                                                              p_hmut=0.1, p_pmut=0.1, complexity_coef=0.01)\n",
    "print(f\"Test rmse: {bnn_gp_test_rmse_score}, r2_score: {bnn_gp_test_r2_score}, pcc: {bnn_gp_test_pcc}, pcc_pval: {bnn_gp_test_pval}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([ 83., 180., 240., 104.,  59.,  32.,  26.,  28.,  12.,   4.]),\n array([1.25000e-04, 4.57500e-02, 9.13750e-02, 1.37000e-01, 1.82625e-01,\n        2.28250e-01, 2.73875e-01, 3.19500e-01, 3.65125e-01, 4.10750e-01,\n        4.56375e-01]),\n <BarContainer object of 10 artists>)"
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.mean(np.array(disc_states), axis=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([6131.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n        1869.]),\n array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n <BarContainer object of 10 artists>)"
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.array(disc_states)[:,448])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}