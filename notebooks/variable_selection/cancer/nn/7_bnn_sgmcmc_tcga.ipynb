{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
      "Collecting jaxlib[cuda112]==0.3.15\n",
      "  Downloading https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.15%2Bcuda11.cudnn82-cp39-none-manylinux2014_x86_64.whl (162.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m162.7/162.7 MB\u001B[0m \u001B[31m14.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25h\u001B[33mWARNING: jaxlib 0.3.15+cuda11.cudnn82 does not provide the extra 'cuda112'\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jaxlib[cuda112]==0.3.15) (1.8.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from jaxlib[cuda112]==0.3.15) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.9/dist-packages (from jaxlib[cuda112]==0.3.15) (1.23.1)\n",
      "Installing collected packages: jaxlib\n",
      "  Attempting uninstall: jaxlib\n",
      "    Found existing installation: jaxlib 0.3.8+cuda11.cudnn82\n",
      "    Uninstalling jaxlib-0.3.8+cuda11.cudnn82:\n",
      "      Successfully uninstalled jaxlib-0.3.8+cuda11.cudnn82\n",
      "Successfully installed jaxlib-0.3.15+cuda11.cudnn82\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mLooking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
      "Collecting jax[cuda112]==0.3.17\n",
      "  Downloading jax-0.3.17.tar.gz (1.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m61.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[33mWARNING: jax 0.3.17 does not provide the extra 'cuda112'\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (1.23.1)\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (1.8.1)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (4.3.0)\n",
      "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (0.6.0)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax[cuda112]==0.3.17) (5.8.0)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax[cuda112]==0.3.17) (3.8.1)\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for jax: filename=jax-0.3.17-py3-none-any.whl size=1217849 sha256=d8d573c234ee675ff3a428db10799c67ff80246aa52e2278e094b14ccbd81a5f\n",
      "  Stored in directory: /root/.cache/pip/wheels/36/cd/88/2d90379f7549c27d5654e893f74210f30f0c645c23a71e6f56\n",
      "Successfully built jax\n",
      "Installing collected packages: jax\n",
      "  Attempting uninstall: jax\n",
      "    Found existing installation: jax 0.3.14\n",
      "    Uninstalling jax-0.3.14:\n",
      "      Successfully uninstalled jax-0.3.14\n",
      "Successfully installed jax-0.3.17\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mCollecting optax\n",
      "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m154.9/154.9 kB\u001B[0m \u001B[31m22.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from optax) (1.23.1)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.9/dist-packages (from optax) (0.3.15+cuda11.cudnn82)\n",
      "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.9/dist-packages (from optax) (0.3.17)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from optax) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.9/dist-packages (from optax) (4.3.0)\n",
      "Collecting chex>=0.1.5\n",
      "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m85.3/85.3 kB\u001B[0m \u001B[31m31.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting dm-tree>=0.1.5\n",
      "  Downloading dm_tree-0.1.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (142 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m142.7/142.7 kB\u001B[0m \u001B[31m50.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting toolz>=0.9.0\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m55.8/55.8 kB\u001B[0m \u001B[31m25.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (1.8.1)\n",
      "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (0.6.0)\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax>=0.1.55->optax) (5.8.0)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax>=0.1.55->optax) (3.8.1)\n",
      "Installing collected packages: dm-tree, toolz, chex, optax\n",
      "Successfully installed chex-0.1.5 dm-tree-0.1.7 optax-0.1.4 toolz-0.12.0\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mCollecting dm-haiku\n",
      "  Downloading dm_haiku-0.0.9-py3-none-any.whl (352 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m352.1/352.1 kB\u001B[0m \u001B[31m45.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (1.23.1)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (1.1.0)\n",
      "Collecting jmp>=0.0.2\n",
      "  Downloading jmp-0.0.2-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: tabulate, jmp, dm-haiku\n",
      "Successfully installed dm-haiku-0.0.9 jmp-0.0.2 tabulate-0.9.0\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mCollecting tensorflow-probability==0.17\n",
      "  Downloading tensorflow_probability-0.17.0-py2.py3-none-any.whl (6.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.5/6.5 MB\u001B[0m \u001B[31m115.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow-probability==0.17) (1.14.0)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (0.1.7)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (5.1.1)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (2.1.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (1.23.1)\n",
      "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (0.4.0)\n",
      "Installing collected packages: tensorflow-probability\n",
      "Successfully installed tensorflow-probability-0.17.0\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mCollecting git+https://github.com/blackjax-devs/blackjax.git\n",
      "  Cloning https://github.com/blackjax-devs/blackjax.git to /tmp/pip-req-build-rt_vpew8\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/blackjax-devs/blackjax.git /tmp/pip-req-build-rt_vpew8\n",
      "  Resolved https://github.com/blackjax-devs/blackjax.git to commit 28167ad35fb880f7b90d0a78d8324f7fda8032e2\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting fastprogress>=0.2.0\n",
      "  Downloading fastprogress-1.0.3-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: jax>=0.3.13 in /usr/local/lib/python3.9/dist-packages (from blackjax==0.9.6+74.g28167ad) (0.3.17)\n",
      "Requirement already satisfied: jaxlib>=0.3.10 in /usr/local/lib/python3.9/dist-packages (from blackjax==0.9.6+74.g28167ad) (0.3.15+cuda11.cudnn82)\n",
      "Collecting jaxopt>=0.5.5\n",
      "  Downloading jaxopt-0.5.5-py3-none-any.whl (132 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m132.3/132.3 kB\u001B[0m \u001B[31m33.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+74.g28167ad) (4.3.0)\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+74.g28167ad) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+74.g28167ad) (1.23.1)\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+74.g28167ad) (3.3.0)\n",
      "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+74.g28167ad) (0.6.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+74.g28167ad) (1.1.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.1 in /usr/local/lib/python3.9/dist-packages (from jaxopt>=0.5.5->blackjax==0.9.6+74.g28167ad) (3.5.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+74.g28167ad) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+74.g28167ad) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+74.g28167ad) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+74.g28167ad) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+74.g28167ad) (1.4.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+74.g28167ad) (4.34.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+74.g28167ad) (2.8.2)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax>=0.3.13->blackjax==0.9.6+74.g28167ad) (3.8.1)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax>=0.3.13->blackjax==0.9.6+74.g28167ad) (5.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+74.g28167ad) (1.14.0)\n",
      "Building wheels for collected packages: blackjax\n",
      "  Building wheel for blackjax (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for blackjax: filename=blackjax-0.9.6+74.g28167ad-py3-none-any.whl size=126443 sha256=f66c3638c06ccaf7012a455e6c63800cb727f37c526b651d7967c0bdf2edd67c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-062sc_ne/wheels/e6/1e/f6/a6e0408a4e374b9cdb789b1769716b4ed61eef520a2dd702b1\n",
      "Successfully built blackjax\n",
      "Installing collected packages: fastprogress, jaxopt, blackjax\n",
      "Successfully installed blackjax-0.9.6+74.g28167ad fastprogress-1.0.3 jaxopt-0.5.5\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:2 https://deb.nodesource.com/node_16.x focal InRelease [4583 B]            \u001B[0m\u001B[33m\n",
      "Get:3 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1712 kB]33m\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]                \u001B[0m\n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2315 kB][33m\n",
      "Get:6 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [967 kB]m\n",
      "Get:7 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [27.5 kB]\n",
      "Get:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease [18.1 kB] \u001B[0m\u001B[33m\n",
      "Get:9 https://deb.nodesource.com/node_16.x focal/main amd64 Packages [774 B]   \u001B[0m\u001B[33m\u001B[33m\u001B[33m\n",
      "Get:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 Packages [29.5 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\u001B[33m\u001B[33m\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]33m\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [1829 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [30.2 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1268 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2786 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [27.4 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Fetched 24.5 MB in 2s (10.3 MB/s)[33m                        \u001B[0m\u001B[33m\u001B[33m\u001B[33m\u001B[33m\u001B[33m\u001B[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "104 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  fonts-liberation libann0 libcdt5 libcgraph6 libgts-0.7-5 libgts-bin libgvc6\n",
      "  libgvpr2 liblab-gamut1 libpathplan4\n",
      "Suggested packages:\n",
      "  gsfonts graphviz-doc\n",
      "The following NEW packages will be installed:\n",
      "  fonts-liberation graphviz libann0 libcdt5 libcgraph6 libgts-0.7-5 libgts-bin\n",
      "  libgvc6 libgvpr2 liblab-gamut1 libpathplan4\n",
      "0 upgraded, 11 newly installed, 0 to remove and 104 not upgraded.\n",
      "Need to get 2701 kB of archives.\n",
      "After this operation, 11.3 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 fonts-liberation all 1:1.07.4-11 [822 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libann0 amd64 1.1.2+doc-7build1 [26.0 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 libcdt5 amd64 2.42.2-3build2 [18.7 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 libcgraph6 amd64 2.42.2-3build2 [41.3 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgts-0.7-5 amd64 0.7.6+darcs121130-4 [150 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/universe amd64 libpathplan4 amd64 2.42.2-3build2 [21.9 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgvc6 amd64 2.42.2-3build2 [647 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgvpr2 amd64 2.42.2-3build2 [167 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/universe amd64 liblab-gamut1 amd64 2.42.2-3build2 [177 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/universe amd64 graphviz amd64 2.42.2-3build2 [590 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgts-bin amd64 0.7.6+darcs121130-4 [41.3 kB]\n",
      "Fetched 2701 kB in 1s (2792 kB/s)      \u001B[0m\u001B[33m\n",
      "\n",
      "\u001B7\u001B[0;23r\u001B8\u001B[1ASelecting previously unselected package fonts-liberation.\n",
      "(Reading database ... 78556 files and directories currently installed.)\n",
      "Preparing to unpack .../00-fonts-liberation_1%3a1.07.4-11_all.deb ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  0%]\u001B[49m\u001B[39m [..........................................................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  2%]\u001B[49m\u001B[39m [#.........................................................] \u001B8Unpacking fonts-liberation (1:1.07.4-11) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  4%]\u001B[49m\u001B[39m [##........................................................] \u001B8Selecting previously unselected package libann0.\n",
      "Preparing to unpack .../01-libann0_1.1.2+doc-7build1_amd64.deb ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  7%]\u001B[49m\u001B[39m [###.......................................................] \u001B8Unpacking libann0 (1.1.2+doc-7build1) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  9%]\u001B[49m\u001B[39m [#####.....................................................] \u001B8Selecting previously unselected package libcdt5:amd64.\n",
      "Preparing to unpack .../02-libcdt5_2.42.2-3build2_amd64.deb ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 11%]\u001B[49m\u001B[39m [######....................................................] \u001B8Unpacking libcdt5:amd64 (2.42.2-3build2) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 13%]\u001B[49m\u001B[39m [#######...................................................] \u001B8Selecting previously unselected package libcgraph6:amd64.\n",
      "Preparing to unpack .../03-libcgraph6_2.42.2-3build2_amd64.deb ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 16%]\u001B[49m\u001B[39m [#########.................................................] \u001B8Unpacking libcgraph6:amd64 (2.42.2-3build2) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 18%]\u001B[49m\u001B[39m [##########................................................] \u001B8Selecting previously unselected package libgts-0.7-5:amd64.\n",
      "Preparing to unpack .../04-libgts-0.7-5_0.7.6+darcs121130-4_amd64.deb ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 20%]\u001B[49m\u001B[39m [###########...............................................] \u001B8Unpacking libgts-0.7-5:amd64 (0.7.6+darcs121130-4) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 22%]\u001B[49m\u001B[39m [############..............................................] \u001B8Selecting previously unselected package libpathplan4:amd64.\n",
      "Preparing to unpack .../05-libpathplan4_2.42.2-3build2_amd64.deb ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 24%]\u001B[49m\u001B[39m [##############............................................] \u001B8Unpacking libpathplan4:amd64 (2.42.2-3build2) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 27%]\u001B[49m\u001B[39m [###############...........................................] \u001B8Selecting previously unselected package libgvc6.\n",
      "Preparing to unpack .../06-libgvc6_2.42.2-3build2_amd64.deb ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 29%]\u001B[49m\u001B[39m [################..........................................] \u001B8Unpacking libgvc6 (2.42.2-3build2) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 31%]\u001B[49m\u001B[39m [##################........................................] \u001B8Selecting previously unselected package libgvpr2:amd64.\n",
      "Preparing to unpack .../07-libgvpr2_2.42.2-3build2_amd64.deb ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 33%]\u001B[49m\u001B[39m [###################.......................................] \u001B8Unpacking libgvpr2:amd64 (2.42.2-3build2) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 36%]\u001B[49m\u001B[39m [####################......................................] \u001B8Selecting previously unselected package liblab-gamut1:amd64.\n",
      "Preparing to unpack .../08-liblab-gamut1_2.42.2-3build2_amd64.deb ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 38%]\u001B[49m\u001B[39m [#####################.....................................] \u001B8Unpacking liblab-gamut1:amd64 (2.42.2-3build2) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 40%]\u001B[49m\u001B[39m [#######################...................................] \u001B8Selecting previously unselected package graphviz.\n",
      "Preparing to unpack .../09-graphviz_2.42.2-3build2_amd64.deb ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 42%]\u001B[49m\u001B[39m [########################..................................] \u001B8Unpacking graphviz (2.42.2-3build2) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 44%]\u001B[49m\u001B[39m [#########################.................................] \u001B8Selecting previously unselected package libgts-bin.\n",
      "Preparing to unpack .../10-libgts-bin_0.7.6+darcs121130-4_amd64.deb ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 47%]\u001B[49m\u001B[39m [###########################...............................] \u001B8Unpacking libgts-bin (0.7.6+darcs121130-4) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 49%]\u001B[49m\u001B[39m [############################..............................] \u001B8Setting up liblab-gamut1:amd64 (2.42.2-3build2) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 51%]\u001B[49m\u001B[39m [#############################.............................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 53%]\u001B[49m\u001B[39m [##############################............................] \u001B8Setting up libgts-0.7-5:amd64 (0.7.6+darcs121130-4) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 56%]\u001B[49m\u001B[39m [################################..........................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 58%]\u001B[49m\u001B[39m [#################################.........................] \u001B8Setting up libpathplan4:amd64 (2.42.2-3build2) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 60%]\u001B[49m\u001B[39m [##################################........................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 62%]\u001B[49m\u001B[39m [####################################......................] \u001B8Setting up libann0 (1.1.2+doc-7build1) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 64%]\u001B[49m\u001B[39m [#####################################.....................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 67%]\u001B[49m\u001B[39m [######################################....................] \u001B8Setting up fonts-liberation (1:1.07.4-11) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 69%]\u001B[49m\u001B[39m [#######################################...................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 71%]\u001B[49m\u001B[39m [#########################################.................] \u001B8Setting up libcdt5:amd64 (2.42.2-3build2) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 73%]\u001B[49m\u001B[39m [##########################################................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 76%]\u001B[49m\u001B[39m [###########################################...............] \u001B8Setting up libcgraph6:amd64 (2.42.2-3build2) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 78%]\u001B[49m\u001B[39m [#############################################.............] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 80%]\u001B[49m\u001B[39m [##############################################............] \u001B8Setting up libgts-bin (0.7.6+darcs121130-4) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 82%]\u001B[49m\u001B[39m [###############################################...........] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 84%]\u001B[49m\u001B[39m [################################################..........] \u001B8Setting up libgvc6 (2.42.2-3build2) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 87%]\u001B[49m\u001B[39m [##################################################........] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 89%]\u001B[49m\u001B[39m [###################################################.......] \u001B8Setting up libgvpr2:amd64 (2.42.2-3build2) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 91%]\u001B[49m\u001B[39m [####################################################......] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 93%]\u001B[49m\u001B[39m [######################################################....] \u001B8Setting up graphviz (2.42.2-3build2) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 96%]\u001B[49m\u001B[39m [#######################################################...] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 98%]\u001B[49m\u001B[39m [########################################################..] \u001B8Processing triggers for libc-bin (2.31-0ubuntu9.7) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "Processing triggers for fontconfig (2.13.1-2ubuntu3) ...\n",
      "\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  swig4.0\n",
      "Suggested packages:\n",
      "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
      "The following NEW packages will be installed:\n",
      "  swig swig4.0\n",
      "0 upgraded, 2 newly installed, 0 to remove and 104 not upgraded.\n",
      "Need to get 1086 kB of archives.\n",
      "After this operation, 5413 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig4.0 amd64 4.0.1-5build1 [1081 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig all 4.0.1-5build1 [5528 B]\n",
      "Fetched 1086 kB in 1s (1370 kB/s)\n",
      "Selecting previously unselected package swig4.0.\n",
      "(Reading database ... 78769 files and directories currently installed.)\n",
      "Preparing to unpack .../swig4.0_4.0.1-5build1_amd64.deb ...\n",
      "Unpacking swig4.0 (4.0.1-5build1) ...\n",
      "Selecting previously unselected package swig.\n",
      "Preparing to unpack .../swig_4.0.1-5build1_all.deb ...\n",
      "Unpacking swig (4.0.1-5build1) ...\n",
      "Setting up swig4.0 (4.0.1-5build1) ...\n",
      "Setting up swig (4.0.1-5build1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "Collecting smac\n",
      "  Downloading smac-1.4.0.tar.gz (202 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m202.9/202.9 kB\u001B[0m \u001B[31m32.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.9/dist-packages (from smac) (1.1.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from smac) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from smac) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.9/dist-packages (from smac) (1.23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from smac) (5.9.1)\n",
      "Collecting emcee>=3.0.0\n",
      "  Downloading emcee-3.1.3-py2.py3-none-any.whl (46 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.2/46.2 kB\u001B[0m \u001B[31m18.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pyrfr>=0.8.3\n",
      "  Downloading pyrfr-0.8.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.5/4.5 MB\u001B[0m \u001B[31m118.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting pynisher<1.0.0\n",
      "  Downloading pynisher-0.6.4.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from smac) (2022.7.9)\n",
      "Collecting dask\n",
      "  Downloading dask-2022.11.1-py3-none-any.whl (1.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m111.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting ConfigSpace>=0.5.0\n",
      "  Downloading ConfigSpace-0.6.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.6/5.6 MB\u001B[0m \u001B[31m123.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting distributed\n",
      "  Downloading distributed-2022.11.1-py3-none-any.whl (923 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m923.4/923.4 kB\u001B[0m \u001B[31m118.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from ConfigSpace>=0.5.0->smac) (4.3.0)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from ConfigSpace>=0.5.0->smac) (3.0.9)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from ConfigSpace>=0.5.0->smac) (0.29.30)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from pynisher<1.0.0->smac) (63.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22.0->smac) (3.1.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (2022.5.0)\n",
      "Collecting partd>=0.3.10\n",
      "  Downloading partd-1.3.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (8.1.3)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (5.4.1)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (0.12.0)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (21.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from distributed->smac) (3.1.2)\n",
      "Collecting locket>=1.0.0\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting msgpack>=0.6.0\n",
      "  Downloading msgpack-1.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m322.4/322.4 kB\u001B[0m \u001B[31m67.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting sortedcontainers!=2.0.0,!=2.0.1\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting tblib>=1.6.0\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting zict>=0.1.3\n",
      "  Downloading zict-2.2.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting tornado<6.2,>=6.0.3\n",
      "  Downloading tornado-6.1-cp39-cp39-manylinux2010_x86_64.whl (427 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m427.2/427.2 kB\u001B[0m \u001B[31m90.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from distributed->smac) (1.26.10)\n",
      "Collecting heapdict\n",
      "  Downloading HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->distributed->smac) (2.1.1)\n",
      "Building wheels for collected packages: smac, pynisher\n",
      "  Building wheel for smac (pyproject.toml) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for smac: filename=smac-1.4.0-py3-none-any.whl size=262348 sha256=35d1cf5f672d27c7a1c8742b44a8cbc68a2c5d6be1921e1058af82e989a02143\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/cc/e7/d683d9404760c4701ea2f64faaf689a8de718f701de63e71ea\n",
      "  Building wheel for pynisher (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for pynisher: filename=pynisher-0.6.4-py3-none-any.whl size=7026 sha256=2e3532d4659f9be80baa74d8d3a902dd9d08d20b85cf7162a4743bdadfa9aa97\n",
      "  Stored in directory: /root/.cache/pip/wheels/1d/de/5e/d4947b76b76ba27581d1e09f395eca1583a802203a41c04873\n",
      "Successfully built smac pynisher\n",
      "Installing collected packages: sortedcontainers, msgpack, heapdict, zict, tornado, tblib, pyrfr, pynisher, locket, emcee, partd, ConfigSpace, dask, distributed, smac\n",
      "  Attempting uninstall: tornado\n",
      "    Found existing installation: tornado 6.2\n",
      "    Uninstalling tornado-6.2:\n",
      "      Successfully uninstalled tornado-6.2\n",
      "Successfully installed ConfigSpace-0.6.0 dask-2022.11.1 distributed-2022.11.1 emcee-3.1.3 heapdict-1.0.1 locket-1.0.0 msgpack-1.0.4 partd-1.3.0 pynisher-0.6.4 pyrfr-0.8.3 smac-1.4.0 sortedcontainers-2.4.0 tblib-1.7.0 tornado-6.1 zict-2.2.0\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mCollecting graphviz\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m47.0/47.0 kB\u001B[0m \u001B[31m15.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.1\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mCollecting gplearn\n",
      "  Downloading gplearn-0.4.2-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from gplearn) (1.1.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from gplearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.23.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (3.1.0)\n",
      "Installing collected packages: gplearn\n",
      "Successfully installed gplearn-0.4.2\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U jaxlib[cuda112]==0.3.15 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install -U jax[cuda112]==0.3.17 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install optax\n",
    "!pip install dm-haiku\n",
    "!pip install tensorflow-probability==0.17\n",
    "!pip install git+https://github.com/blackjax-devs/blackjax.git\n",
    "!apt update\n",
    "!apt install -y graphviz\n",
    "!apt-get -y install swig\n",
    "!pip install smac\n",
    "!pip install graphviz\n",
    "!pip install gplearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"False\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "SERVER = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "if not SERVER:\n",
    "    %cd /home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# tfd = tfp.distributions\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import numpy as np\n",
    "import optax\n",
    "from nn_util import *\n",
    "plt.style.use('ggplot')\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if SERVER:\n",
    "    data_dir = \".\"\n",
    "else:\n",
    "    data_dir = \"/home/xabush/code/snet/moses-incons-pen-xp/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### GDSC Load Cell Line Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 37263)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdsc_dir = f\"{data_dir}/cell_line/gdsc2\"\n",
    "gdsc_exp_data = pd.read_csv(f\"{gdsc_dir}/gdsc_gene_expr.csv\", index_col=\"model_id\")\n",
    "gdsc_exp_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATASET</th>\n",
       "      <th>NLME_RESULT_ID</th>\n",
       "      <th>NLME_CURVE_ID</th>\n",
       "      <th>COSMIC_ID</th>\n",
       "      <th>CELL_LINE_NAME</th>\n",
       "      <th>TCGA_DESC</th>\n",
       "      <th>DRUG_ID</th>\n",
       "      <th>DRUG_NAME</th>\n",
       "      <th>PUTATIVE_TARGET</th>\n",
       "      <th>PATHWAY_NAME</th>\n",
       "      <th>COMPANY_ID</th>\n",
       "      <th>WEBRELEASE</th>\n",
       "      <th>MIN_CONC</th>\n",
       "      <th>MAX_CONC</th>\n",
       "      <th>LN_IC50</th>\n",
       "      <th>AUC</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Z_SCORE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SANGER_MODEL_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SIDM01132</th>\n",
       "      <td>GDSC2</td>\n",
       "      <td>401</td>\n",
       "      <td>18945558</td>\n",
       "      <td>683667</td>\n",
       "      <td>PFSK-1</td>\n",
       "      <td>MB</td>\n",
       "      <td>1003</td>\n",
       "      <td>Camptothecin</td>\n",
       "      <td>TOP1</td>\n",
       "      <td>DNA replication</td>\n",
       "      <td>1046</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-1.462148</td>\n",
       "      <td>0.930105</td>\n",
       "      <td>0.088999</td>\n",
       "      <td>0.432482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDM00848</th>\n",
       "      <td>GDSC2</td>\n",
       "      <td>401</td>\n",
       "      <td>18945796</td>\n",
       "      <td>684052</td>\n",
       "      <td>A673</td>\n",
       "      <td>UNCLASSIFIED</td>\n",
       "      <td>1003</td>\n",
       "      <td>Camptothecin</td>\n",
       "      <td>TOP1</td>\n",
       "      <td>DNA replication</td>\n",
       "      <td>1046</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-4.869447</td>\n",
       "      <td>0.614932</td>\n",
       "      <td>0.111423</td>\n",
       "      <td>-1.420322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDM00263</th>\n",
       "      <td>GDSC2</td>\n",
       "      <td>401</td>\n",
       "      <td>18946078</td>\n",
       "      <td>684057</td>\n",
       "      <td>ES5</td>\n",
       "      <td>UNCLASSIFIED</td>\n",
       "      <td>1003</td>\n",
       "      <td>Camptothecin</td>\n",
       "      <td>TOP1</td>\n",
       "      <td>DNA replication</td>\n",
       "      <td>1046</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-3.360684</td>\n",
       "      <td>0.790953</td>\n",
       "      <td>0.142754</td>\n",
       "      <td>-0.599894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDM00269</th>\n",
       "      <td>GDSC2</td>\n",
       "      <td>401</td>\n",
       "      <td>18946335</td>\n",
       "      <td>684059</td>\n",
       "      <td>ES7</td>\n",
       "      <td>UNCLASSIFIED</td>\n",
       "      <td>1003</td>\n",
       "      <td>Camptothecin</td>\n",
       "      <td>TOP1</td>\n",
       "      <td>DNA replication</td>\n",
       "      <td>1046</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-5.045014</td>\n",
       "      <td>0.592624</td>\n",
       "      <td>0.135642</td>\n",
       "      <td>-1.515791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDM00203</th>\n",
       "      <td>GDSC2</td>\n",
       "      <td>401</td>\n",
       "      <td>18946617</td>\n",
       "      <td>684062</td>\n",
       "      <td>EW-11</td>\n",
       "      <td>UNCLASSIFIED</td>\n",
       "      <td>1003</td>\n",
       "      <td>Camptothecin</td>\n",
       "      <td>TOP1</td>\n",
       "      <td>DNA replication</td>\n",
       "      <td>1046</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-3.741620</td>\n",
       "      <td>0.733992</td>\n",
       "      <td>0.128066</td>\n",
       "      <td>-0.807038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDM00216</th>\n",
       "      <td>GDSC2</td>\n",
       "      <td>401</td>\n",
       "      <td>19187490</td>\n",
       "      <td>1659928</td>\n",
       "      <td>SNU-175</td>\n",
       "      <td>COREAD</td>\n",
       "      <td>2499</td>\n",
       "      <td>N-acetyl cysteine</td>\n",
       "      <td>Metabolism</td>\n",
       "      <td>Metabolism</td>\n",
       "      <td>1101</td>\n",
       "      <td>Y</td>\n",
       "      <td>2.001054</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>10.134495</td>\n",
       "      <td>0.976798</td>\n",
       "      <td>0.074441</td>\n",
       "      <td>0.159946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDM00214</th>\n",
       "      <td>GDSC2</td>\n",
       "      <td>401</td>\n",
       "      <td>19187943</td>\n",
       "      <td>1660034</td>\n",
       "      <td>SNU-407</td>\n",
       "      <td>COREAD</td>\n",
       "      <td>2499</td>\n",
       "      <td>N-acetyl cysteine</td>\n",
       "      <td>Metabolism</td>\n",
       "      <td>Metabolism</td>\n",
       "      <td>1101</td>\n",
       "      <td>Y</td>\n",
       "      <td>2.001054</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>8.575555</td>\n",
       "      <td>0.913182</td>\n",
       "      <td>0.057743</td>\n",
       "      <td>-1.626059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDM00194</th>\n",
       "      <td>GDSC2</td>\n",
       "      <td>401</td>\n",
       "      <td>19188201</td>\n",
       "      <td>1660035</td>\n",
       "      <td>SNU-61</td>\n",
       "      <td>COREAD</td>\n",
       "      <td>2499</td>\n",
       "      <td>N-acetyl cysteine</td>\n",
       "      <td>Metabolism</td>\n",
       "      <td>Metabolism</td>\n",
       "      <td>1101</td>\n",
       "      <td>Y</td>\n",
       "      <td>2.001054</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>10.520666</td>\n",
       "      <td>0.974889</td>\n",
       "      <td>0.058094</td>\n",
       "      <td>0.602364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDM00498</th>\n",
       "      <td>GDSC2</td>\n",
       "      <td>401</td>\n",
       "      <td>19188741</td>\n",
       "      <td>1674021</td>\n",
       "      <td>SNU-C5</td>\n",
       "      <td>COREAD</td>\n",
       "      <td>2499</td>\n",
       "      <td>N-acetyl cysteine</td>\n",
       "      <td>Metabolism</td>\n",
       "      <td>Metabolism</td>\n",
       "      <td>1101</td>\n",
       "      <td>Y</td>\n",
       "      <td>2.001054</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>10.701430</td>\n",
       "      <td>0.970009</td>\n",
       "      <td>0.100980</td>\n",
       "      <td>0.809457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDM00049</th>\n",
       "      <td>GDSC2</td>\n",
       "      <td>401</td>\n",
       "      <td>19189023</td>\n",
       "      <td>1789883</td>\n",
       "      <td>DiFi</td>\n",
       "      <td>COREAD</td>\n",
       "      <td>2499</td>\n",
       "      <td>N-acetyl cysteine</td>\n",
       "      <td>Metabolism</td>\n",
       "      <td>Metabolism</td>\n",
       "      <td>1101</td>\n",
       "      <td>Y</td>\n",
       "      <td>2.001054</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>10.038769</td>\n",
       "      <td>0.966966</td>\n",
       "      <td>0.089022</td>\n",
       "      <td>0.050277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242036 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                DATASET  NLME_RESULT_ID  NLME_CURVE_ID  COSMIC_ID  \\\n",
       "SANGER_MODEL_ID                                                     \n",
       "SIDM01132         GDSC2             401       18945558     683667   \n",
       "SIDM00848         GDSC2             401       18945796     684052   \n",
       "SIDM00263         GDSC2             401       18946078     684057   \n",
       "SIDM00269         GDSC2             401       18946335     684059   \n",
       "SIDM00203         GDSC2             401       18946617     684062   \n",
       "...                 ...             ...            ...        ...   \n",
       "SIDM00216         GDSC2             401       19187490    1659928   \n",
       "SIDM00214         GDSC2             401       19187943    1660034   \n",
       "SIDM00194         GDSC2             401       19188201    1660035   \n",
       "SIDM00498         GDSC2             401       19188741    1674021   \n",
       "SIDM00049         GDSC2             401       19189023    1789883   \n",
       "\n",
       "                CELL_LINE_NAME     TCGA_DESC  DRUG_ID          DRUG_NAME  \\\n",
       "SANGER_MODEL_ID                                                            \n",
       "SIDM01132               PFSK-1            MB     1003       Camptothecin   \n",
       "SIDM00848                 A673  UNCLASSIFIED     1003       Camptothecin   \n",
       "SIDM00263                  ES5  UNCLASSIFIED     1003       Camptothecin   \n",
       "SIDM00269                  ES7  UNCLASSIFIED     1003       Camptothecin   \n",
       "SIDM00203                EW-11  UNCLASSIFIED     1003       Camptothecin   \n",
       "...                        ...           ...      ...                ...   \n",
       "SIDM00216              SNU-175        COREAD     2499  N-acetyl cysteine   \n",
       "SIDM00214              SNU-407        COREAD     2499  N-acetyl cysteine   \n",
       "SIDM00194               SNU-61        COREAD     2499  N-acetyl cysteine   \n",
       "SIDM00498               SNU-C5        COREAD     2499  N-acetyl cysteine   \n",
       "SIDM00049                 DiFi        COREAD     2499  N-acetyl cysteine   \n",
       "\n",
       "                PUTATIVE_TARGET     PATHWAY_NAME  COMPANY_ID WEBRELEASE  \\\n",
       "SANGER_MODEL_ID                                                           \n",
       "SIDM01132                  TOP1  DNA replication        1046          Y   \n",
       "SIDM00848                  TOP1  DNA replication        1046          Y   \n",
       "SIDM00263                  TOP1  DNA replication        1046          Y   \n",
       "SIDM00269                  TOP1  DNA replication        1046          Y   \n",
       "SIDM00203                  TOP1  DNA replication        1046          Y   \n",
       "...                         ...              ...         ...        ...   \n",
       "SIDM00216            Metabolism       Metabolism        1101          Y   \n",
       "SIDM00214            Metabolism       Metabolism        1101          Y   \n",
       "SIDM00194            Metabolism       Metabolism        1101          Y   \n",
       "SIDM00498            Metabolism       Metabolism        1101          Y   \n",
       "SIDM00049            Metabolism       Metabolism        1101          Y   \n",
       "\n",
       "                 MIN_CONC  MAX_CONC    LN_IC50       AUC      RMSE   Z_SCORE  \n",
       "SANGER_MODEL_ID                                                               \n",
       "SIDM01132        0.000100       0.1  -1.462148  0.930105  0.088999  0.432482  \n",
       "SIDM00848        0.000100       0.1  -4.869447  0.614932  0.111423 -1.420322  \n",
       "SIDM00263        0.000100       0.1  -3.360684  0.790953  0.142754 -0.599894  \n",
       "SIDM00269        0.000100       0.1  -5.045014  0.592624  0.135642 -1.515791  \n",
       "SIDM00203        0.000100       0.1  -3.741620  0.733992  0.128066 -0.807038  \n",
       "...                   ...       ...        ...       ...       ...       ...  \n",
       "SIDM00216        2.001054    2000.0  10.134495  0.976798  0.074441  0.159946  \n",
       "SIDM00214        2.001054    2000.0   8.575555  0.913182  0.057743 -1.626059  \n",
       "SIDM00194        2.001054    2000.0  10.520666  0.974889  0.058094  0.602364  \n",
       "SIDM00498        2.001054    2000.0  10.701430  0.970009  0.100980  0.809457  \n",
       "SIDM00049        2.001054    2000.0  10.038769  0.966966  0.089022  0.050277  \n",
       "\n",
       "[242036 rows x 18 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdsc_response_data = pd.read_csv(f\"{gdsc_dir}/GDSC2_fitted_dose_response_24Jul22.csv\", index_col=\"SANGER_MODEL_ID\")\n",
    "gdsc_response_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Tamoxifen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(957, 18)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamox_response_data = gdsc_response_data[gdsc_response_data[\"DRUG_NAME\"] == \"Tamoxifen\"]\n",
    "tamox_response_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(406, 37264)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdsc_exp_tamox_data = pd.merge(gdsc_exp_data, tamox_response_data[\"LN_IC50\"], left_index=True, right_index=True)\n",
    "gdsc_exp_tamox_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X, y = gdsc_exp_tamox_data.iloc[:,:-1], gdsc_exp_tamox_data.iloc[:,-1]\n",
    "# change to -log10(IC_50) to make it comparable\n",
    "y = -np.log10(np.exp(y)) # exp b/c the values are natural logs of raw IC_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>method_of_action</th>\n",
       "      <th>cosmic_moa</th>\n",
       "      <th>intogen_moa</th>\n",
       "      <th>gene_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABCB1</td>\n",
       "      <td>Act</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Act</td>\n",
       "      <td>SIDG00064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABI1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>TSG, fusion</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>SIDG00145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABL1</td>\n",
       "      <td>Act</td>\n",
       "      <td>oncogene, fusion</td>\n",
       "      <td>Act</td>\n",
       "      <td>SIDG00150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABL2</td>\n",
       "      <td>Act</td>\n",
       "      <td>oncogene, fusion</td>\n",
       "      <td>Act</td>\n",
       "      <td>SIDG00151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACKR3</td>\n",
       "      <td>Act</td>\n",
       "      <td>oncogene, fusion</td>\n",
       "      <td>Act</td>\n",
       "      <td>SIDG00205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>ZNF814</td>\n",
       "      <td>Act</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Act</td>\n",
       "      <td>SIDG42334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>ZNF93</td>\n",
       "      <td>LoF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LoF</td>\n",
       "      <td>SIDG41755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>ZNRF3</td>\n",
       "      <td>LoF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LoF</td>\n",
       "      <td>SIDG42403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>ZRSR2</td>\n",
       "      <td>LoF</td>\n",
       "      <td>TSG</td>\n",
       "      <td>LoF</td>\n",
       "      <td>SIDG42422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>ZXDB</td>\n",
       "      <td>LoF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LoF</td>\n",
       "      <td>SIDG42467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>783 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     symbol method_of_action        cosmic_moa intogen_moa    gene_id\n",
       "0     ABCB1              Act               NaN         Act  SIDG00064\n",
       "1      ABI1        ambiguous       TSG, fusion   ambiguous  SIDG00145\n",
       "2      ABL1              Act  oncogene, fusion         Act  SIDG00150\n",
       "3      ABL2              Act  oncogene, fusion         Act  SIDG00151\n",
       "4     ACKR3              Act  oncogene, fusion         Act  SIDG00205\n",
       "..      ...              ...               ...         ...        ...\n",
       "778  ZNF814              Act               NaN         Act  SIDG42334\n",
       "779   ZNF93              LoF               NaN         LoF  SIDG41755\n",
       "780   ZNRF3              LoF               NaN         LoF  SIDG42403\n",
       "781   ZRSR2              LoF               TSG         LoF  SIDG42422\n",
       "782    ZXDB              LoF               NaN         LoF  SIDG42467\n",
       "\n",
       "[783 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_driver_genes = pd.read_csv(f\"{data_dir}/cell_line/driver_genes_20221018.csv\")\n",
    "cancer_driver_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(406, 768)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_genes = list(set(X.columns) & set(cancer_driver_genes[\"symbol\"]))\n",
    "X_selected = X[cancer_genes]\n",
    "X_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "seed = 775\n",
    "\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_selected, y, random_state=seed, shuffle=True, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_scaler = StandardScaler().fit(X_train_df)\n",
    "train_scaled = train_scaler.transform(X_train_df)\n",
    "test_scaled = train_scaler.transform(X_test_df)\n",
    "\n",
    "X_train_df = pd.DataFrame(train_scaled, columns=X_train_df.columns)\n",
    "X_test_df = pd.DataFrame(test_scaled, columns=X_test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226, 768)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df = pd.DataFrame(train_scaled, columns=X_train_df.columns)\n",
    "X_test_df = pd.DataFrame(test_scaled, columns=X_test_df.columns)\n",
    "X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X_train_df, y_train_df, random_state=seed, \n",
    "                                                        shuffle=True, test_size=0.3)\n",
    "X_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_df.to_numpy(), y_train_df.to_numpy()\n",
    "X_test, y_test = X_test_df.to_numpy(), y_test_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gene_names = [col.strip() for col in X_train_df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Interaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbolA</th>\n",
       "      <th>symbolB</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARF5</td>\n",
       "      <td>ACAP2</td>\n",
       "      <td>0.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARF5</td>\n",
       "      <td>RAB1A</td>\n",
       "      <td>0.730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARF5</td>\n",
       "      <td>COPE</td>\n",
       "      <td>0.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARF5</td>\n",
       "      <td>ACAP1</td>\n",
       "      <td>0.765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARF5</td>\n",
       "      <td>COPZ1</td>\n",
       "      <td>0.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474973</th>\n",
       "      <td>EIF3L</td>\n",
       "      <td>EIF3K</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474974</th>\n",
       "      <td>EIF3L</td>\n",
       "      <td>EIF4G1</td>\n",
       "      <td>0.908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474975</th>\n",
       "      <td>EIF3L</td>\n",
       "      <td>EIF3E</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474976</th>\n",
       "      <td>EIF3L</td>\n",
       "      <td>RPL22</td>\n",
       "      <td>0.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474977</th>\n",
       "      <td>EIF3L</td>\n",
       "      <td>RPS2</td>\n",
       "      <td>0.963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>474978 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       symbolA symbolB  weight\n",
       "0         ARF5   ACAP2   0.767\n",
       "1         ARF5   RAB1A   0.730\n",
       "2         ARF5    COPE   0.745\n",
       "3         ARF5   ACAP1   0.765\n",
       "4         ARF5   COPZ1   0.757\n",
       "...        ...     ...     ...\n",
       "474973   EIF3L   EIF3K   0.999\n",
       "474974   EIF3L  EIF4G1   0.908\n",
       "474975   EIF3L   EIF3E   0.999\n",
       "474976   EIF3L   RPL22   0.738\n",
       "474977   EIF3L    RPS2   0.963\n",
       "\n",
       "[474978 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_ppi = pd.read_csv(f\"{data_dir}/cell_line/string_ppi.csv\")\n",
    "string_ppi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gene_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %autoreload\n",
    "# from nn_util import build_network_string\n",
    "# J = build_network_string(gene_names, string_ppi)\n",
    "# np.count_nonzero(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11862"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J = np.load(f\"{data_dir}/cell_line/cancer_genes_net.npy\")\n",
    "np.count_nonzero(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_bnn(model, X, y, params, gammas):\n",
    "    eval_fn = lambda p, g: model.apply(p, X, g).ravel()\n",
    "    preds = jax.vmap(eval_fn)(params, gammas)\n",
    "    preds = preds.reshape(-1, preds.shape[-1])\n",
    "    losses = jax.vmap(optax.l2_loss, in_axes=(0, None))(preds, y)\n",
    "    mean_loss = jnp.mean(losses, axis=-1)\n",
    "    return jnp.sum(mean_loss)\n",
    "\n",
    "def get_feats_dropout_loss(sgmcmc, X, y):\n",
    "    var_loss_dict = {\"feats_idx\": [], \"num_models\": [] , \"loss_on\": [], \"loss_off\": [], \"loss_diff\": []}\n",
    "    \n",
    "    disc_states = mixed_sgmcmc.states_.discrete_position\n",
    "    contin_states = mixed_sgmcmc.states_.contin_position\n",
    "    \n",
    "    \n",
    "\n",
    "    num_models = disc_states.shape[0]\n",
    "\n",
    "    p = X.shape[1]\n",
    "\n",
    "    for idx in range(p):\n",
    "        # idx = feats_idx[i]\n",
    "        idx_on = np.argwhere(disc_states[:,idx] == 1.).ravel()\n",
    "        loss_on, loss_off = 0., 0.\n",
    "        if idx_on.size == 0: ## irrelevant feature\n",
    "            loss_diff = 1e9\n",
    "        else:\n",
    "            disc_states_on = disc_states[idx_on]\n",
    "            params_on = jax.tree_util.tree_map(lambda x: x[idx_on], contin_states)\n",
    "            loss_on = evaluate_bnn(sgmcmc.model_, X, y, params_on, disc_states_on)\n",
    "\n",
    "            # Turn-off the variable, and see hwo the loss changes\n",
    "            disc_states_off = disc_states_on.at[:,idx].set(0)\n",
    "            loss_off = evaluate_bnn(sgmcmc.model_, X, y, params_on, disc_states_off)\n",
    "\n",
    "            loss_diff = (loss_on - loss_off) * (len(idx_on) / num_models)\n",
    "\n",
    "\n",
    "        var_loss_dict[\"feats_idx\"].append(idx)\n",
    "        var_loss_dict[\"num_models\"].append(idx_on.size)\n",
    "        var_loss_dict[\"loss_on\"].append(loss_on)\n",
    "        var_loss_dict[\"loss_off\"].append(loss_off)\n",
    "        var_loss_dict[\"loss_diff\"].append(loss_diff)\n",
    "\n",
    "\n",
    "    var_loss_df = pd.DataFrame(var_loss_dict).sort_values(by=\"loss_diff\")\n",
    "\n",
    "    return var_loss_df\n",
    "\n",
    "def get_gene_names(gene_cols):\n",
    "    return [gene.split(\"(\")[0].strip() for gene in gene_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from smac.facade.smac_mf_facade import SMAC4MF\n",
    "from smac.facade.smac_hpo_facade import SMAC4HPO\n",
    "from smac.scenario.scenario import Scenario\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from sgmcmc_cyclical_bkp import MixedSGMCMC\n",
    "from run_nn_fisher_test_exp import run_logistic_regression\n",
    "from smac.configspace import ConfigurationSpace\n",
    "from ConfigSpace.hyperparameters import (\n",
    "    CategoricalHyperparameter,\n",
    "    UniformFloatHyperparameter,\n",
    "    UniformIntegerHyperparameter,\n",
    ")\n",
    "\n",
    "from ConfigSpace import InCondition, Configuration\n",
    "\n",
    "import math\n",
    "\n",
    "def get_configspace(input_size)-> ConfigurationSpace:\n",
    "    # Build Configuration Space which defines all parameters and their ranges.\n",
    "\n",
    "    layer_dims = [50, 100, 150, 200, 250, 300, 350]\n",
    "\n",
    "    cs  = ConfigurationSpace()\n",
    "\n",
    "    # layer_dim = CategoricalHyperparameter(\"layer_dim\", layer_dims, default_value=layer_dims[0])\n",
    "    activation = CategoricalHyperparameter(\"activation\", [\"relu\", \"tanh\"], default_value=\"tanh\")\n",
    "    \n",
    "    disc_lr = CategoricalHyperparameter(\"disc_lr\", [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.5] , default_value=1e-1)\n",
    "    contin_lr = CategoricalHyperparameter(\"contin_lr\", [1e-5, 1e-4, 1e-3, 1e-2], default_value=1e-5) #TODO Extend range\n",
    "\n",
    "    # lr_schedule = CategoricalHyperparameter(\"lr_schedule\", [\"cyclical\"], default_value=\"cyclical\")\n",
    "    num_cycles = CategoricalHyperparameter(\"num_cycles\", [3, 4, 5, 7], default_value=5)\n",
    "    beta = CategoricalHyperparameter(\"beta\", [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95], default_value=0.9)\n",
    "\n",
    "    # batch_size = CategoricalHyperparameter(\"batch_size\", [16, 32, 64, 128], default_value=32)\n",
    "    # thinning_interval = CategoricalHyperparameter(\"thinning_interval\", [10, 50, 100, 150, 200], default_value=100)\n",
    "\n",
    "    # eta = CategoricalHyperparameter(\"eta\", [0.01, 0.1, 1., 5, 10., 50., 100.], default_value=1.)\n",
    "    mu = CategoricalHyperparameter(\"mu\", [0.01, 0.1, 1., 5, 10., 50., 100.], default_value=1.)\n",
    "    temp = CategoricalHyperparameter(\"temp\", [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1.], default_value=1.)\n",
    "    sigma = CategoricalHyperparameter(\"sigma\", [1e-3, 1e-2, 1e-1, 0.5, 1., 10.], default_value=1.)\n",
    "\n",
    "\n",
    "    # Add hyper-parameters\n",
    "    cs.add_hyperparameters([activation, disc_lr, contin_lr, mu, temp, sigma, beta])\n",
    "    # cs.add_hyperparameters([mu])\n",
    "\n",
    "    # # Cyclical SG-MCMC condition\n",
    "    # use_cycle_len = InCondition(child=cycle_len, parent=lr_schedule, values=[\"cyclical\"])\n",
    "    # use_beta = InCondition(child=beta, parent=lr_schedule, values=[\"cyclical\"])\n",
    "    #\n",
    "    # cs.add_conditions([use_cycle_len, use_beta])\n",
    "\n",
    "    return cs\n",
    "\n",
    "def get_cv_score(model, X, y, cv, activation_fns, J):\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        x_train_cv, y_train_cv = X[train_idx], y[train_idx]\n",
    "        x_test_cv, y_test_cv = X[test_idx], y[test_idx]\n",
    "        model.fit(x_train_cv, y_train_cv, activation_fns=activation_fns, J=J)\n",
    "        score = jax.device_get(model.score(x_test_cv, y_test_cv))\n",
    "        scores.append(score)\n",
    "    \n",
    "    mean_score = np.mean(np.array(scores))\n",
    "    if math.isnan(mean_score):\n",
    "        return -1e9\n",
    "    \n",
    "    return mean_score\n",
    "\n",
    "def generate_train_cs(seed, X, y, J):\n",
    "    cv = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "    # X_tr, X_val, y_tr, y_val = train_test_split(X, y, stratify=y, test_size=0.1, shuffle=seed)\n",
    "    def train_cs(config: Configuration)-> float:\n",
    "\n",
    "        params = {\"disc_lr\":config[\"disc_lr\"], \"contin_lr\": config[\"contin_lr\"], \"batch_size\": 32,\n",
    "                  \"mu\": config[\"mu\"], \"eta\": 1.0, \"temp\": config[\"temp\"],\n",
    "                  \"sigma\": config[\"sigma\"], \"beta\": config[\"beta\"], \"num_cycles\": 5}\n",
    "\n",
    "        # params = {\"disc_lr\": 0.1, \"contin_lr\": 1e-5, \"batch_size\": 20,\n",
    "        #           \"mu\": config[\"mu\"], \"eta\": 1.0, \"temp\": 1.0,\n",
    "        #           \"sigma\": 1.0, \"beta\": 0.95, \"num_cycles\": 5}\n",
    "\n",
    "        # print(config)\n",
    "        mixed_sgmcmc = MixedSGMCMC(seed=seed, n_samples=200, n_warmup=0, lr_schedule=\"cyclical\",\n",
    "                                    layer_dims=[400, 300, 100], classifier=False ,**params)\n",
    "\n",
    "        score = get_cv_score(mixed_sgmcmc, X, y, cv, [config[\"activation\"], config[\"activation\"], config[\"activation\"]], J)\n",
    "        # print(score)\n",
    "        return 1 - score\n",
    "\n",
    "\n",
    "    return train_cs\n",
    "\n",
    "\n",
    "def optimize_hyper_parameters(seed, X, y, J, total_time=60):\n",
    "    cs = get_configspace(X.shape[1])\n",
    "    scenario = Scenario({\n",
    "        \"run_obj\": \"quality\",\n",
    "        \"wallclock-limit\": total_time,\n",
    "        \"cs\": cs,\n",
    "        \"deterministic\": True,\n",
    "        \"cutoff\": 10,  # runtime limit for the target algorithm\n",
    "        \"verbose_level\": \"DEBUG\", \n",
    "        \"seed\": seed\n",
    "    })\n",
    "\n",
    "    max_steps = 5000\n",
    "\n",
    "    train_cs = generate_train_cs(seed, X, y, J)\n",
    "\n",
    "    intensifier_kwargs = {\"initial_budget\": 2000, \"max_budget\": max_steps}\n",
    "\n",
    "    # smac = SMAC4MF(scenario=scenario, rng=np.random.RandomState(seed),\n",
    "    #                tae_runner=train_cs, intensifier_kwargs=intensifier_kwargs)\n",
    "\n",
    "    smac = SMAC4HPO(scenario=scenario, rng=np.random.RandomState(seed), tae_runner=train_cs)\n",
    "\n",
    "    tae = smac.get_tae_runner()\n",
    "\n",
    "    try:\n",
    "        incumbent = smac.optimize()\n",
    "\n",
    "    finally:\n",
    "        incumbent = smac.solver.incumbent\n",
    "\n",
    "\n",
    "    inc_val = tae.run(config=incumbent, budget=max_steps, seed=seed)\n",
    "\n",
    "    return incumbent, 1 - inc_val[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smac.utils.io.cmd_reader.CMDReader:Adding unsupported scenario options: {'verbose_level': 'DEBUG', 'seed': 775}\n",
      "2022-11-30 15:20:46.997469: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2130] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Failed to allocate request for 300.00MiB (314572800B) on device ordinal 0\n",
      "BufferAssignment OOM Debugging.\n",
      "BufferAssignment stats:\n",
      "             parameter allocation:  300.00MiB\n",
      "              constant allocation:         0B\n",
      "        maybe_live_out allocation:  300.00MiB\n",
      "     preallocated temp allocation:         0B\n",
      "                 total allocation:  600.00MiB\n",
      "              total fragmentation:         0B (0.00%)\n",
      "Peak buffers:\n",
      "\tBuffer 1:\n",
      "\t\tSize: 300.00MiB\n",
      "\t\tOperator: op_name=\"jit(concatenate)/jit(main)/concatenate[dimension=0]\" source_file=\"/notebooks/tree_utils.py\" source_line=178\n",
      "\t\tXLA Label: concatenate\n",
      "\t\tShape: f32[256,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 2:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 3:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 4:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 5:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 6:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 7:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 8:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 9:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 10:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 11:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 12:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 13:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 14:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 15:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\n",
      "ERROR:smac.tae.execute_func.ExecuteTAFuncDict:RESOURCE_EXHAUSTED: Failed to allocate request for 300.00MiB (314572800B) on device ordinal 0\n",
      "BufferAssignment OOM Debugging.\n",
      "BufferAssignment stats:\n",
      "             parameter allocation:  300.00MiB\n",
      "              constant allocation:         0B\n",
      "        maybe_live_out allocation:  300.00MiB\n",
      "     preallocated temp allocation:         0B\n",
      "                 total allocation:  600.00MiB\n",
      "              total fragmentation:         0B (0.00%)\n",
      "Peak buffers:\n",
      "\tBuffer 1:\n",
      "\t\tSize: 300.00MiB\n",
      "\t\tOperator: op_name=\"jit(concatenate)/jit(main)/concatenate[dimension=0]\" source_file=\"/notebooks/tree_utils.py\" source_line=178\n",
      "\t\tXLA Label: concatenate\n",
      "\t\tShape: f32[256,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 2:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 3:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 4:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 5:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 6:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 7:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 8:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 9:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 10:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 11:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 12:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 13:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 14:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 15:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/smac/tae/execute_func.py\", line 217, in run\n",
      "    rval = self._call_ta(self._ta, config, obj_kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/smac/tae/execute_func.py\", line 314, in _call_ta\n",
      "    return obj(config, **obj_kwargs)\n",
      "  File \"/tmp/ipykernel_2857/2850916560.py\", line 92, in train_cs\n",
      "    score = get_cv_score(mixed_sgmcmc, X, y, cv, [config[\"activation\"], config[\"activation\"], config[\"activation\"]], J)\n",
      "  File \"/tmp/ipykernel_2857/2850916560.py\", line 65, in get_cv_score\n",
      "    model.fit(x_train_cv, y_train_cv, activation_fns=activation_fns, J=J)\n",
      "  File \"/notebooks/sgmcmc_cyclical_bkp.py\", line 98, in fit\n",
      "    self.exp_states = tree_utils.tree_stack(exp_samples) # TODO Remove this , only for debugging\n",
      "  File \"/notebooks/tree_utils.py\", line 178, in tree_stack\n",
      "    result_leaves = [jnp.stack(l) for l in grouped_leaves]\n",
      "  File \"/notebooks/tree_utils.py\", line 178, in <listcomp>\n",
      "    result_leaves = [jnp.stack(l) for l in grouped_leaves]\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/jax/_src/numpy/lax_numpy.py\", line 1636, in stack\n",
      "    return concatenate(new_arrays, axis=axis, dtype=dtype)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/jax/_src/numpy/lax_numpy.py\", line 1693, in concatenate\n",
      "    arrays = [lax.concatenate(arrays[i:i+k], axis)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/jax/_src/numpy/lax_numpy.py\", line 1693, in <listcomp>\n",
      "    arrays = [lax.concatenate(arrays[i:i+k], axis)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/jax/_src/lax/lax.py\", line 629, in concatenate\n",
      "    return concatenate_p.bind(*operands, dimension=dimension)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/jax/core.py\", line 325, in bind\n",
      "    return self.bind_with_trace(find_top_trace(args), args, params)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/jax/core.py\", line 328, in bind_with_trace\n",
      "    out = trace.process_primitive(self, map(trace.full_raise, args), params)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/jax/core.py\", line 686, in process_primitive\n",
      "    return primitive.impl(*tracers, **params)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/jax/_src/dispatch.py\", line 113, in apply_primitive\n",
      "    return compiled_fun(*args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/jax/_src/dispatch.py\", line 198, in <lambda>\n",
      "    return lambda *args, **kw: compiled(*args, **kw)[0]\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/jax/_src/dispatch.py\", line 837, in _execute_compiled\n",
      "    out_flat = compiled.execute(in_flat)\n",
      "jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Failed to allocate request for 300.00MiB (314572800B) on device ordinal 0\n",
      "BufferAssignment OOM Debugging.\n",
      "BufferAssignment stats:\n",
      "             parameter allocation:  300.00MiB\n",
      "              constant allocation:         0B\n",
      "        maybe_live_out allocation:  300.00MiB\n",
      "     preallocated temp allocation:         0B\n",
      "                 total allocation:  600.00MiB\n",
      "              total fragmentation:         0B (0.00%)\n",
      "Peak buffers:\n",
      "\tBuffer 1:\n",
      "\t\tSize: 300.00MiB\n",
      "\t\tOperator: op_name=\"jit(concatenate)/jit(main)/concatenate[dimension=0]\" source_file=\"/notebooks/tree_utils.py\" source_line=178\n",
      "\t\tXLA Label: concatenate\n",
      "\t\tShape: f32[256,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 2:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 3:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 4:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 5:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 6:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 7:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 8:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 9:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 10:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 11:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 12:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 13:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 14:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 15:\n",
      "\t\tSize: 18.75MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,400]\n",
      "\t\t==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31mFailed to connect to the remote Jupyter Server 'https://n4sy1opk5k.clg07azjl.paperspacegradient.com/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "# config, score = optimize_hyper_parameters(seed, X_train, y_train, J, total_time=300)\n",
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  'activation': 'tanh',\n",
    "  'beta': 0.90,\n",
    "  'contin_lr': 1e-5,\n",
    "  'disc_lr': 0.001,\n",
    "  'eta': 1.0,\n",
    "  'mu': 1.0,\n",
    "  'sigma': 1.0,\n",
    "  'temp': 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 16:22:43.395999: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2130] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Failed to allocate request for 487.50MiB (511180800B) on device ordinal 0\n",
      "BufferAssignment OOM Debugging.\n",
      "BufferAssignment stats:\n",
      "             parameter allocation:  487.50MiB\n",
      "              constant allocation:         0B\n",
      "        maybe_live_out allocation:  487.50MiB\n",
      "     preallocated temp allocation:         0B\n",
      "                 total allocation:  975.00MiB\n",
      "              total fragmentation:         0B (0.00%)\n",
      "Peak buffers:\n",
      "\tBuffer 1:\n",
      "\t\tSize: 487.50MiB\n",
      "\t\tOperator: op_name=\"jit(concatenate)/jit(main)/concatenate[dimension=0]\" source_file=\"/notebooks/tree_utils.py\" source_line=178\n",
      "\t\tXLA Label: concatenate\n",
      "\t\tShape: f32[208,768,800]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 2:\n",
      "\t\tSize: 37.50MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,800]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 3:\n",
      "\t\tSize: 37.50MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,800]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 4:\n",
      "\t\tSize: 37.50MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,800]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 5:\n",
      "\t\tSize: 37.50MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,800]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 6:\n",
      "\t\tSize: 37.50MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,800]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 7:\n",
      "\t\tSize: 37.50MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,800]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 8:\n",
      "\t\tSize: 37.50MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,800]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 9:\n",
      "\t\tSize: 37.50MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,800]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 10:\n",
      "\t\tSize: 37.50MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,800]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 11:\n",
      "\t\tSize: 37.50MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,800]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 12:\n",
      "\t\tSize: 37.50MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,800]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 13:\n",
      "\t\tSize: 37.50MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,800]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 14:\n",
      "\t\tSize: 37.50MiB\n",
      "\t\tEntry Parameter Subshape: f32[16,768,800]\n",
      "\t\t==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Failed to allocate request for 487.50MiB (511180800B) on device ordinal 0\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:  487.50MiB\n              constant allocation:         0B\n        maybe_live_out allocation:  487.50MiB\n     preallocated temp allocation:         0B\n                 total allocation:  975.00MiB\n              total fragmentation:         0B (0.00%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 487.50MiB\n\t\tOperator: op_name=\"jit(concatenate)/jit(main)/concatenate[dimension=0]\" source_file=\"/notebooks/tree_utils.py\" source_line=178\n\t\tXLA Label: concatenate\n\t\tShape: f32[208,768,800]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mXlaRuntimeError\u001B[0m                           Traceback (most recent call last)",
      "\u001B[1;32m/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/7_bnn_sgmcmc_tcga.ipynb Cell 29\u001B[0m in \u001B[0;36m<cell line: 14>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/7_bnn_sgmcmc_tcga.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001B[0m mixed_sgmcmc \u001B[39m=\u001B[39m MixedSGMCMC(seed\u001B[39m=\u001B[39mseed, n_samples\u001B[39m=\u001B[39m\u001B[39m100\u001B[39m, n_warmup\u001B[39m=\u001B[39m\u001B[39m0\u001B[39m, lr_schedule\u001B[39m=\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mcyclical\u001B[39m\u001B[39m\"\u001B[39m,\n\u001B[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/7_bnn_sgmcmc_tcga.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001B[0m                             layer_dims\u001B[39m=\u001B[39m[\u001B[39m800\u001B[39m], classifier\u001B[39m=\u001B[39m\u001B[39mFalse\u001B[39;00m ,\u001B[39m*\u001B[39m\u001B[39m*\u001B[39mparams)\n\u001B[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/7_bnn_sgmcmc_tcga.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001B[0m \u001B[39m# cv_score_bnn = get_cv_score(mixed_sgmcmc, X_train_sig, y_train, cv, config[\"activation_fns\"], J)\u001B[39;00m\n\u001B[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/7_bnn_sgmcmc_tcga.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001B[0m \u001B[39m# print(cv_score_bnn)\u001B[39;00m\n\u001B[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/7_bnn_sgmcmc_tcga.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001B[0m mixed_sgmcmc\u001B[39m.\u001B[39;49mfit(X_train, y_train, activation_fns\u001B[39m=\u001B[39;49m[\u001B[39m\"\u001B[39;49m\u001B[39mrelu\u001B[39;49m\u001B[39m\"\u001B[39;49m] , J\u001B[39m=\u001B[39;49mJ)\n\u001B[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/7_bnn_sgmcmc_tcga.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001B[0m n_models \u001B[39m=\u001B[39m mixed_sgmcmc\u001B[39m.\u001B[39mstates_\u001B[39m.\u001B[39mdiscrete_position\u001B[39m.\u001B[39mshape[\u001B[39m0\u001B[39m]\n\u001B[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/7_bnn_sgmcmc_tcga.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001B[0m \u001B[39mprint\u001B[39m(\u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mNum models: \u001B[39m\u001B[39m{\u001B[39;00mn_models\u001B[39m}\u001B[39;00m\u001B[39m\"\u001B[39m)\n",
      "File \u001B[0;32m/notebooks/sgmcmc_cyclical_bkp.py:98\u001B[0m, in \u001B[0;36mMixedSGMCMC.fit\u001B[0;34m(self, X, y, J, activation_fns)\u001B[0m\n\u001B[1;32m     88\u001B[0m \u001B[39m# if self.n_warmup > 0:\u001B[39;00m\n\u001B[1;32m     89\u001B[0m \u001B[39m#     disc_pos = tree_utils.tree_stack(np.array(disc_states)[:,self.n_warmup:])\u001B[39;00m\n\u001B[1;32m     90\u001B[0m \u001B[39m#     contin_pos = tree_utils.tree_stack([tree_utils.tree_stack(contin_states[i]) for i in range(self.n_chains)])\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[39m#     disc_pos = tree_utils.tree_stack(np.array(disc_states))\u001B[39;00m\n\u001B[1;32m     95\u001B[0m \u001B[39m#     contin_pos = tree_utils.tree_stack([tree_utils.tree_stack(contin_states[i]) for i in range(self.n_chains)])\u001B[39;00m\n\u001B[1;32m     97\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mstates_ \u001B[39m=\u001B[39m tree_utils\u001B[39m.\u001B[39mtree_stack(samples)\n\u001B[0;32m---> 98\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mexp_states \u001B[39m=\u001B[39m tree_utils\u001B[39m.\u001B[39;49mtree_stack(exp_samples) \u001B[39m# TODO Remove this , only for debugging\u001B[39;00m\n\u001B[1;32m     99\u001B[0m \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\n",
      "File \u001B[0;32m/notebooks/tree_utils.py:178\u001B[0m, in \u001B[0;36mtree_stack\u001B[0;34m(trees)\u001B[0m\n\u001B[1;32m    175\u001B[0m   treedef_list\u001B[39m.\u001B[39mappend(treedef)\n\u001B[1;32m    177\u001B[0m grouped_leaves \u001B[39m=\u001B[39m \u001B[39mzip\u001B[39m(\u001B[39m*\u001B[39mleaves_list)\n\u001B[0;32m--> 178\u001B[0m result_leaves \u001B[39m=\u001B[39m [jnp\u001B[39m.\u001B[39mstack(l) \u001B[39mfor\u001B[39;00m l \u001B[39min\u001B[39;00m grouped_leaves]\n\u001B[1;32m    179\u001B[0m \u001B[39mreturn\u001B[39;00m treedef_list[\u001B[39m0\u001B[39m]\u001B[39m.\u001B[39munflatten(result_leaves)\n",
      "File \u001B[0;32m/notebooks/tree_utils.py:178\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    175\u001B[0m   treedef_list\u001B[39m.\u001B[39mappend(treedef)\n\u001B[1;32m    177\u001B[0m grouped_leaves \u001B[39m=\u001B[39m \u001B[39mzip\u001B[39m(\u001B[39m*\u001B[39mleaves_list)\n\u001B[0;32m--> 178\u001B[0m result_leaves \u001B[39m=\u001B[39m [jnp\u001B[39m.\u001B[39;49mstack(l) \u001B[39mfor\u001B[39;00m l \u001B[39min\u001B[39;00m grouped_leaves]\n\u001B[1;32m    179\u001B[0m \u001B[39mreturn\u001B[39;00m treedef_list[\u001B[39m0\u001B[39m]\u001B[39m.\u001B[39munflatten(result_leaves)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/numpy/lax_numpy.py:1636\u001B[0m, in \u001B[0;36mstack\u001B[0;34m(arrays, axis, out, dtype)\u001B[0m\n\u001B[1;32m   1634\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\u001B[39m\"\u001B[39m\u001B[39mAll input arrays must have the same shape.\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[1;32m   1635\u001B[0m   new_arrays\u001B[39m.\u001B[39mappend(expand_dims(a, axis))\n\u001B[0;32m-> 1636\u001B[0m \u001B[39mreturn\u001B[39;00m concatenate(new_arrays, axis\u001B[39m=\u001B[39;49maxis, dtype\u001B[39m=\u001B[39;49mdtype)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/numpy/lax_numpy.py:1693\u001B[0m, in \u001B[0;36mconcatenate\u001B[0;34m(arrays, axis, dtype)\u001B[0m\n\u001B[1;32m   1691\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m   1692\u001B[0m   \u001B[39mwhile\u001B[39;00m \u001B[39mlen\u001B[39m(arrays) \u001B[39m>\u001B[39m \u001B[39m1\u001B[39m:\n\u001B[0;32m-> 1693\u001B[0m     arrays \u001B[39m=\u001B[39m [lax\u001B[39m.\u001B[39mconcatenate(arrays[i:i\u001B[39m+\u001B[39mk], axis)\n\u001B[1;32m   1694\u001B[0m               \u001B[39mfor\u001B[39;00m i \u001B[39min\u001B[39;00m \u001B[39mrange\u001B[39m(\u001B[39m0\u001B[39m, \u001B[39mlen\u001B[39m(arrays), k)]\n\u001B[1;32m   1695\u001B[0m   \u001B[39mreturn\u001B[39;00m arrays[\u001B[39m0\u001B[39m]\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/numpy/lax_numpy.py:1693\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1691\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m   1692\u001B[0m   \u001B[39mwhile\u001B[39;00m \u001B[39mlen\u001B[39m(arrays) \u001B[39m>\u001B[39m \u001B[39m1\u001B[39m:\n\u001B[0;32m-> 1693\u001B[0m     arrays \u001B[39m=\u001B[39m [lax\u001B[39m.\u001B[39;49mconcatenate(arrays[i:i\u001B[39m+\u001B[39;49mk], axis)\n\u001B[1;32m   1694\u001B[0m               \u001B[39mfor\u001B[39;00m i \u001B[39min\u001B[39;00m \u001B[39mrange\u001B[39m(\u001B[39m0\u001B[39m, \u001B[39mlen\u001B[39m(arrays), k)]\n\u001B[1;32m   1695\u001B[0m   \u001B[39mreturn\u001B[39;00m arrays[\u001B[39m0\u001B[39m]\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/lax/lax.py:629\u001B[0m, in \u001B[0;36mconcatenate\u001B[0;34m(operands, dimension)\u001B[0m\n\u001B[1;32m    627\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mlen\u001B[39m(operands) \u001B[39m==\u001B[39m \u001B[39m0\u001B[39m:\n\u001B[1;32m    628\u001B[0m   \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\u001B[39m\"\u001B[39m\u001B[39mconcatenate requires a non-empty sequences of arrays\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[0;32m--> 629\u001B[0m \u001B[39mreturn\u001B[39;00m concatenate_p\u001B[39m.\u001B[39;49mbind(\u001B[39m*\u001B[39;49moperands, dimension\u001B[39m=\u001B[39;49mdimension)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/jax/core.py:325\u001B[0m, in \u001B[0;36mPrimitive.bind\u001B[0;34m(self, *args, **params)\u001B[0m\n\u001B[1;32m    322\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mbind\u001B[39m(\u001B[39mself\u001B[39m, \u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mparams):\n\u001B[1;32m    323\u001B[0m   \u001B[39massert\u001B[39;00m (\u001B[39mnot\u001B[39;00m config\u001B[39m.\u001B[39mjax_enable_checks \u001B[39mor\u001B[39;00m\n\u001B[1;32m    324\u001B[0m           \u001B[39mall\u001B[39m(\u001B[39misinstance\u001B[39m(arg, Tracer) \u001B[39mor\u001B[39;00m valid_jaxtype(arg) \u001B[39mfor\u001B[39;00m arg \u001B[39min\u001B[39;00m args)), args\n\u001B[0;32m--> 325\u001B[0m   \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mbind_with_trace(find_top_trace(args), args, params)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/jax/core.py:328\u001B[0m, in \u001B[0;36mPrimitive.bind_with_trace\u001B[0;34m(self, trace, args, params)\u001B[0m\n\u001B[1;32m    327\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mbind_with_trace\u001B[39m(\u001B[39mself\u001B[39m, trace, args, params):\n\u001B[0;32m--> 328\u001B[0m   out \u001B[39m=\u001B[39m trace\u001B[39m.\u001B[39;49mprocess_primitive(\u001B[39mself\u001B[39;49m, \u001B[39mmap\u001B[39;49m(trace\u001B[39m.\u001B[39;49mfull_raise, args), params)\n\u001B[1;32m    329\u001B[0m   \u001B[39mreturn\u001B[39;00m \u001B[39mmap\u001B[39m(full_lower, out) \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mmultiple_results \u001B[39melse\u001B[39;00m full_lower(out)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/jax/core.py:686\u001B[0m, in \u001B[0;36mEvalTrace.process_primitive\u001B[0;34m(self, primitive, tracers, params)\u001B[0m\n\u001B[1;32m    685\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mprocess_primitive\u001B[39m(\u001B[39mself\u001B[39m, primitive, tracers, params):\n\u001B[0;32m--> 686\u001B[0m   \u001B[39mreturn\u001B[39;00m primitive\u001B[39m.\u001B[39;49mimpl(\u001B[39m*\u001B[39;49mtracers, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mparams)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/dispatch.py:113\u001B[0m, in \u001B[0;36mapply_primitive\u001B[0;34m(prim, *args, **params)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[39m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001B[39;00m\n\u001B[1;32m    111\u001B[0m compiled_fun \u001B[39m=\u001B[39m xla_primitive_callable(prim, \u001B[39m*\u001B[39munsafe_map(arg_spec, args),\n\u001B[1;32m    112\u001B[0m                                       \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mparams)\n\u001B[0;32m--> 113\u001B[0m \u001B[39mreturn\u001B[39;00m compiled_fun(\u001B[39m*\u001B[39;49margs)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/dispatch.py:198\u001B[0m, in \u001B[0;36mxla_primitive_callable.<locals>.<lambda>\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    195\u001B[0m compiled \u001B[39m=\u001B[39m _xla_callable_uncached(lu\u001B[39m.\u001B[39mwrap_init(prim_fun), device, \u001B[39mNone\u001B[39;00m,\n\u001B[1;32m    196\u001B[0m                                   prim\u001B[39m.\u001B[39mname, donated_invars, \u001B[39mFalse\u001B[39;00m, \u001B[39m*\u001B[39marg_specs)\n\u001B[1;32m    197\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m prim\u001B[39m.\u001B[39mmultiple_results:\n\u001B[0;32m--> 198\u001B[0m   \u001B[39mreturn\u001B[39;00m \u001B[39mlambda\u001B[39;00m \u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkw: compiled(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkw)[\u001B[39m0\u001B[39m]\n\u001B[1;32m    199\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    200\u001B[0m   \u001B[39mreturn\u001B[39;00m compiled\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/dispatch.py:837\u001B[0m, in \u001B[0;36m_execute_compiled\u001B[0;34m(name, compiled, input_handler, output_buffer_counts, result_handler, has_unordered_effects, ordered_effects, kept_var_idx, has_host_callbacks, *args)\u001B[0m\n\u001B[1;32m    835\u001B[0m     runtime_token \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m\n\u001B[1;32m    836\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m--> 837\u001B[0m   out_flat \u001B[39m=\u001B[39m compiled\u001B[39m.\u001B[39;49mexecute(in_flat)\n\u001B[1;32m    838\u001B[0m check_special(name, out_flat)\n\u001B[1;32m    839\u001B[0m out_bufs \u001B[39m=\u001B[39m unflatten(out_flat, output_buffer_counts)\n",
      "\u001B[0;31mXlaRuntimeError\u001B[0m: RESOURCE_EXHAUSTED: Failed to allocate request for 487.50MiB (511180800B) on device ordinal 0\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:  487.50MiB\n              constant allocation:         0B\n        maybe_live_out allocation:  487.50MiB\n     preallocated temp allocation:         0B\n                 total allocation:  975.00MiB\n              total fragmentation:         0B (0.00%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 487.50MiB\n\t\tOperator: op_name=\"jit(concatenate)/jit(main)/concatenate[dimension=0]\" source_file=\"/notebooks/tree_utils.py\" source_line=178\n\t\tXLA Label: concatenate\n\t\tShape: f32[208,768,800]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 37.50MiB\n\t\tEntry Parameter Subshape: f32[16,768,800]\n\t\t==========================\n\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from sgmcmc_cyclical_bkp import MixedSGMCMC\n",
    "from sklearn.metrics import r2_score\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "params = {\"disc_lr\":config[\"disc_lr\"], \"contin_lr\": config[\"contin_lr\"], \"batch_size\": 32,\n",
    "                  \"mu\": config[\"mu\"], \"eta\": config[\"eta\"], \"temp\": config[\"temp\"],\n",
    "                  \"sigma\": config[\"sigma\"], \"beta\": config[\"beta\"], \"num_cycles\": 5}\n",
    "\n",
    "mixed_sgmcmc = MixedSGMCMC(seed=seed, n_samples=100, n_warmup=0, lr_schedule=\"cyclical\",\n",
    "                            layer_dims=[300, 100], classifier=False ,**params)\n",
    "\n",
    "# cv_score_bnn = get_cv_score(mixed_sgmcmc, X_train_sig, y_train, cv, config[\"activation_fns\"], J)\n",
    "# print(cv_score_bnn)\n",
    "mixed_sgmcmc.fit(X_train, y_train, activation_fns=[\"relu\"] , J=J)\n",
    "n_models = mixed_sgmcmc.states_.discrete_position.shape[0]\n",
    "print(f\"Num models: {n_models}\")\n",
    "print(f\"test_score: {mixed_sgmcmc.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from gibbs_sampler_cyclical_bkp import *\n",
    "from nn_models import *\n",
    "disc_logprior_fn = generate_disc_logprior_fn(J, config[\"mu\"], config[\"eta\"], config[\"temp\"])\n",
    "contin_logprior_fn = generate_contin_logprior_fn(config[\"sigma\"], config[\"temp\"])\n",
    "log_ll_fn = make_gaussian_likelihood(config[\"sigma\"], config[\"temp\"], X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(-0.5793755, dtype=float32),\n",
       " DeviceArray([ 3.325289  ,  1.3569763 ,  3.9624488 ,  5.126395  ,\n",
       "               5.0288014 ,  1.1789916 ,  1.8843458 ,  1.2986592 ,\n",
       "               0.14187036,  2.7664676 ,  4.1776967 ,  4.354251  ,\n",
       "               3.9846854 ,  3.1764054 ,  3.2539606 ,  4.0630555 ,\n",
       "               2.5592132 ,  7.8799405 ,  3.6811771 ,  0.39229938,\n",
       "               2.874559  ,  3.1386042 ,  4.4332466 ,  1.9259742 ,\n",
       "               0.86165994,  5.2674828 ,  0.29588765,  1.9810389 ,\n",
       "               7.1189327 ,  5.4239845 ,  2.3757286 ,  1.8636016 ,\n",
       "               5.238155  ,  0.7995595 ,  0.79229647,  2.2837548 ,\n",
       "               2.4337616 ,  1.028684  ,  0.6879557 ,  0.5541406 ,\n",
       "               1.0129316 ,  4.274734  ,  1.4592621 ,  1.0597363 ,\n",
       "               0.79880494,  2.669365  ,  1.1404643 ,  4.3138433 ,\n",
       "               0.70505804,  2.7612405 ,  3.0075426 ,  6.1055617 ,\n",
       "               3.750311  ,  1.2408602 ,  2.9639528 ,  4.8589306 ,\n",
       "               9.72441   ,  0.5923012 ,  2.0433834 ,  0.33815596,\n",
       "               0.74095577,  5.9190207 ,  1.8873838 ,  9.453848  ,\n",
       "               2.3308127 ,  2.6210775 ,  8.187281  ,  6.2338386 ,\n",
       "               6.67389   ,  4.1218543 ,  0.2337703 ,  4.2833214 ,\n",
       "               2.3801124 ,  5.7557635 ,  4.6937785 ,  5.7013073 ,\n",
       "               3.0181653 ,  2.4198258 ,  2.9059737 ,  1.708078  ,\n",
       "               1.9184977 ,  1.6035732 ,  0.550202  ,  1.4148937 ,\n",
       "               6.1138945 ,  3.2443113 ,  3.3976107 ,  3.045582  ,\n",
       "               0.4089324 ,  5.955312  ,  3.4650083 ,  0.03880749,\n",
       "               4.38681   ,  0.88616246,  4.572445  ,  0.4039505 ,\n",
       "               3.2866468 ,  1.3193156 ,  3.250021  ,  3.9711914 ,\n",
       "               1.913724  ,  2.0766776 ,  0.65586895,  4.467641  ,\n",
       "               3.0213692 ,  4.3775086 ,  4.367671  ,  1.7455307 ,\n",
       "               3.3734677 ,  3.3653915 ,  3.581344  ,  2.2273686 ,\n",
       "               4.90845   ,  1.212039  ,  2.3184917 ,  3.0930946 ,\n",
       "               2.5859401 ,  0.704991  ,  0.804094  ,  3.180392  ,\n",
       "               2.9087753 ,  5.6217194 ,  2.0084155 ,  0.58938384,\n",
       "               0.30908144,  0.9580295 ,  5.02736   ,  0.9236308 ,\n",
       "               1.9607781 ,  2.005004  ,  0.52196544,  1.7650745 ,\n",
       "               1.7129362 ,  3.2632897 ,  0.9834015 ,  2.9039419 ,\n",
       "               0.22214782,  0.5833066 ,  4.3068275 ,  0.9374987 ,\n",
       "               3.3319893 ,  3.3297362 , 10.966433  ,  6.9757314 ,\n",
       "               1.8706212 ,  1.946619  ,  5.182845  ,  4.9787736 ,\n",
       "               4.465899  ,  2.1891441 ,  1.2019367 ,  3.52309   ,\n",
       "               2.4682665 , 11.322557  ,  2.4473538 ,  5.6334257 ,\n",
       "               1.100281  ,  2.6806264 ,  2.2511327 ,  3.083617  ,\n",
       "               4.795532  ,  0.8728519 ,  2.798572  ,  7.641222  ,\n",
       "               2.4156444 ,  1.8657721 ,  2.6583745 ,  5.1276617 ,\n",
       "               6.062443  ,  1.6385391 ,  4.8554816 ,  3.0712852 ,\n",
       "               3.925303  ,  4.648536  ,  1.5490053 ,  1.8879462 ,\n",
       "               1.9440777 ,  2.4691277 ,  1.6867378 ,  3.1762135 ,\n",
       "               3.4173255 ,  0.8729449 ,  0.02012365,  1.8214364 ,\n",
       "               1.4222465 ,  2.0434003 ,  1.0665926 ,  3.1891816 ,\n",
       "               0.55190647,  5.1765814 ,  5.549215  ,  1.0733399 ,\n",
       "               1.2370099 ,  3.7005467 ,  2.6227267 ,  8.525386  ,\n",
       "               2.9753664 ,  1.5148209 ,  5.26947   ,  4.943788  ,\n",
       "               3.7105331 ,  1.0492461 ,  1.4442415 ,  2.8003821 ,\n",
       "               3.1005518 ,  4.244257  ,  1.2661932 ,  2.0119002 ,\n",
       "               1.4234108 ,  0.42971408,  8.021919  ,  4.3736925 ,\n",
       "               0.16876073,  4.232468  ,  4.0881968 ,  6.821519  ,\n",
       "               2.9393334 ,  2.4176092 ,  3.2445831 ,  0.52073586,\n",
       "               7.4499636 ,  2.4446177 ,  2.4037824 ,  2.7258153 ,\n",
       "               1.0130457 ,  2.0680387 ], dtype=float32))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "log_ll_fn(mixed_sgmcmc.model_, contin_pos, [X_train, y_train], disc_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(182463.14, dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "             nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],            dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_sgmcmc.exp_states.contin_logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/7_bnn_sgmcmc_tcga.ipynb Cell 31\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/7_bnn_sgmcmc_tcga.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001B[0m stats\u001B[39m.\u001B[39;49mpearsonr(y_test, mixed_sgmcmc\u001B[39m.\u001B[39;49mpredict(X_test))\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/scipy/stats/_stats_py.py:4090\u001B[0m, in \u001B[0;36mpearsonr\u001B[0;34m(x, y)\u001B[0m\n\u001B[1;32m   4085\u001B[0m ym \u001B[39m=\u001B[39m y\u001B[39m.\u001B[39mastype(dtype) \u001B[39m-\u001B[39m ymean\n\u001B[1;32m   4087\u001B[0m \u001B[39m# Unlike np.linalg.norm or the expression sqrt((xm*xm).sum()),\u001B[39;00m\n\u001B[1;32m   4088\u001B[0m \u001B[39m# scipy.linalg.norm(xm) does not overflow if xm is, for example,\u001B[39;00m\n\u001B[1;32m   4089\u001B[0m \u001B[39m# [-5e210, 5e210, 3e200, -3e200]\u001B[39;00m\n\u001B[0;32m-> 4090\u001B[0m normxm \u001B[39m=\u001B[39m linalg\u001B[39m.\u001B[39;49mnorm(xm)\n\u001B[1;32m   4091\u001B[0m normym \u001B[39m=\u001B[39m linalg\u001B[39m.\u001B[39mnorm(ym)\n\u001B[1;32m   4093\u001B[0m threshold \u001B[39m=\u001B[39m \u001B[39m1e-13\u001B[39m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/scipy/linalg/_misc.py:145\u001B[0m, in \u001B[0;36mnorm\u001B[0;34m(a, ord, axis, keepdims, check_finite)\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[39m# Differs from numpy only in non-finite handling and the use of blas.\u001B[39;00m\n\u001B[1;32m    144\u001B[0m \u001B[39mif\u001B[39;00m check_finite:\n\u001B[0;32m--> 145\u001B[0m     a \u001B[39m=\u001B[39m np\u001B[39m.\u001B[39;49masarray_chkfinite(a)\n\u001B[1;32m    146\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    147\u001B[0m     a \u001B[39m=\u001B[39m np\u001B[39m.\u001B[39masarray(a)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/numpy/lib/function_base.py:627\u001B[0m, in \u001B[0;36masarray_chkfinite\u001B[0;34m(a, dtype, order)\u001B[0m\n\u001B[1;32m    625\u001B[0m a \u001B[39m=\u001B[39m asarray(a, dtype\u001B[39m=\u001B[39mdtype, order\u001B[39m=\u001B[39morder)\n\u001B[1;32m    626\u001B[0m \u001B[39mif\u001B[39;00m a\u001B[39m.\u001B[39mdtype\u001B[39m.\u001B[39mchar \u001B[39min\u001B[39;00m typecodes[\u001B[39m'\u001B[39m\u001B[39mAllFloat\u001B[39m\u001B[39m'\u001B[39m] \u001B[39mand\u001B[39;00m \u001B[39mnot\u001B[39;00m np\u001B[39m.\u001B[39misfinite(a)\u001B[39m.\u001B[39mall():\n\u001B[0;32m--> 627\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\n\u001B[1;32m    628\u001B[0m         \u001B[39m\"\u001B[39m\u001B[39marray must not contain infs or NaNs\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[1;32m    629\u001B[0m \u001B[39mreturn\u001B[39;00m a\n",
      "\u001B[0;31mValueError\u001B[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "stats.pearsonr(y_test, mixed_sgmcmc.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "disc_per_cycle = mixed_sgmcmc.states_.discrete_position.reshape(mixed_sgmcmc.num_cycles, -1, mixed_sgmcmc.states_.discrete_position.shape[-1])\n",
    "contin_per_cycle = jax.tree_util.tree_map(lambda x: x.reshape((mixed_sgmcmc.num_cycles, -1) + x.shape[1:]), mixed_sgmcmc.states_.contin_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gibbs_sampler_cyclical_bkp import make_gaussian_likelihood\n",
    "\n",
    "log_ll = make_gaussian_likelihood(mixed_sgmcmc.sigma, mixed_sgmcmc.temp, X_train_sig.shape[0])\n",
    "\n",
    "per_cycle_ll = jax.vmap(jax.vmap(lambda p, g: log_ll(mixed_sgmcmc.model_, p, [X_train_sig, y_train], g)))(contin_per_cycle, disc_per_cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(mixed_sgmcmc.num_cycles):\n",
    "    plt.plot(per_cycle_ll[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-136.03757, -125.16908, -128.7287 ], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "hmean = 1./(jnp.sum(1/per_cycle_ll, axis=-1)/per_cycle_ll.shape[-1])\n",
    "hmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred_fn = lambda p, g: mixed_sgmcmc.model_.apply(p, X_train_sig, g).ravel()\n",
    "\n",
    "per_cycle_pred = jax.vmap(jax.vmap(pred_fn))(contin_per_cycle, disc_per_cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feats_idx</th>\n",
       "      <th>num_models</th>\n",
       "      <th>loss_on</th>\n",
       "      <th>loss_off</th>\n",
       "      <th>loss_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>1452333.2</td>\n",
       "      <td>1452557.6</td>\n",
       "      <td>-224.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>50</td>\n",
       "      <td>1452333.2</td>\n",
       "      <td>1452422.8</td>\n",
       "      <td>-89.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>50</td>\n",
       "      <td>1452333.2</td>\n",
       "      <td>1452417.0</td>\n",
       "      <td>-83.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>50</td>\n",
       "      <td>1452333.2</td>\n",
       "      <td>1452404.2</td>\n",
       "      <td>-71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>50</td>\n",
       "      <td>1452333.2</td>\n",
       "      <td>1452399.9</td>\n",
       "      <td>-66.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>50</td>\n",
       "      <td>1452333.2</td>\n",
       "      <td>1452247.2</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>50</td>\n",
       "      <td>1452333.2</td>\n",
       "      <td>1452235.8</td>\n",
       "      <td>97.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>50</td>\n",
       "      <td>1452333.2</td>\n",
       "      <td>1452218.8</td>\n",
       "      <td>114.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>50</td>\n",
       "      <td>1452333.2</td>\n",
       "      <td>1452212.0</td>\n",
       "      <td>121.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>50</td>\n",
       "      <td>1452333.2</td>\n",
       "      <td>1452193.8</td>\n",
       "      <td>139.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feats_idx  num_models    loss_on   loss_off loss_diff\n",
       "100        100          50  1452333.2  1452557.6  -224.375\n",
       "64          64          50  1452333.2  1452422.8     -89.5\n",
       "67          67          50  1452333.2  1452417.0    -83.75\n",
       "82          82          50  1452333.2  1452404.2     -71.0\n",
       "41          41          50  1452333.2  1452399.9   -66.625\n",
       "..         ...         ...        ...        ...       ...\n",
       "72          72          50  1452333.2  1452247.2      86.0\n",
       "99          99          50  1452333.2  1452235.8      97.5\n",
       "90          90          50  1452333.2  1452218.8     114.5\n",
       "76          76          50  1452333.2  1452212.0    121.25\n",
       "79          79          50  1452333.2  1452193.8     139.5\n",
       "\n",
       "[101 rows x 5 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_loss = get_feats_dropout_loss(mixed_sgmcmc, X_train_sig, y_train)\n",
    "dropout_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ESR1',\n",
       " 'INHBB',\n",
       " 'GPRC5C',\n",
       " 'KRT15',\n",
       " 'KRT80',\n",
       " 'KRT8',\n",
       " 'KRT18',\n",
       " 'CUEDC1',\n",
       " 'S100A10',\n",
       " 'TPBG',\n",
       " 'WFDC3',\n",
       " 'ARHGEF28',\n",
       " 'ABCC3',\n",
       " 'S100A11',\n",
       " 'TUFT1',\n",
       " 'AHNAK2',\n",
       " 'LRRC8E',\n",
       " 'TM4SF1',\n",
       " 'TNFRSF12A',\n",
       " 'GNG12']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_feat_idx = dropout_loss[\"feats_idx\"][:20].to_list()\n",
    "bnn_gene_list = X_train_sig_df.iloc[:,bnn_feat_idx].columns.to_list()\n",
    "bnn_gene_list = get_gene_names(bnn_gene_list)\n",
    "bnn_gene_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 15:02:36.314463: W external/org_tensorflow/tensorflow/compiler/xla/service/platform_util.cc:190] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 16900227072\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "2022-11-30 15:02:36.508867: W external/org_tensorflow/tensorflow/compiler/xla/service/platform_util.cc:190] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 16900227072\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "2022-11-30 15:02:36.568897: W external/org_tensorflow/tensorflow/compiler/xla/service/platform_util.cc:190] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 16900227072\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "2022-11-30 15:02:36.606662: W external/org_tensorflow/tensorflow/compiler/xla/service/platform_util.cc:190] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 16900227072\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "2022-11-30 15:02:36.643585: W external/org_tensorflow/tensorflow/compiler/xla/service/platform_util.cc:190] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 16900227072\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "2022-11-30 15:02:36.660471: W external/org_tensorflow/tensorflow/compiler/xla/service/platform_util.cc:190] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 16900227072\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF params: {'bootstrap': True, 'max_depth': 80, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "cv score: 0.24886105920299037, test_score:  0.26936704281143586\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "rf_reg = RandomForestRegressor(random_state=seed, max_samples=0.9)\n",
    "grid_cv = GridSearchCV(estimator = rf_reg, param_grid = param_grid, \n",
    "                          cv = cv, n_jobs = -1, verbose = 1, scoring=\"explained_variance\").fit(X_train, y_train)\n",
    "\n",
    "rf_reg = RandomForestRegressor(random_state=seed, max_samples=0.9,**grid_cv.best_params_)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "y_test_pred_rf = rf_reg.predict(X_test)\n",
    "print(f\"RF params: {grid_cv.best_params_}\")\n",
    "print(f\"cv score: {grid_cv.best_score_}, test_score:  {r2_score(y_test, y_test_pred_rf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5520994714048856, 7.580750109139147e-08)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.pearsonr(y_test, rf_reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IKZF1',\n",
       " 'HSPG2',\n",
       " 'RHPN2',\n",
       " 'TFAP4',\n",
       " 'SMARCA1',\n",
       " 'ARID1A',\n",
       " 'MSH6',\n",
       " 'GNAQ',\n",
       " 'RGL3',\n",
       " 'EGFR',\n",
       " 'GNA11',\n",
       " 'CNTRL',\n",
       " 'FLI1',\n",
       " 'LMNA',\n",
       " 'LOX',\n",
       " 'WWTR1',\n",
       " 'SETD2',\n",
       " 'VHL',\n",
       " 'FSTL3',\n",
       " 'ARHGEF10L']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_feat_idx = np.argsort(rf_reg.feature_importances_)[::-1][:20]\n",
    "rf_gene_list = X_train_df.iloc[:,rf_feat_idx].columns.to_list()\n",
    "rf_gene_list = get_gene_names(rf_gene_list)\n",
    "rf_gene_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gplearn.genetic import SymbolicTransformer, SymbolicClassifier, SymbolicRegressor\n",
    "from gplearn.functions import make_function\n",
    "import operator\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_best_programs(gp, num_models, classifier=True, ascending=True, sort_fit=\"OOB_fitness\"):\n",
    "    gp_dict = {'Gen': [], \"Ind\": [], \"Fitness\": [], 'OOB_fitness': [], \"Equation\": []}\n",
    "\n",
    "    if classifier:\n",
    "        for idGen in range(len(gp._programs)):\n",
    "            for idPopulation in range(gp.population_size):\n",
    "                gp_dict[\"Gen\"].append(idGen)\n",
    "                gp_dict[\"Ind\"].append(idPopulation)\n",
    "                gp_dict[\"Fitness\"].append(gp._programs[idGen][idPopulation].fitness_)\n",
    "                gp_dict[\"OOB_fitness\"].append(gp._programs[idGen][idPopulation].oob_fitness_)\n",
    "                gp_dict[\"Equation\"].append(str(gp._programs[idGen][idPopulation]))\n",
    "    else:\n",
    "        for idx, prog in enumerate(gp._programs[-1]):\n",
    "                gp_dict[\"Gen\"].append(-1)\n",
    "                gp_dict[\"Ind\"].append(idx)\n",
    "                gp_dict[\"Fitness\"].append(prog.fitness_)\n",
    "                gp_dict[\"OOB_fitness\"].append(prog.oob_fitness_)\n",
    "                gp_dict[\"Equation\"].append(str(prog))\n",
    "\n",
    "    gp_df = pd.DataFrame(gp_dict).sort_values(sort_fit, ascending=ascending)[:num_models]\n",
    "    programs = []\n",
    "    for i in range(num_models):\n",
    "        gen, ind = int(gp_df.iloc[i][\"Gen\"]), int(gp_df.iloc[i][\"Ind\"])\n",
    "        programs.append(gp._programs[gen][ind])\n",
    "\n",
    "    return programs, gp_df\n",
    "\n",
    "\n",
    "def gp_transform(est, X, classifier=False, num_models=100, sort_fit=\"Fitness\"):\n",
    "    if classifier or (sort_fit == \"OOB_fitness\"):\n",
    "        programs, gp_df = get_best_programs(est, num_models, classifier, sort_fit=sort_fit, ascending=classifier)\n",
    "        out = np.zeros((X.shape[0], len(programs)))\n",
    "        for i, prog in enumerate(programs):\n",
    "            out[:, i] = prog.execute(X)\n",
    "\n",
    "        return out, gp_df\n",
    "    else:\n",
    "        return est.transform(X), None\n",
    "\n",
    "function_set = ['add', 'sub', 'mul', 'div', 'sqrt', 'log',\n",
    "                'abs', 'neg', 'inv', 'max', 'min']\n",
    "\n",
    "def train_gp(feats_idx, X_train, X_val, X_test, y_train, y_val, y_test, num_models=5, sort_fit=\"OOB_fitness\"):\n",
    "    X_gp_train, X_gp_val, X_gp_test = X_val[:,feats_idx], X_train[:,feats_idx], X_test[:,feats_idx]\n",
    "    gp_est = SymbolicTransformer(population_size=1000, hall_of_fame=100, n_components=20, generations=100,\n",
    "                           p_crossover=0.7, p_subtree_mutation=0.1,\n",
    "                           p_hoist_mutation=0.05, p_point_mutation=0.1,\n",
    "                           max_samples=0.8, verbose=0,\n",
    "                           parsimony_coefficient=0.005, random_state=seed)\n",
    "\n",
    "    gp_est.fit(X_gp_train, y_val)\n",
    "    gp_features_val, gp_val_df = gp_transform(gp_est, X_gp_val, classifier=False, sort_fit=sort_fit, num_models=num_models)\n",
    "    gp_features_test, gp_test_df = gp_transform(gp_est, X_gp_test, classifier=False, sort_fit=sort_fit, num_models=num_models)\n",
    "\n",
    "    X_val_comb, X_test_comb = np.concatenate([X_gp_val, gp_features_val], axis=1),\\\n",
    "                             np.concatenate([X_gp_test, gp_features_test], axis=1)\n",
    "\n",
    "    param_grid = {\"alpha\":np.logspace(-2, 2, 20)}\n",
    "    grid_cv = GridSearchCV(estimator=Ridge(max_iter=10000), param_grid=param_grid, \n",
    "                                            verbose=0, scoring=\"explained_variance\", cv=cv).fit(X_val_comb, y_train)\n",
    "    lin_model = Ridge(max_iter=10000, **grid_cv.best_params_)\n",
    "    cv_score = grid_cv.best_score_\n",
    "    lin_model.fit(X_val_comb, y_train)\n",
    "    y_test_pred = lin_model.predict(X_test_comb)\n",
    "    test_score = r2_score(y_test, y_test_pred)\n",
    "    test_pearson, pval = stats.pearsonr(y_test, y_test_pred)\n",
    "    return cv_score, test_score, test_pearson, pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### BNN Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv_score: -0.2119184646386674, test_score: 0.06639932000486792,\n",
      "Pearson Corr: 0.30425614450499644, pvalue: 0.0010515269643341033\n"
     ]
    }
   ],
   "source": [
    "cv_score, test_score, test_pc, pc_pval = train_gp(bnn_feat_idx, X_train_sig, X_val_sig, X_test_sig, \n",
    "                                                    y_train, y_val, y_test, num_models=1)\n",
    "\n",
    "print(f\"cv_score: {cv_score}, test_score: {test_score},\\nPearson Corr: {test_pc}, pvalue: {pc_pval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### RF Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_sig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/7_bnn_sgmcmc_tcga.ipynb Cell 46\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/7_bnn_sgmcmc_tcga.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001B[0m cv_score, test_score, test_pc, pc_pval \u001B[39m=\u001B[39m train_gp(rf_feat_idx, X_train_sig, X_val_sig, X_test_sig, \n\u001B[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/7_bnn_sgmcmc_tcga.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001B[0m                                                     y_train, y_val, y_test, num_models\u001B[39m=\u001B[39m\u001B[39m1\u001B[39m)\n\u001B[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/7_bnn_sgmcmc_tcga.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001B[0m \u001B[39mprint\u001B[39m(\u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mcv_score: \u001B[39m\u001B[39m{\u001B[39;00mcv_score\u001B[39m}\u001B[39;00m\u001B[39m, test_score: \u001B[39m\u001B[39m{\u001B[39;00mtest_score\u001B[39m}\u001B[39;00m\u001B[39m,\u001B[39m\u001B[39m\\n\u001B[39;00m\u001B[39mPearson Corr: \u001B[39m\u001B[39m{\u001B[39;00mtest_pc\u001B[39m}\u001B[39;00m\u001B[39m, pvalue: \u001B[39m\u001B[39m{\u001B[39;00mpc_pval\u001B[39m}\u001B[39;00m\u001B[39m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'X_train_sig' is not defined"
     ]
    }
   ],
   "source": [
    "cv_score, test_score, test_pc, pc_pval = train_gp(rf_feat_idx, X_train_sig, X_val_sig, X_test_sig, \n",
    "                                                    y_train, y_val, y_test, num_models=1)\n",
    "\n",
    "print(f\"cv_score: {cv_score}, test_score: {test_score},\\nPearson Corr: {test_pc}, pvalue: {pc_pval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Multiple Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# seeds = [422,261,968,282,739]\n",
    "seeds = [422,261,968,282,739,573,220,413,745,775,482,442,210,423,760,57,769,920,226,196]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running seed: 422\n",
      "Num batches: 1\n",
      "RF cv_score: 0.07444513100837169, RF test_score: 0.14340958099649903\n",
      "BNN cv_score: 0.05867323303143723, BNN test_score: 0.04118898890076794\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: 0.004919826697642948, test_score: 0.13149002422509182, Pearson Corr: 0.3701427883774948, pvalue: 5.455307892371551e-05\n",
      "RF GP - cv_score: -0.30814202885863556, test_score: 0.12099939913504965, Pearson Corr: 0.3495700529689586, pvalue: 0.0001475716901833638\n",
      "Done - seed 422, Elpased time: 0:08:09.180125\n",
      "============================\n",
      "Running seed: 261\n",
      "Num batches: 1\n",
      "RF cv_score: 0.11352835269504746, RF test_score: 0.11687619049433673\n",
      "BNN cv_score: 0.045516233310539445, BNN test_score: 0.05277284450093633\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: -0.040481192025634535, test_score: 0.10632144072383776, Pearson Corr: 0.3269483778793585, pvalue: 0.00040851481955343137\n",
      "RF GP - cv_score: -0.13186501374314913, test_score: 0.08880894418216034, Pearson Corr: 0.3038347469816982, pvalue: 0.0010694095721978832\n",
      "Done - seed 261, Elpased time: 0:07:47.144215\n",
      "============================\n",
      "Running seed: 968\n",
      "Num batches: 1\n",
      "RF cv_score: 0.08061866293877851, RF test_score: 0.10485553355026744\n",
      "BNN cv_score: 0.05529170433194808, BNN test_score: 0.018350071411904145\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: -3.959822046961952, test_score: 0.04997039527878255, Pearson Corr: 0.23278661550523277, pvalue: 0.013093027487591051\n",
      "RF GP - cv_score: -2.6709908697909306, test_score: 0.019449786661359214, Pearson Corr: 0.22981323564465736, pvalue: 0.014339116664273071\n",
      "Done - seed 968, Elpased time: 0:08:01.857940\n",
      "============================\n",
      "Running seed: 282\n",
      "Num batches: 1\n",
      "RF cv_score: 0.06412780554385245, RF test_score: 0.06858229145353567\n",
      "BNN cv_score: -0.04111459017884278, BNN test_score: -0.033087202097953794\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: -15.927015364867094, test_score: 0.013091028873890131, Pearson Corr: 0.18064458554702267, pvalue: 0.05552939294349853\n",
      "RF GP - cv_score: 0.04037608759796503, test_score: -0.39282833474320134, Pearson Corr: 0.06848723510256828, pvalue: 0.47104394710520725\n",
      "Done - seed 282, Elpased time: 0:07:56.294441\n",
      "============================\n",
      "Running seed: 739\n",
      "Num batches: 1\n",
      "RF cv_score: 0.06272304101493387, RF test_score: 0.0459615143168669\n",
      "BNN cv_score: 0.03139424286033443, BNN test_score: 0.018562400167005833\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: 0.05313641958071047, test_score: -0.10486267269391192, Pearson Corr: 0.14262174052530888, pvalue: 0.13182402960523384\n",
      "RF GP - cv_score: 0.10555552703949744, test_score: -0.1086181821717862, Pearson Corr: 0.06446738347533475, pvalue: 0.4975280822784113\n",
      "Done - seed 739, Elpased time: 0:07:42.628414\n",
      "============================\n",
      "Running seed: 573\n",
      "Num batches: 1\n",
      "RF cv_score: 0.06362947161709986, RF test_score: 0.060869973694114066\n",
      "BNN cv_score: 0.040051903713499426, BNN test_score: -0.023312892544436803\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: -4.537465507907994, test_score: -0.05840963941363264, Pearson Corr: 0.04463029162101193, pvalue: 0.6387940925368549\n",
      "RF GP - cv_score: -2.388887499545341, test_score: -0.9174552828422387, Pearson Corr: 0.03490035513597817, pvalue: 0.7136319035903479\n",
      "Done - seed 573, Elpased time: 0:07:50.844192\n",
      "============================\n",
      "Running seed: 220\n",
      "Num batches: 1\n",
      "RF cv_score: 0.07050879354460526, RF test_score: 0.08106789467500286\n",
      "BNN cv_score: -0.0033150085827209352, BNN test_score: 0.01229449670796301\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: -0.035899071573029406, test_score: 0.008694496619340297, Pearson Corr: 0.16578852168378033, pvalue: 0.07927177097551644\n",
      "RF GP - cv_score: -0.1370541747403783, test_score: 0.06192518367010014, Pearson Corr: 0.25659136973206054, pvalue: 0.006081304217227457\n",
      "Done - seed 220, Elpased time: 0:07:56.236835\n",
      "============================\n",
      "Running seed: 413\n",
      "Num batches: 1\n",
      "RF cv_score: -0.013965931528387765, RF test_score: 0.05554945553616897\n",
      "BNN cv_score: -0.08194786939518606, BNN test_score: 0.038977930977185826\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: -0.03839437976603144, test_score: 0.04751367301819032, Pearson Corr: 0.24240856741941702, pvalue: 0.009684605368778378\n",
      "RF GP - cv_score: -0.04410396484186711, test_score: -0.08714239180116179, Pearson Corr: 0.09509856483974538, pvalue: 0.31637036011180075\n",
      "Done - seed 413, Elpased time: 0:07:43.662170\n",
      "============================\n",
      "Running seed: 745\n",
      "Num batches: 1\n",
      "RF cv_score: 0.009821579610890696, RF test_score: 0.06739017942144054\n",
      "BNN cv_score: -0.0671585513658679, BNN test_score: 0.07533650330401132\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: -0.047490396840106294, test_score: 0.06613009233692779, Pearson Corr: 0.2578536959610641, pvalue: 0.005827452585897841\n",
      "RF GP - cv_score: -1034.1398165972969, test_score: 0.004691231781215666, Pearson Corr: 0.18943345126408562, pvalue: 0.04447887713734244\n",
      "Done - seed 745, Elpased time: 0:07:54.845574\n",
      "============================\n",
      "Running seed: 775\n",
      "Num batches: 1\n",
      "RF cv_score: 0.0460849936937362, RF test_score: 0.11314647292300217\n",
      "BNN cv_score: 0.01977391501719461, BNN test_score: 0.032478331101878255\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: -0.1346381534202637, test_score: 0.12423124472817171, Pearson Corr: 0.383321233628404, pvalue: 2.7811045830634652e-05\n",
      "RF GP - cv_score: -1.1482427402963538, test_score: 0.027871592466117923, Pearson Corr: 0.19906427338280747, pvalue: 0.03453598050143973\n",
      "Done - seed 775, Elpased time: 0:07:58.815455\n",
      "============================\n",
      "Running seed: 482\n",
      "Num batches: 1\n",
      "RF cv_score: 0.1046857149931798, RF test_score: 0.05720979934077808\n",
      "BNN cv_score: 0.031091770495272453, BNN test_score: -0.019975853612485217\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: -0.07654465727790336, test_score: 0.0616125404416783, Pearson Corr: 0.26124697073158587, pvalue: 0.0051911560605864705\n",
      "RF GP - cv_score: -0.1396264856200527, test_score: -0.02693608407618653, Pearson Corr: 0.18261689599260222, pvalue: 0.052871031451895895\n",
      "Done - seed 482, Elpased time: 0:07:53.381388\n",
      "============================\n",
      "Running seed: 442\n",
      "Num batches: 1\n",
      "RF cv_score: 0.15695040945560046, RF test_score: 0.09483126907817319\n",
      "BNN cv_score: 0.08275800218283114, BNN test_score: 0.06454266467469039\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: -0.3746670070961879, test_score: 0.017230576331239034, Pearson Corr: 0.16466159991778576, pvalue: 0.08136294315762589\n",
      "RF GP - cv_score: -3.6205613440855964, test_score: -0.087822236236196, Pearson Corr: 0.12348760165738196, pvalue: 0.19254420048231563\n",
      "Done - seed 442, Elpased time: 0:08:01.668549\n",
      "============================\n",
      "Running seed: 210\n",
      "Num batches: 1\n",
      "RF cv_score: 0.05435404558690815, RF test_score: 0.07226816135327807\n",
      "BNN cv_score: 0.07013622398385388, BNN test_score: -0.023061251797950266\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: -0.44340813247352606, test_score: -7.380995019127553, Pearson Corr: 0.07458354180458722, pvalue: 0.432387296247221\n",
      "RF GP - cv_score: -0.3429390458677589, test_score: -8.532875880900654, Pearson Corr: -0.003707318130467093, pvalue: 0.9689132835849682\n",
      "Done - seed 210, Elpased time: 0:07:47.490444\n",
      "============================\n",
      "Running seed: 423\n",
      "Num batches: 1\n",
      "RF cv_score: 0.0524178178078496, RF test_score: 0.07710524298758636\n",
      "BNN cv_score: -0.006117193939978671, BNN test_score: 0.04260854362586286\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: -0.07348698278530759, test_score: 0.02781398831617299, Pearson Corr: 0.1784397484186316, pvalue: 0.058630208846503266\n",
      "RF GP - cv_score: 0.009908992850018871, test_score: -0.011480528063536388, Pearson Corr: 0.12198989838076418, pvalue: 0.19803669262726314\n",
      "Done - seed 423, Elpased time: 0:07:49.534829\n",
      "============================\n",
      "Running seed: 760\n",
      "Num batches: 1\n",
      "RF cv_score: 0.043088680070187846, RF test_score: 0.11693921873919866\n",
      "BNN cv_score: 0.03572856819185166, BNN test_score: 0.03683748894752381\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: 0.07147373077698176, test_score: 0.08550218221523964, Pearson Corr: 0.29502185711272977, pvalue: 0.0015130308442809458\n",
      "RF GP - cv_score: -0.015000509491825676, test_score: 0.09297411713477954, Pearson Corr: 0.30680838721013615, pvalue: 0.0009489263359313395\n",
      "Done - seed 760, Elpased time: 0:07:47.494144\n",
      "============================\n",
      "Running seed: 57\n",
      "Num batches: 1\n",
      "RF cv_score: 0.11118768557173819, RF test_score: 0.03680698070982413\n",
      "BNN cv_score: 0.01686260908723447, BNN test_score: 0.04219200549476709\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: 0.023172488809779936, test_score: -0.006355181965487011, Pearson Corr: 0.14830407271820792, pvalue: 0.11696341818934249\n",
      "RF GP - cv_score: -1.107461458440548, test_score: -0.05287750853000861, Pearson Corr: 0.10884307356003825, pvalue: 0.25114765403546147\n",
      "Done - seed 57, Elpased time: 0:07:45.551295\n",
      "============================\n",
      "Running seed: 769\n",
      "Num batches: 1\n",
      "RF cv_score: 0.0005598381344148473, RF test_score: 0.10132123644332636\n",
      "BNN cv_score: 0.0002053334575040644, BNN test_score: 0.06636826808516783\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: -1.054888088752475, test_score: -0.2479218214940715, Pearson Corr: 0.08878383580997944, pvalue: 0.3497162174503255\n",
      "RF GP - cv_score: -0.34728240149183864, test_score: -0.17902828017963635, Pearson Corr: 0.18895586329466116, pvalue: 0.04502831517927888\n",
      "Done - seed 769, Elpased time: 0:07:48.348070\n",
      "============================\n",
      "Running seed: 920\n",
      "Num batches: 1\n",
      "RF cv_score: 0.11443924648472914, RF test_score: 0.06098589022667189\n",
      "BNN cv_score: 0.05149941895340704, BNN test_score: 0.012229509938924799\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: -0.058385580168268204, test_score: 0.001734484220534016, Pearson Corr: 0.15309320225730377, pvalue: 0.10547592746574334\n",
      "RF GP - cv_score: -0.05659875072810052, test_score: 0.06095944362573791, Pearson Corr: 0.2549828728693919, pvalue: 0.006418979145558419\n",
      "Done - seed 920, Elpased time: 0:07:48.486713\n",
      "============================\n",
      "Running seed: 226\n",
      "Num batches: 1\n",
      "RF cv_score: 0.09033354639467428, RF test_score: 0.04811780367102525\n",
      "BNN cv_score: 0.00650071513548911, BNN test_score: 0.05517067763250316\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: 0.02087581481757379, test_score: -0.00035739573804827174, Pearson Corr: 0.16602920060044618, pvalue: 0.07883079114249172\n",
      "RF GP - cv_score: -0.9596652279578939, test_score: -0.043219587456797104, Pearson Corr: 0.13886442290075665, pvalue: 0.14241601492201975\n",
      "Done - seed 226, Elpased time: 0:07:45.469580\n",
      "============================\n",
      "Running seed: 196\n",
      "Num batches: 1\n",
      "RF cv_score: 0.11889145380000399, RF test_score: 0.08562619791649329\n",
      "BNN cv_score: 0.04199673096773604, BNN test_score: 0.021843840040847162\n",
      "=================== GP Train ======================\n",
      "BNN GP - cv_score: -0.03901518853809505, test_score: 0.10618997093955951, Pearson Corr: 0.3328742078804303, pvalue: 0.0003152203388695323\n",
      "RF GP - cv_score: -70.3335158487359, test_score: -1.2549657975419088, Pearson Corr: -0.007162041626902115, pvalue: 0.9399855667978672\n",
      "Done - seed 196, Elpased time: 0:07:45.405157\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle \n",
    "from datetime import timedelta\n",
    "import time\n",
    "\n",
    "save_dir = f\"{data_dir}/exp_data_5/cancer/ccle\"\n",
    "\n",
    "bnn_exp_dict = {\"seed\": [], \"model\": [], \"cv_score\": [], \"test_score\": []}\n",
    "bnn_gp_dict = {\"seed\": [], \"model\": [], \"num_feats\": [], \"n_models\": [], \"cv_score\": [], \"test_score\": [], \"test_pearson\": [], \"pvalue\": []}\n",
    "\n",
    "num_feats = 20 # TODO this should be selected by CV\n",
    "n_models = 5 # TODO this should be selected by CV\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"Running seed: {seed}\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    ## TODO Wrap this in a function\n",
    "    X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X, y, random_state=seed, shuffle=True, test_size=0.2)\n",
    "    \n",
    "    train_df, test_df = pd.concat([X_train_df, y_train_df], axis=1),\\\n",
    "                                     pd.concat([X_test_df, y_test_df], axis=1)\n",
    "\n",
    "\n",
    "    train_scaler = StandardScaler().fit(train_df)\n",
    "    train_scaled = train_scaler.transform(train_df)\n",
    "\n",
    "    test_scaler = StandardScaler().fit(test_df)\n",
    "    test_scaled = test_scaler.transform(test_df)\n",
    "\n",
    "    X_train_df, y_train_df = pd.DataFrame(train_scaled[:,:-1], columns=X_train_df.columns), pd.Series(train_scaled[:,-1])\n",
    "    X_test_df, y_test_df = pd.DataFrame(test_scaled[:,:-1], columns=X_test_df.columns), pd.Series(test_scaled[:,-1])\n",
    "\n",
    "    X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X_train_df, y_train_df, random_state=seed, \n",
    "                                                        shuffle=True, test_size=0.3)\n",
    "\n",
    "    sig_genes_idx = calculate_sig_genes(X_train_df, y_train_df)\n",
    "    sig_genes_idx = sig_genes_idx[:500]\n",
    "\n",
    "    target_gene_idx = [i for i, col in enumerate(X_train_df.columns) if target_gene in col]\n",
    "    X_train_sig_df = pd.concat([X_train_df.iloc[:,sig_genes_idx], X_train_df.iloc[:,target_gene_idx]], axis=1)\n",
    "    X_val_sig_df = pd.concat([X_val_df.iloc[:,sig_genes_idx], X_val_df.iloc[:,target_gene_idx]], axis=1)\n",
    "    X_test_sig_df = pd.concat([X_test_df.iloc[:,sig_genes_idx], X_test_df.iloc[:,target_gene_idx]], axis=1)\n",
    "\n",
    "    gene_names = [col.split(\"(\")[0].strip() for col in X_train_sig_df.columns]\n",
    "   \n",
    "    J = build_network_string(gene_names, string_ppi)\n",
    "\n",
    "    X_train_sig, X_val_sig, X_test_sig = X_train_sig_df.to_numpy(), X_val_sig_df.to_numpy(), X_test_sig_df.to_numpy()\n",
    "    y_train, y_test, y_val = y_train_df.to_numpy(), y_test_df.to_numpy(), y_val_df.to_numpy()\n",
    "    X_train_sig, X_test_sig, y_train, y_test = jax.device_put(X_train_sig), jax.device_put(X_test_sig), \\\n",
    "                                    jax.device_put(y_train), jax.device_put(y_test)\n",
    "\n",
    "    param_grid = {\n",
    "        'bootstrap': [True],\n",
    "        'max_depth': [80, 90, 100, 110],\n",
    "        'max_features': [2, 3],\n",
    "        'min_samples_leaf': [3, 4, 5],\n",
    "        'min_samples_split': [8, 10, 12],\n",
    "        'n_estimators': [100, 200, 300, 1000]\n",
    "    }\n",
    "\n",
    "    rf_reg = RandomForestRegressor(random_state=seed, max_samples=0.9)\n",
    "    grid_cv = GridSearchCV(estimator = rf_reg, param_grid = param_grid, \n",
    "                            cv = cv, n_jobs = -1, verbose = 0, scoring=\"explained_variance\").fit(X_train_sig, y_train)\n",
    "\n",
    "    rf_reg = RandomForestRegressor(random_state=seed, max_samples=0.9, **grid_cv.best_params_)\n",
    "    rf_reg.fit(X_train_sig, y_train)\n",
    "    rf_val_score = r2_score(y_val, rf_reg.predict(X_val_sig))\n",
    "    rf_test_score = r2_score(y_test,  rf_reg.predict(X_test_sig))\n",
    "\n",
    "    # config, bnn_cv_score, _ = optimize_hyper_parameters(seed, X_train_sig, y_train, J, total_time=180)\n",
    "    # config = pickle.load(open(f\"{save_dir_100}/bnn_params_s_{seed}.pickle\", \"rb\"))\n",
    "\n",
    "    params = {\"disc_lr\":config[\"disc_lr\"], \"contin_lr\": config[\"contin_lr\"], \"batch_size\": X_train_sig.shape[0],\n",
    "                    \"mu\": config[\"mu\"], \"eta\": config[\"eta\"], \"temp\": config[\"temp\"],\n",
    "                    \"sigma\": config[\"sigma\"], \"beta\": config[\"beta\"], \"num_cycles\": config[\"num_cycles\"]}\n",
    "\n",
    "    mixed_sgmcmc = MixedSGMCMC(seed=seed, n_samples=1000, n_warmup=0, lr_schedule=\"cyclical\",\n",
    "                            layer_dims=config[\"layer_dim\"], classifier=False ,**params)\n",
    "\n",
    "    mixed_sgmcmc.fit(X_train_sig, y_train, activation_fns=config[\"activation_fns\"] , J=J)\n",
    "\n",
    "    bnn_val_score = mixed_sgmcmc.score(X_val_sig, y_val)\n",
    "    bnn_test_score = mixed_sgmcmc.score(X_test_sig, y_test)\n",
    "\n",
    "    dropout_loss = get_feats_dropout_loss(mixed_sgmcmc, X_train_sig, y_train)\n",
    "    dropout_loss.to_csv(f\"{save_dir}/drop_out_loss_s_{seed}.csv\", index=False)\n",
    "\n",
    "\n",
    "    bnn_exp_dict[\"seed\"].append(seed)\n",
    "    bnn_exp_dict[\"model\"].append(\"RF\")\n",
    "    bnn_exp_dict[\"cv_score\"].append(rf_val_score)\n",
    "    bnn_exp_dict[\"test_score\"].append(rf_test_score)\n",
    "\n",
    "    bnn_exp_dict[\"seed\"].append(seed)\n",
    "    bnn_exp_dict[\"model\"].append(\"BNN\")\n",
    "    bnn_exp_dict[\"cv_score\"].append(bnn_val_score)\n",
    "    bnn_exp_dict[\"test_score\"].append(bnn_test_score)\n",
    "    \n",
    "\n",
    "    \n",
    "    print(f\"RF cv_score: {rf_val_score}, RF test_score: {rf_test_score}\")\n",
    "    print(f\"BNN cv_score: {bnn_val_score}, BNN test_score: {bnn_test_score}\")\n",
    "    print(\"=================== GP Train ======================\")\n",
    "\n",
    "    bnn_feat_idx = dropout_loss[\"feats_idx\"][:num_feats].to_list()\n",
    "    rf_feat_idx = np.argsort(rf_reg.feature_importances_)[::-1][:num_feats]\n",
    "    bnn_gp_cv_score, bnn_gp_test_score, bnn_gp_test_pc, bnn_gp_pc_pval = train_gp(bnn_feat_idx, X_train_sig, X_val_sig, X_test_sig, \n",
    "                                                                                    y_train, y_val, y_test, num_models=n_models)\n",
    "\n",
    "    rf_gp_cv_score, rf_gp_test_score, rf_gp_test_pc, rf_gp_pc_pval = train_gp(rf_feat_idx, X_train_sig, X_val_sig, X_test_sig, \n",
    "                                                                                    y_train, y_val, y_test, num_models=n_models)\n",
    "    bnn_gp_dict[\"seed\"].append(seed)\n",
    "    bnn_gp_dict[\"model\"].append(\"RF\")\n",
    "    bnn_gp_dict[\"num_feats\"].append(num_feats)\n",
    "    bnn_gp_dict[\"n_models\"].append(n_models)\n",
    "    bnn_gp_dict[\"cv_score\"].append(rf_gp_cv_score)\n",
    "    bnn_gp_dict[\"test_score\"].append(rf_gp_test_score)\n",
    "    bnn_gp_dict[\"test_pearson\"].append(rf_gp_test_pc)\n",
    "    bnn_gp_dict[\"pvalue\"].append(rf_gp_pc_pval)\n",
    "\n",
    "    bnn_gp_dict[\"seed\"].append(seed)\n",
    "    bnn_gp_dict[\"model\"].append(\"BNN\")\n",
    "    bnn_gp_dict[\"num_feats\"].append(num_feats)\n",
    "    bnn_gp_dict[\"n_models\"].append(n_models)\n",
    "    bnn_gp_dict[\"cv_score\"].append(bnn_gp_cv_score)\n",
    "    bnn_gp_dict[\"test_score\"].append(bnn_gp_test_score)\n",
    "    bnn_gp_dict[\"test_pearson\"].append(bnn_gp_test_pc)\n",
    "    bnn_gp_dict[\"pvalue\"].append(bnn_gp_pc_pval)\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed = timedelta(seconds=(end - start))\n",
    "\n",
    "    print(f\"BNN GP - cv_score: {bnn_gp_cv_score}, test_score: {bnn_gp_test_score}, Pearson Corr: {bnn_gp_test_pc}, pvalue: {bnn_gp_pc_pval}\")\n",
    "    print(f\"RF GP - cv_score: {rf_gp_cv_score}, test_score: {rf_gp_test_score}, Pearson Corr: {rf_gp_test_pc}, pvalue: {rf_gp_pc_pval}\")\n",
    "    print(f\"Done - seed {seed}, Elpased time: {elapsed}\")\n",
    "    print(\"============================\")\n",
    "\n",
    "bnn_exp_df = pd.DataFrame(bnn_exp_dict)\n",
    "bnn_exp_df.to_csv(f\"{save_dir}/res_bnn_exp_summary.csv\", index=False)\n",
    "bnn_gp_df = pd.DataFrame(bnn_gp_dict)\n",
    "bnn_gp_df.to_csv(f\"{save_dir}/res_bnn_gp_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BNN</th>\n",
       "      <td>0.019391</td>\n",
       "      <td>0.026616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.070922</td>\n",
       "      <td>0.080446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cv_score  test_score\n",
       "model                      \n",
       "BNN    0.019391    0.026616\n",
       "RF     0.070922    0.080446"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = f\"{data_dir}/exp_data_5/cancer/ccle\"\n",
    "bnn_exp_df = pd.read_csv(f\"{save_dir}/res_bnn_exp_summary.csv\")\n",
    "bnn_exp_df.groupby([\"model\"])[[\"cv_score\", \"test_score\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_pearson</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BNN</th>\n",
       "      <td>-1.333401</td>\n",
       "      <td>-0.347569</td>\n",
       "      <td>0.210509</td>\n",
       "      <td>0.108245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>-55.886796</td>\n",
       "      <td>-0.560879</td>\n",
       "      <td>0.160347</td>\n",
       "      <td>0.244877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cv_score  test_score  test_pearson    pvalue\n",
       "model                                               \n",
       "BNN    -1.333401   -0.347569      0.210509  0.108245\n",
       "RF    -55.886796   -0.560879      0.160347  0.244877"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_gp_df = pd.read_csv(f\"{save_dir}/res_bnn_gp_summary.csv\")\n",
    "bnn_gp_df.groupby([\"model\"])[\"cv_score\", \"test_score\", \"test_pearson\", \"pvalue\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>model</th>\n",
       "      <th>cv_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>422</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.074445</td>\n",
       "      <td>0.143410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>422</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.058673</td>\n",
       "      <td>0.041189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.113528</td>\n",
       "      <td>0.116876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>261</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.045516</td>\n",
       "      <td>0.052773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>968</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.080619</td>\n",
       "      <td>0.104856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>968</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.055292</td>\n",
       "      <td>0.018350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>282</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.064128</td>\n",
       "      <td>0.068582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>282</td>\n",
       "      <td>BNN</td>\n",
       "      <td>-0.041115</td>\n",
       "      <td>-0.033087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>739</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.045962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>739</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.031394</td>\n",
       "      <td>0.018562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>573</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.063629</td>\n",
       "      <td>0.060870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>573</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.040052</td>\n",
       "      <td>-0.023313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>220</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.070509</td>\n",
       "      <td>0.081068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>220</td>\n",
       "      <td>BNN</td>\n",
       "      <td>-0.003315</td>\n",
       "      <td>0.012294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>413</td>\n",
       "      <td>RF</td>\n",
       "      <td>-0.013966</td>\n",
       "      <td>0.055549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>413</td>\n",
       "      <td>BNN</td>\n",
       "      <td>-0.081948</td>\n",
       "      <td>0.038978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>745</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.009822</td>\n",
       "      <td>0.067390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>745</td>\n",
       "      <td>BNN</td>\n",
       "      <td>-0.067159</td>\n",
       "      <td>0.075337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>775</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.046085</td>\n",
       "      <td>0.113146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>775</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.019774</td>\n",
       "      <td>0.032478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>482</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.104686</td>\n",
       "      <td>0.057210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>482</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.031092</td>\n",
       "      <td>-0.019976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>442</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.156950</td>\n",
       "      <td>0.094831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>442</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.082758</td>\n",
       "      <td>0.064543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>210</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.054354</td>\n",
       "      <td>0.072268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>210</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.070136</td>\n",
       "      <td>-0.023061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>423</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.052418</td>\n",
       "      <td>0.077105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>423</td>\n",
       "      <td>BNN</td>\n",
       "      <td>-0.006117</td>\n",
       "      <td>0.042609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>760</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.043089</td>\n",
       "      <td>0.116939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>760</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.035729</td>\n",
       "      <td>0.036837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>57</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.111188</td>\n",
       "      <td>0.036807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>57</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.016863</td>\n",
       "      <td>0.042192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>769</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.101321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>769</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.066368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>920</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.114439</td>\n",
       "      <td>0.060986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>920</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.051499</td>\n",
       "      <td>0.012230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>226</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.090334</td>\n",
       "      <td>0.048118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>226</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.055171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>196</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.118891</td>\n",
       "      <td>0.085626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>196</td>\n",
       "      <td>BNN</td>\n",
       "      <td>0.041997</td>\n",
       "      <td>0.021844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seed model  cv_score  test_score\n",
       "0    422    RF  0.074445    0.143410\n",
       "1    422   BNN  0.058673    0.041189\n",
       "2    261    RF  0.113528    0.116876\n",
       "3    261   BNN  0.045516    0.052773\n",
       "4    968    RF  0.080619    0.104856\n",
       "5    968   BNN  0.055292    0.018350\n",
       "6    282    RF  0.064128    0.068582\n",
       "7    282   BNN -0.041115   -0.033087\n",
       "8    739    RF  0.062723    0.045962\n",
       "9    739   BNN  0.031394    0.018562\n",
       "10   573    RF  0.063629    0.060870\n",
       "11   573   BNN  0.040052   -0.023313\n",
       "12   220    RF  0.070509    0.081068\n",
       "13   220   BNN -0.003315    0.012294\n",
       "14   413    RF -0.013966    0.055549\n",
       "15   413   BNN -0.081948    0.038978\n",
       "16   745    RF  0.009822    0.067390\n",
       "17   745   BNN -0.067159    0.075337\n",
       "18   775    RF  0.046085    0.113146\n",
       "19   775   BNN  0.019774    0.032478\n",
       "20   482    RF  0.104686    0.057210\n",
       "21   482   BNN  0.031092   -0.019976\n",
       "22   442    RF  0.156950    0.094831\n",
       "23   442   BNN  0.082758    0.064543\n",
       "24   210    RF  0.054354    0.072268\n",
       "25   210   BNN  0.070136   -0.023061\n",
       "26   423    RF  0.052418    0.077105\n",
       "27   423   BNN -0.006117    0.042609\n",
       "28   760    RF  0.043089    0.116939\n",
       "29   760   BNN  0.035729    0.036837\n",
       "30    57    RF  0.111188    0.036807\n",
       "31    57   BNN  0.016863    0.042192\n",
       "32   769    RF  0.000560    0.101321\n",
       "33   769   BNN  0.000205    0.066368\n",
       "34   920    RF  0.114439    0.060986\n",
       "35   920   BNN  0.051499    0.012230\n",
       "36   226    RF  0.090334    0.048118\n",
       "37   226   BNN  0.006501    0.055171\n",
       "38   196    RF  0.118891    0.085626\n",
       "39   196   BNN  0.041997    0.021844"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_exp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>model</th>\n",
       "      <th>num_feats</th>\n",
       "      <th>n_models</th>\n",
       "      <th>cv_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_pearson</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>422</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.308142</td>\n",
       "      <td>0.120999</td>\n",
       "      <td>0.349570</td>\n",
       "      <td>0.000148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>422</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>0.131490</td>\n",
       "      <td>0.370143</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.131865</td>\n",
       "      <td>0.088809</td>\n",
       "      <td>0.303835</td>\n",
       "      <td>0.001069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>261</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.040481</td>\n",
       "      <td>0.106321</td>\n",
       "      <td>0.326948</td>\n",
       "      <td>0.000409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>968</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.670991</td>\n",
       "      <td>0.019450</td>\n",
       "      <td>0.229813</td>\n",
       "      <td>0.014339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>968</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.959822</td>\n",
       "      <td>0.049970</td>\n",
       "      <td>0.232787</td>\n",
       "      <td>0.013093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>282</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.040376</td>\n",
       "      <td>-0.392828</td>\n",
       "      <td>0.068487</td>\n",
       "      <td>0.471044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>282</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-15.927015</td>\n",
       "      <td>0.013091</td>\n",
       "      <td>0.180645</td>\n",
       "      <td>0.055529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>739</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.105556</td>\n",
       "      <td>-0.108618</td>\n",
       "      <td>0.064467</td>\n",
       "      <td>0.497528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>739</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.053136</td>\n",
       "      <td>-0.104863</td>\n",
       "      <td>0.142622</td>\n",
       "      <td>0.131824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>573</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.388887</td>\n",
       "      <td>-0.917455</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.713632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>573</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-4.537466</td>\n",
       "      <td>-0.058410</td>\n",
       "      <td>0.044630</td>\n",
       "      <td>0.638794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>220</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.137054</td>\n",
       "      <td>0.061925</td>\n",
       "      <td>0.256591</td>\n",
       "      <td>0.006081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>220</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.035899</td>\n",
       "      <td>0.008694</td>\n",
       "      <td>0.165789</td>\n",
       "      <td>0.079272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>413</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.044104</td>\n",
       "      <td>-0.087142</td>\n",
       "      <td>0.095099</td>\n",
       "      <td>0.316370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>413</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.038394</td>\n",
       "      <td>0.047514</td>\n",
       "      <td>0.242409</td>\n",
       "      <td>0.009685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>745</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-1034.139817</td>\n",
       "      <td>0.004691</td>\n",
       "      <td>0.189433</td>\n",
       "      <td>0.044479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>745</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.047490</td>\n",
       "      <td>0.066130</td>\n",
       "      <td>0.257854</td>\n",
       "      <td>0.005827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>775</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.148243</td>\n",
       "      <td>0.027872</td>\n",
       "      <td>0.199064</td>\n",
       "      <td>0.034536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>775</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.134638</td>\n",
       "      <td>0.124231</td>\n",
       "      <td>0.383321</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>482</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.139626</td>\n",
       "      <td>-0.026936</td>\n",
       "      <td>0.182617</td>\n",
       "      <td>0.052871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>482</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.076545</td>\n",
       "      <td>0.061613</td>\n",
       "      <td>0.261247</td>\n",
       "      <td>0.005191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>442</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.620561</td>\n",
       "      <td>-0.087822</td>\n",
       "      <td>0.123488</td>\n",
       "      <td>0.192544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>442</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.374667</td>\n",
       "      <td>0.017231</td>\n",
       "      <td>0.164662</td>\n",
       "      <td>0.081363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>210</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.342939</td>\n",
       "      <td>-8.532876</td>\n",
       "      <td>-0.003707</td>\n",
       "      <td>0.968913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>210</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.443408</td>\n",
       "      <td>-7.380995</td>\n",
       "      <td>0.074584</td>\n",
       "      <td>0.432387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>423</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.009909</td>\n",
       "      <td>-0.011481</td>\n",
       "      <td>0.121990</td>\n",
       "      <td>0.198037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>423</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.073487</td>\n",
       "      <td>0.027814</td>\n",
       "      <td>0.178440</td>\n",
       "      <td>0.058630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>760</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.015001</td>\n",
       "      <td>0.092974</td>\n",
       "      <td>0.306808</td>\n",
       "      <td>0.000949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>760</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.071474</td>\n",
       "      <td>0.085502</td>\n",
       "      <td>0.295022</td>\n",
       "      <td>0.001513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>57</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.107461</td>\n",
       "      <td>-0.052878</td>\n",
       "      <td>0.108843</td>\n",
       "      <td>0.251148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>57</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.023172</td>\n",
       "      <td>-0.006355</td>\n",
       "      <td>0.148304</td>\n",
       "      <td>0.116963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>769</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.347282</td>\n",
       "      <td>-0.179028</td>\n",
       "      <td>0.188956</td>\n",
       "      <td>0.045028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>769</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.054888</td>\n",
       "      <td>-0.247922</td>\n",
       "      <td>0.088784</td>\n",
       "      <td>0.349716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>920</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.056599</td>\n",
       "      <td>0.060959</td>\n",
       "      <td>0.254983</td>\n",
       "      <td>0.006419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>920</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.058386</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.153093</td>\n",
       "      <td>0.105476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>226</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.959665</td>\n",
       "      <td>-0.043220</td>\n",
       "      <td>0.138864</td>\n",
       "      <td>0.142416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>226</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.020876</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>0.166029</td>\n",
       "      <td>0.078831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>196</td>\n",
       "      <td>RF</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-70.333516</td>\n",
       "      <td>-1.254966</td>\n",
       "      <td>-0.007162</td>\n",
       "      <td>0.939986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>196</td>\n",
       "      <td>BNN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.039015</td>\n",
       "      <td>0.106190</td>\n",
       "      <td>0.332874</td>\n",
       "      <td>0.000315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seed model  num_feats  n_models     cv_score  test_score  test_pearson  \\\n",
       "0    422    RF         20         5    -0.308142    0.120999      0.349570   \n",
       "1    422   BNN         20         5     0.004920    0.131490      0.370143   \n",
       "2    261    RF         20         5    -0.131865    0.088809      0.303835   \n",
       "3    261   BNN         20         5    -0.040481    0.106321      0.326948   \n",
       "4    968    RF         20         5    -2.670991    0.019450      0.229813   \n",
       "5    968   BNN         20         5    -3.959822    0.049970      0.232787   \n",
       "6    282    RF         20         5     0.040376   -0.392828      0.068487   \n",
       "7    282   BNN         20         5   -15.927015    0.013091      0.180645   \n",
       "8    739    RF         20         5     0.105556   -0.108618      0.064467   \n",
       "9    739   BNN         20         5     0.053136   -0.104863      0.142622   \n",
       "10   573    RF         20         5    -2.388887   -0.917455      0.034900   \n",
       "11   573   BNN         20         5    -4.537466   -0.058410      0.044630   \n",
       "12   220    RF         20         5    -0.137054    0.061925      0.256591   \n",
       "13   220   BNN         20         5    -0.035899    0.008694      0.165789   \n",
       "14   413    RF         20         5    -0.044104   -0.087142      0.095099   \n",
       "15   413   BNN         20         5    -0.038394    0.047514      0.242409   \n",
       "16   745    RF         20         5 -1034.139817    0.004691      0.189433   \n",
       "17   745   BNN         20         5    -0.047490    0.066130      0.257854   \n",
       "18   775    RF         20         5    -1.148243    0.027872      0.199064   \n",
       "19   775   BNN         20         5    -0.134638    0.124231      0.383321   \n",
       "20   482    RF         20         5    -0.139626   -0.026936      0.182617   \n",
       "21   482   BNN         20         5    -0.076545    0.061613      0.261247   \n",
       "22   442    RF         20         5    -3.620561   -0.087822      0.123488   \n",
       "23   442   BNN         20         5    -0.374667    0.017231      0.164662   \n",
       "24   210    RF         20         5    -0.342939   -8.532876     -0.003707   \n",
       "25   210   BNN         20         5    -0.443408   -7.380995      0.074584   \n",
       "26   423    RF         20         5     0.009909   -0.011481      0.121990   \n",
       "27   423   BNN         20         5    -0.073487    0.027814      0.178440   \n",
       "28   760    RF         20         5    -0.015001    0.092974      0.306808   \n",
       "29   760   BNN         20         5     0.071474    0.085502      0.295022   \n",
       "30    57    RF         20         5    -1.107461   -0.052878      0.108843   \n",
       "31    57   BNN         20         5     0.023172   -0.006355      0.148304   \n",
       "32   769    RF         20         5    -0.347282   -0.179028      0.188956   \n",
       "33   769   BNN         20         5    -1.054888   -0.247922      0.088784   \n",
       "34   920    RF         20         5    -0.056599    0.060959      0.254983   \n",
       "35   920   BNN         20         5    -0.058386    0.001734      0.153093   \n",
       "36   226    RF         20         5    -0.959665   -0.043220      0.138864   \n",
       "37   226   BNN         20         5     0.020876   -0.000357      0.166029   \n",
       "38   196    RF         20         5   -70.333516   -1.254966     -0.007162   \n",
       "39   196   BNN         20         5    -0.039015    0.106190      0.332874   \n",
       "\n",
       "      pvalue  \n",
       "0   0.000148  \n",
       "1   0.000055  \n",
       "2   0.001069  \n",
       "3   0.000409  \n",
       "4   0.014339  \n",
       "5   0.013093  \n",
       "6   0.471044  \n",
       "7   0.055529  \n",
       "8   0.497528  \n",
       "9   0.131824  \n",
       "10  0.713632  \n",
       "11  0.638794  \n",
       "12  0.006081  \n",
       "13  0.079272  \n",
       "14  0.316370  \n",
       "15  0.009685  \n",
       "16  0.044479  \n",
       "17  0.005827  \n",
       "18  0.034536  \n",
       "19  0.000028  \n",
       "20  0.052871  \n",
       "21  0.005191  \n",
       "22  0.192544  \n",
       "23  0.081363  \n",
       "24  0.968913  \n",
       "25  0.432387  \n",
       "26  0.198037  \n",
       "27  0.058630  \n",
       "28  0.000949  \n",
       "29  0.001513  \n",
       "30  0.251148  \n",
       "31  0.116963  \n",
       "32  0.045028  \n",
       "33  0.349716  \n",
       "34  0.006419  \n",
       "35  0.105476  \n",
       "36  0.142416  \n",
       "37  0.078831  \n",
       "38  0.939986  \n",
       "39  0.000315  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_gp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from itertools import product\n",
    "\n",
    "data_dim, num_cls = 2, 3\n",
    "\n",
    "zeros_mat = torch.zeros((1, data_dim, num_cls)) \n",
    "\n",
    "classes = [i for i in range(num_cls)]\n",
    "probs = []\n",
    "combos = list(product(classes, repeat=data_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9672, 0.6902, 0.1459],\n",
       "         [0.0576, 0.8922, 0.0325]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_mat = torch.rand_like(zeros_mat)\n",
    "rand_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9672, 0.6902],\n",
       "         [0.0576, 0.8922]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_mat[:, :, combos[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[1, 2],\n",
       "             [1, 1]], dtype=int32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = tfd.Categorical(probs=[[0.1, 0.5, 0.4], [0.1, 0.5, 0.4]])\n",
    "x_cur  = dist.sample(seed=rng_key, sample_shape=(2,))\n",
    "x_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 3)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cur_oh = jax.nn.one_hot(x_cur, 3)\n",
    "x_cur_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[ 0.3708746 , -1.1752596 , -0.51433975],\n",
       "              [ 1.9475722 , -1.298365  , -0.61030054]],\n",
       "\n",
       "             [[ 0.5575748 ,  0.10283418, -0.8314232 ],\n",
       "              [-0.37285715, -0.54886764, -0.5990561 ]]], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = jax.random.normal(rng_key, x_cur_oh.shape)\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-1.1752596 , -1.298365  ],\n",
       "             [-0.51433975, -0.61030054]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_cur = grad[0, :, x_cur[0, :]]\n",
    "grad_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_cur.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[-1.1752596 , -1.1752596 ],\n",
       "              [-1.298365  , -1.298365  ]],\n",
       "\n",
       "             [[-0.51433975, -0.51433975],\n",
       "              [-0.61030054, -0.61030054]]], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.tile(grad_cur[:,:,None], (1, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.10283418, -0.54886764],\n",
       "             [ 0.10283418, -0.54886764]], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad[1, :, x_cur[1, :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([1, 1], dtype=int32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cur[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}