{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn\n"
     ]
    }
   ],
   "source": [
    "%cd /home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from deap import tools, algorithms, creator, base, gp\n",
    "import ea_utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import itertools\n",
    "%load_ext autoreload"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'cpu'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use cpu for PRNG generation for speed-up\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\") # If you change this, you have to restart the notebook\n",
    "jax.default_backend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data_dir = \"/home/xabush/code/snet/moses-incons-pen-xp/data\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   posOutcome  4111  4110  10661  131  4438  330  1109  2637  2642  ...  7634  \\\n0           0     0     0      0    1     0    1     1     1     1  ...     0   \n1           1     1     0      0    0     0    0     1     0     1  ...     0   \n2           0     0     0      0    1     0    0     1     1     1  ...     0   \n3           0     0     0      0    0     0    0     1     1     1  ...     0   \n4           1     0     0      0    0     0    1     1     1     1  ...     0   \n\n   55769  7637  7644  741  54993  79364  7791  23140  26009  \n0      0     0     1    1      0      0     1      0      0  \n1      0     0     1    1      0      0     1      0      1  \n2      0     0     1    1      0      0     1      0      0  \n3      0     0     1    0      0      0     1      0      0  \n4      0     0     1    1      0      0     1      0      0  \n\n[5 rows x 8414 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posOutcome</th>\n      <th>4111</th>\n      <th>4110</th>\n      <th>10661</th>\n      <th>131</th>\n      <th>4438</th>\n      <th>330</th>\n      <th>1109</th>\n      <th>2637</th>\n      <th>2642</th>\n      <th>...</th>\n      <th>7634</th>\n      <th>55769</th>\n      <th>7637</th>\n      <th>7644</th>\n      <th>741</th>\n      <th>54993</th>\n      <th>79364</th>\n      <th>7791</th>\n      <th>23140</th>\n      <th>26009</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 8414 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamox_df = pd.read_csv(f\"{data_dir}/tamoxBinaryEntrez.csv\")\n",
    "tamox_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "X_df, y_df = tamox_df[tamox_df.columns.difference([\"posOutcome\"])], tamox_df[\"posOutcome\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "exp_dir = f\"{data_dir}/exp_data_4/cancer/fisher\"\n",
    "\n",
    "exp_seeds = []\n",
    "with open(f\"{exp_dir}/deap/seeds_main.txt\", \"r\") as fp:\n",
    "    for line in fp.readlines():\n",
    "        exp_seeds.append(int(line.strip()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-25 22:01:28,531 [INFO], Num models: 3, EA Validation Score: 0.5707070707070707, Test Score: 0.509596796465065\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 22:01:29,895 [INFO], LR best params {'C': 0.046415888336127774}\n",
      "2022-10-25 22:01:29,910 [INFO], LR scores - cv score:  0.8845, test_score:  0.6722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                               | 1/20 [00:50<15:53, 50.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_10067/2558079440.py\", line 40, in <cell line: 17>\n",
      "    hof, val_score, test_score, train_preds, val_preds, test_preds = run_deap(rng_key, X_train_sel, X_val_sel, X_test_sel,\n",
      "  File \"/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/ea_train_cosmic.py\", line 154, in run_deap\n",
      "    _, logbook = ea_utils.eaSimple(rng_key ,pop, toolbox, cxpb, mutbp, num_gen, stats=mstats, halloffame=hof,\n",
      "  File \"/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/ea_utils.py\", line 168, in eaSimple\n",
      "    offspring = varAnd(gen_key, offspring, toolbox, cxpb, mutpb)\n",
      "  File \"/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn/ea_utils.py\", line 70, in varAnd\n",
      "    if random.uniform(mut_key) < mutpb:\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py\", line 4658, in deferring_binary_op\n",
      "    return binary_op(*args)\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/jax/_src/numpy/ufuncs.py\", line 97, in fn\n",
      "    x1, x2 =  _promote_args(numpy_fn.__name__, x1, x2)\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/jax/_src/numpy/util.py\", line 355, in _promote_args\n",
      "    return _promote_shapes(fun_name, *_promote_dtypes(*args))\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/jax/_src/numpy/util.py\", line 276, in _promote_dtypes\n",
      "    return [lax_internal._convert_element_type(x, to_dtype, weak_type) for x in args]\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/jax/_src/numpy/util.py\", line 276, in <listcomp>\n",
      "    return [lax_internal._convert_element_type(x, to_dtype, weak_type) for x in args]\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/jax/_src/lax/lax.py\", line 579, in _convert_element_type\n",
      "    return convert_element_type_p.bind(operand, new_dtype=new_dtype,\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/jax/core.py\", line 325, in bind\n",
      "    return self.bind_with_trace(find_top_trace(args), args, params)\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/jax/core.py\", line 328, in bind_with_trace\n",
      "    out = trace.process_primitive(self, map(trace.full_raise, args), params)\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/jax/core.py\", line 686, in process_primitive\n",
      "    return primitive.impl(*tracers, **params)\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/jax/_src/dispatch.py\", line 113, in apply_primitive\n",
      "    return compiled_fun(*args)\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/jax/_src/dispatch.py\", line 198, in <lambda>\n",
      "    return lambda *args, **kw: compiled(*args, **kw)[0]\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/jax/_src/dispatch.py\", line 842, in _execute_compiled\n",
      "    return result_handler(env, out_bufs)\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/jax/_src/dispatch.py\", line 703, in __call__\n",
      "    return tuple(h(env, *bs) for h, bs in zip(self.handlers, lists_of_bufs))\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/jax/_src/dispatch.py\", line 703, in <genexpr>\n",
      "    return tuple(h(env, *bs) for h, bs in zip(self.handlers, lists_of_bufs))\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/jax/_src/dispatch.py\", line 737, in <lambda>\n",
      "    handler = lambda _, b: maybe_create_array_from_da(b, aval, sticky_device)\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/jax/_src/dispatch.py\", line 706, in maybe_create_array_from_da\n",
      "    def maybe_create_array_from_da(buf, aval, device):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1993, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/xabush/miniconda3/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from ea_train_cosmic import run_deap, run_logistc_regression\n",
    "from nn_util import setup_logger\n",
    "import datetime\n",
    "import warnings\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bnn_deap_dict = {\"seed\": [], \"classifier\": [], \"num_feats\": [], \"top_5_feats\": [] ,\"cv_score\": [], \"test_score\": []}\n",
    "\n",
    "cxpb, mutpb = 0.5, 0.3\n",
    "n_gen = 1000\n",
    "\n",
    "num_feats = 70\n",
    "\n",
    "for seed in tqdm(exp_seeds):\n",
    "    # print(f\"Running seed {seed}\")\n",
    "    logger = setup_logger(None, seed)\n",
    "    start_time = time.time()\n",
    "\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    cv = StratifiedKFold(random_state=seed, shuffle=True, n_splits=5)\n",
    "\n",
    "    idx_sig = np.load(f\"{exp_dir}/fisher_idx_sig_{seed}.npy\")\n",
    "    selected_idx = np.load(f\"{exp_dir}/bnn_sel_idx_s_{seed}_n_{num_feats}.npy\")\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, random_state=seed, shuffle=True, stratify=y_df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, random_state=seed, shuffle=True,\n",
    "                                                        stratify=y_df, test_size=0.3)\n",
    "\n",
    "    X_train_sig, X_test_sig = X_train.iloc[:, idx_sig], X_test.iloc[:,idx_sig]\n",
    "    X_train_sel, X_test_sel = X_train_sig.iloc[:,selected_idx].to_numpy(), X_test_sig.iloc[:,selected_idx].to_numpy()\n",
    "    y_train, y_test = y_train.to_numpy(), y_test.to_numpy()\n",
    "\n",
    "    X_train_sel, X_val_sel, y_train, y_val = train_test_split(X_train_sel, y_train, random_state=seed, shuffle=True,\n",
    "                                                              stratify=y_train, test_size=0.2)\n",
    "\n",
    "\n",
    "    hof, val_score, test_score, train_preds, val_preds, test_preds = run_deap(rng_key, X_train_sel, X_val_sel, X_test_sel,\n",
    "                                                                              y_train, y_val, y_test, cxpb, mutpb, n_gen, logger)\n",
    "\n",
    "    X_train_sel_ea = np.concatenate([X_train_sel, train_preds], axis=1)\n",
    "    X_val_sel_ea = np.concatenate([X_val_sel, val_preds], axis=1)\n",
    "    X_test_sel_ea = np.concatenate([X_test_sel, test_preds], axis=1)\n",
    "\n",
    "    clf_log, log_best_params, log_cv_score, log_test_score = run_logistc_regression(X_train_sel_ea, X_val_sel_ea, X_test_sel_ea,\n",
    "                                                                                    y_train, y_val ,y_test, cv, logger)\n",
    "\n",
    "    # Check the top 5 feats acc. Logistic Regression classifier\n",
    "    top_5_feats = \",\".join([str(idx) for idx in np.argsort(np.abs(clf_log.coef_[0]))[::-1][:5]])\n",
    "\n",
    "    bnn_deap_dict[\"classifier\"].append(\"DEAP\")\n",
    "    bnn_deap_dict[\"seed\"].append(seed)\n",
    "    bnn_deap_dict[\"num_feats\"].append(num_feats)\n",
    "    bnn_deap_dict[\"cv_score\"].append(val_score)\n",
    "    bnn_deap_dict[\"test_score\"].append(test_score)\n",
    "    bnn_deap_dict[\"top_5_feats\"].append(\"-\")\n",
    "\n",
    "    bnn_deap_dict[\"classifier\"].append(\"DEAP + LR\")\n",
    "    bnn_deap_dict[\"seed\"].append(seed)\n",
    "    bnn_deap_dict[\"num_feats\"].append(num_feats + len(hof))\n",
    "    bnn_deap_dict[\"cv_score\"].append(log_cv_score)\n",
    "    bnn_deap_dict[\"test_score\"].append(log_test_score)\n",
    "    bnn_deap_dict[\"top_5_feats\"].append(top_5_feats)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    elapsed = datetime.timedelta(seconds=(end_time - start_time))\n",
    "    # logger.info(f\"Done for seed {seed}. Time elapsed - {elapsed}\")\n",
    "\n",
    "bnn_deap_df = pd.DataFrame(bnn_deap_dict)\n",
    "bnn_deap_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "            num_feats  cv_score  test_score\nclassifier                                 \nDEAP            70.00  0.627825    0.582743\nDEAP + LR       74.35  0.845297    0.692495",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num_feats</th>\n      <th>cv_score</th>\n      <th>test_score</th>\n    </tr>\n    <tr>\n      <th>classifier</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>DEAP</th>\n      <td>70.00</td>\n      <td>0.627825</td>\n      <td>0.582743</td>\n    </tr>\n    <tr>\n      <th>DEAP + LR</th>\n      <td>74.35</td>\n      <td>0.845297</td>\n      <td>0.692495</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_deap_df.groupby([\"classifier\"])[[\"num_feats\" ,\"cv_score\", \"test_score\"]].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "            num_feats  cv_score  test_score\nclassifier                                 \nDEAP         0.000000  0.072141    0.049923\nDEAP + LR    1.136708  0.040808    0.034661",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num_feats</th>\n      <th>cv_score</th>\n      <th>test_score</th>\n    </tr>\n    <tr>\n      <th>classifier</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>DEAP</th>\n      <td>0.000000</td>\n      <td>0.072141</td>\n      <td>0.049923</td>\n    </tr>\n    <tr>\n      <th>DEAP + LR</th>\n      <td>1.136708</td>\n      <td>0.040808</td>\n      <td>0.034661</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_deap_df.groupby([\"classifier\"])[[\"num_feats\" ,\"cv_score\", \"test_score\"]].std()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "bnn_feat_len_svm_df = pd.read_csv(f\"{exp_dir}/moses/bnn_fisher_feat_len_svm.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running seed 422\n",
      "2022-10-25 18:32:30,558 [INFO], Num models: 7, EA Validation Score: 0.6571969696969697, Test Score: 0.5424606462303232\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:32:31,334 [INFO], LR best params {'C': 0.1}\n",
      "2022-10-25 18:32:31,343 [INFO], LR scores - cv score:  0.8592, test_score:  0.6613\n",
      "2022-10-25 18:32:31,345 [INFO], Done for seed 422. Time elapsed - 0:00:17.515286\n",
      "Running seed 261\n",
      "2022-10-25 18:33:20,876 [INFO], Num models: 3, EA Validation Score: 0.5804924242424242, Test Score: 0.5808478320905827\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:33:21,806 [INFO], LR best params {'C': 0.021544346900318832}\n",
      "2022-10-25 18:33:21,814 [INFO], LR scores - cv score:  0.9072, test_score:  0.7117\n",
      "2022-10-25 18:33:21,816 [INFO], Done for seed 261. Time elapsed - 0:00:50.466828\n",
      "Running seed 968\n",
      "2022-10-25 18:33:40,779 [INFO], Num models: 6, EA Validation Score: 0.7111742424242424, Test Score: 0.5796741231703949\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:33:41,666 [INFO], LR best params {'C': 0.021544346900318832}\n",
      "2022-10-25 18:33:41,675 [INFO], LR scores - cv score:  0.8617, test_score:  0.6977\n",
      "2022-10-25 18:33:41,677 [INFO], Done for seed 968. Time elapsed - 0:00:19.859030\n",
      "Running seed 282\n",
      "2022-10-25 18:34:03,471 [INFO], Num models: 4, EA Validation Score: 0.6944444444444444, Test Score: 0.658036454018227\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:34:04,097 [INFO], LR best params {'C': 0.021544346900318832}\n",
      "2022-10-25 18:34:04,104 [INFO], LR scores - cv score:  0.8908, test_score:  0.7085\n",
      "2022-10-25 18:34:04,106 [INFO], Done for seed 282. Time elapsed - 0:00:22.426477\n",
      "Running seed 739\n",
      "2022-10-25 18:34:21,097 [INFO], Num models: 3, EA Validation Score: 0.5568181818181819, Test Score: 0.5457056061861364\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:34:21,850 [INFO], LR best params {'C': 0.046415888336127774}\n",
      "2022-10-25 18:34:21,860 [INFO], LR scores - cv score:  0.8914, test_score:  0.6940\n",
      "2022-10-25 18:34:21,863 [INFO], Done for seed 739. Time elapsed - 0:00:17.755329\n",
      "Running seed 573\n",
      "2022-10-25 18:37:20,349 [INFO], Num models: 4, EA Validation Score: 0.523989898989899, Test Score: 0.5493648163490749\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:37:21,155 [INFO], LR best params {'C': 0.01}\n",
      "2022-10-25 18:37:21,166 [INFO], LR scores - cv score:  0.8194, test_score:  0.7266\n",
      "2022-10-25 18:37:21,169 [INFO], Done for seed 573. Time elapsed - 0:02:59.301657\n",
      "Running seed 220\n",
      "2022-10-25 18:37:42,832 [INFO], Num models: 4, EA Validation Score: 0.6448863636363636, Test Score: 0.5545429439381386\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:37:43,669 [INFO], LR best params {'C': 0.046415888336127774}\n",
      "2022-10-25 18:37:43,682 [INFO], LR scores - cv score:  0.8851, test_score:  0.7049\n",
      "2022-10-25 18:37:43,685 [INFO], Done for seed 220. Time elapsed - 0:00:22.512175\n",
      "Running seed 413\n",
      "2022-10-25 18:38:23,269 [INFO], Num models: 3, EA Validation Score: 0.5839646464646465, Test Score: 0.5489505661419498\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:38:27,729 [INFO], LR best params {'C': 0.046415888336127774}\n",
      "2022-10-25 18:38:27,741 [INFO], LR scores - cv score:  0.8384, test_score:  0.6696\n",
      "2022-10-25 18:38:27,743 [INFO], Done for seed 413. Time elapsed - 0:00:44.055437\n",
      "Running seed 745\n",
      "2022-10-25 18:38:49,200 [INFO], Num models: 4, EA Validation Score: 0.6439393939393939, Test Score: 0.5854045843689588\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:38:50,687 [INFO], LR best params {'C': 0.1}\n",
      "2022-10-25 18:38:50,703 [INFO], LR scores - cv score:  0.8920, test_score:  0.6837\n",
      "2022-10-25 18:38:50,705 [INFO], Done for seed 745. Time elapsed - 0:00:22.960244\n",
      "Running seed 775\n",
      "2022-10-25 18:39:15,074 [INFO], Num models: 5, EA Validation Score: 0.6205808080808081, Test Score: 0.5467412317039492\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:39:15,728 [INFO], LR best params {'C': 0.021544346900318832}\n",
      "2022-10-25 18:39:15,743 [INFO], LR scores - cv score:  0.8169, test_score:  0.6622\n",
      "2022-10-25 18:39:15,745 [INFO], Done for seed 775. Time elapsed - 0:00:25.038188\n",
      "Running seed 482\n",
      "2022-10-25 18:39:43,108 [INFO], Num models: 4, EA Validation Score: 0.5473484848484849, Test Score: 0.6204087268710301\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:39:44,538 [INFO], LR best params {'C': 0.021544346900318832}\n",
      "2022-10-25 18:39:44,547 [INFO], LR scores - cv score:  0.8668, test_score:  0.7425\n",
      "2022-10-25 18:39:44,549 [INFO], Done for seed 482. Time elapsed - 0:00:28.802431\n",
      "Running seed 442\n",
      "2022-10-25 18:40:52,375 [INFO], Num models: 3, EA Validation Score: 0.5912247474747475, Test Score: 0.5682822424744546\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:40:53,378 [INFO], LR best params {'C': 0.046415888336127774}\n",
      "2022-10-25 18:40:53,389 [INFO], LR scores - cv score:  0.8523, test_score:  0.6574\n",
      "2022-10-25 18:40:53,390 [INFO], Done for seed 442. Time elapsed - 0:01:08.839992\n",
      "Running seed 210\n",
      "2022-10-25 18:43:00,286 [INFO], Num models: 4, EA Validation Score: 0.5643939393939394, Test Score: 0.5288594310963822\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:43:00,687 [INFO], LR best params {'C': 0.1}\n",
      "2022-10-25 18:43:00,871 [INFO], LR scores - cv score:  0.7746, test_score:  0.6838\n",
      "2022-10-25 18:43:00,876 [INFO], Done for seed 210. Time elapsed - 0:02:07.483824\n",
      "Running seed 423\n",
      "2022-10-25 18:45:53,645 [INFO], Num models: 4, EA Validation Score: 0.6354166666666666, Test Score: 0.6230323115161558\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:45:54,923 [INFO], LR best params {'C': 0.046415888336127774}\n",
      "2022-10-25 18:45:54,933 [INFO], LR scores - cv score:  0.8807, test_score:  0.7595\n",
      "2022-10-25 18:45:54,935 [INFO], Done for seed 423. Time elapsed - 0:02:54.056978\n",
      "Running seed 760\n",
      "2022-10-25 18:48:45,343 [INFO], Num models: 3, EA Validation Score: 0.6701388888888888, Test Score: 0.5669704501518917\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:48:46,026 [INFO], LR best params {'C': 0.01}\n",
      "2022-10-25 18:48:46,034 [INFO], LR scores - cv score:  0.8119, test_score:  0.6792\n",
      "2022-10-25 18:48:46,036 [INFO], Done for seed 760. Time elapsed - 0:02:51.099646\n",
      "Running seed 57\n",
      "2022-10-25 18:49:07,656 [INFO], Num models: 2, EA Validation Score: 0.5378787878787878, Test Score: 0.5299640983153826\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:49:08,290 [INFO], LR best params {'C': 0.021544346900318832}\n",
      "2022-10-25 18:49:08,299 [INFO], LR scores - cv score:  0.8807, test_score:  0.7567\n",
      "2022-10-25 18:49:08,301 [INFO], Done for seed 57. Time elapsed - 0:00:22.262456\n",
      "Running seed 769\n",
      "2022-10-25 18:49:39,516 [INFO], Num models: 4, EA Validation Score: 0.75, Test Score: 0.5898922949461475\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:49:39,990 [INFO], LR best params {'C': 0.021544346900318832}\n",
      "2022-10-25 18:49:40,000 [INFO], LR scores - cv score:  0.9299, test_score:  0.7067\n",
      "2022-10-25 18:49:40,002 [INFO], Done for seed 769. Time elapsed - 0:00:31.700030\n",
      "Running seed 920\n",
      "2022-10-25 18:50:52,899 [INFO], Num models: 5, EA Validation Score: 0.6483585858585859, Test Score: 0.5342446837890086\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:50:53,390 [INFO], LR best params {'C': 0.1}\n",
      "2022-10-25 18:50:53,401 [INFO], LR scores - cv score:  0.8542, test_score:  0.7099\n",
      "2022-10-25 18:50:53,403 [INFO], Done for seed 920. Time elapsed - 0:01:13.398667\n",
      "Running seed 226\n",
      "2022-10-25 18:51:11,226 [INFO], Num models: 4, EA Validation Score: 0.6470959595959596, Test Score: 0.571803369235018\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:51:11,932 [INFO], LR best params {'C': 0.021544346900318832}\n",
      "2022-10-25 18:51:11,941 [INFO], LR scores - cv score:  0.8232, test_score:  0.6542\n",
      "2022-10-25 18:51:11,943 [INFO], Done for seed 226. Time elapsed - 0:00:18.538419\n",
      "Running seed 196\n",
      "2022-10-25 18:52:36,152 [INFO], Num models: 5, EA Validation Score: 0.6934974747474748, Test Score: 0.6717067108533554\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "2022-10-25 18:52:36,722 [INFO], LR best params {'C': 0.1}\n",
      "2022-10-25 18:52:36,731 [INFO], LR scores - cv score:  0.8580, test_score:  0.6723\n",
      "2022-10-25 18:52:36,733 [INFO], Done for seed 196. Time elapsed - 0:01:24.788577\n"
     ]
    },
    {
     "data": {
      "text/plain": "    seed classifier  num_feats      top_5_feats  cv_score  test_score  kernel\n0    422       DEAP         40                -  0.657197    0.542461  linear\n1    422  DEAP + LR         47     6,37,32,19,1  0.859217    0.661281  linear\n2    261       DEAP         90                -  0.580492    0.580848    poly\n3    261  DEAP + LR         93    88,15,48,7,44  0.907197    0.711682    poly\n4    968       DEAP        100                -  0.711174    0.579674  linear\n5    968  DEAP + LR        106  3,101,22,100,58  0.861742    0.697735  linear\n6    282       DEAP         80                -  0.694444    0.658036    poly\n7    282  DEAP + LR         84   29,78,77,14,59  0.890783    0.708506    poly\n8    739       DEAP        100                -  0.556818    0.545706     rbf\n9    739  DEAP + LR        103   72,2,36,16,102  0.891414    0.694007     rbf\n10   573       DEAP         90                -  0.523990    0.549365  linear\n11   573  DEAP + LR         94   90,78,67,91,92  0.819444    0.726595  linear\n12   220       DEAP         80                -  0.644886    0.554543     rbf\n13   220  DEAP + LR         84   62,32,41,25,13  0.885101    0.704916     rbf\n14   413       DEAP         60                -  0.583965    0.548951  linear\n15   413  DEAP + LR         63     3,9,60,21,58  0.838384    0.669566  linear\n16   745       DEAP        100                -  0.643939    0.585405     rbf\n17   745  DEAP + LR        104  94,58,31,100,43  0.892045    0.683651     rbf\n18   775       DEAP        100                -  0.620581    0.546741     rbf\n19   775  DEAP + LR        105    47,8,90,87,65  0.816919    0.662248     rbf\n20   482       DEAP         90                -  0.547348    0.620409  linear\n21   482  DEAP + LR         94   87,66,32,25,55  0.866793    0.742474  linear\n22   442       DEAP        100                -  0.591225    0.568282  linear\n23   442  DEAP + LR        103  58,13,15,100,18  0.852273    0.657415  linear\n24   210       DEAP         70                -  0.564394    0.528859  linear\n25   210  DEAP + LR         74    1,36,20,10,70  0.774621    0.683789  linear\n26   423       DEAP        100                -  0.635417    0.623032  linear\n27   423  DEAP + LR        104  100,20,21,25,29  0.880682    0.759459  linear\n28   760       DEAP         80                -  0.670139    0.566970  linear\n29   760  DEAP + LR         83   71,26,67,78,65  0.811869    0.679232  linear\n30    57       DEAP        100                -  0.537879    0.529964    poly\n31    57  DEAP + LR        102     29,6,7,31,62  0.880682    0.756697    poly\n32   769       DEAP         70                -  0.750000    0.589892     rbf\n33   769  DEAP + LR         74   20,35,23,58,30  0.929924    0.706711     rbf\n34   920       DEAP         90                -  0.648359    0.534245    poly\n35   920  DEAP + LR         95   91,63,55,28,59  0.854167    0.709887    poly\n36   226       DEAP        100                -  0.647096    0.571803  linear\n37   226  DEAP + LR        104     2,3,69,91,46  0.823232    0.654239  linear\n38   196       DEAP        100                -  0.693497    0.671707     rbf\n39   196  DEAP + LR        105    2,83,92,43,19  0.857955    0.672328     rbf",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>seed</th>\n      <th>classifier</th>\n      <th>num_feats</th>\n      <th>top_5_feats</th>\n      <th>cv_score</th>\n      <th>test_score</th>\n      <th>kernel</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>422</td>\n      <td>DEAP</td>\n      <td>40</td>\n      <td>-</td>\n      <td>0.657197</td>\n      <td>0.542461</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>422</td>\n      <td>DEAP + LR</td>\n      <td>47</td>\n      <td>6,37,32,19,1</td>\n      <td>0.859217</td>\n      <td>0.661281</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>261</td>\n      <td>DEAP</td>\n      <td>90</td>\n      <td>-</td>\n      <td>0.580492</td>\n      <td>0.580848</td>\n      <td>poly</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>261</td>\n      <td>DEAP + LR</td>\n      <td>93</td>\n      <td>88,15,48,7,44</td>\n      <td>0.907197</td>\n      <td>0.711682</td>\n      <td>poly</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>968</td>\n      <td>DEAP</td>\n      <td>100</td>\n      <td>-</td>\n      <td>0.711174</td>\n      <td>0.579674</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>968</td>\n      <td>DEAP + LR</td>\n      <td>106</td>\n      <td>3,101,22,100,58</td>\n      <td>0.861742</td>\n      <td>0.697735</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>282</td>\n      <td>DEAP</td>\n      <td>80</td>\n      <td>-</td>\n      <td>0.694444</td>\n      <td>0.658036</td>\n      <td>poly</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>282</td>\n      <td>DEAP + LR</td>\n      <td>84</td>\n      <td>29,78,77,14,59</td>\n      <td>0.890783</td>\n      <td>0.708506</td>\n      <td>poly</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>739</td>\n      <td>DEAP</td>\n      <td>100</td>\n      <td>-</td>\n      <td>0.556818</td>\n      <td>0.545706</td>\n      <td>rbf</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>739</td>\n      <td>DEAP + LR</td>\n      <td>103</td>\n      <td>72,2,36,16,102</td>\n      <td>0.891414</td>\n      <td>0.694007</td>\n      <td>rbf</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>573</td>\n      <td>DEAP</td>\n      <td>90</td>\n      <td>-</td>\n      <td>0.523990</td>\n      <td>0.549365</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>573</td>\n      <td>DEAP + LR</td>\n      <td>94</td>\n      <td>90,78,67,91,92</td>\n      <td>0.819444</td>\n      <td>0.726595</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>220</td>\n      <td>DEAP</td>\n      <td>80</td>\n      <td>-</td>\n      <td>0.644886</td>\n      <td>0.554543</td>\n      <td>rbf</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>220</td>\n      <td>DEAP + LR</td>\n      <td>84</td>\n      <td>62,32,41,25,13</td>\n      <td>0.885101</td>\n      <td>0.704916</td>\n      <td>rbf</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>413</td>\n      <td>DEAP</td>\n      <td>60</td>\n      <td>-</td>\n      <td>0.583965</td>\n      <td>0.548951</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>413</td>\n      <td>DEAP + LR</td>\n      <td>63</td>\n      <td>3,9,60,21,58</td>\n      <td>0.838384</td>\n      <td>0.669566</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>745</td>\n      <td>DEAP</td>\n      <td>100</td>\n      <td>-</td>\n      <td>0.643939</td>\n      <td>0.585405</td>\n      <td>rbf</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>745</td>\n      <td>DEAP + LR</td>\n      <td>104</td>\n      <td>94,58,31,100,43</td>\n      <td>0.892045</td>\n      <td>0.683651</td>\n      <td>rbf</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>775</td>\n      <td>DEAP</td>\n      <td>100</td>\n      <td>-</td>\n      <td>0.620581</td>\n      <td>0.546741</td>\n      <td>rbf</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>775</td>\n      <td>DEAP + LR</td>\n      <td>105</td>\n      <td>47,8,90,87,65</td>\n      <td>0.816919</td>\n      <td>0.662248</td>\n      <td>rbf</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>482</td>\n      <td>DEAP</td>\n      <td>90</td>\n      <td>-</td>\n      <td>0.547348</td>\n      <td>0.620409</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>482</td>\n      <td>DEAP + LR</td>\n      <td>94</td>\n      <td>87,66,32,25,55</td>\n      <td>0.866793</td>\n      <td>0.742474</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>442</td>\n      <td>DEAP</td>\n      <td>100</td>\n      <td>-</td>\n      <td>0.591225</td>\n      <td>0.568282</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>442</td>\n      <td>DEAP + LR</td>\n      <td>103</td>\n      <td>58,13,15,100,18</td>\n      <td>0.852273</td>\n      <td>0.657415</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>210</td>\n      <td>DEAP</td>\n      <td>70</td>\n      <td>-</td>\n      <td>0.564394</td>\n      <td>0.528859</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>210</td>\n      <td>DEAP + LR</td>\n      <td>74</td>\n      <td>1,36,20,10,70</td>\n      <td>0.774621</td>\n      <td>0.683789</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>423</td>\n      <td>DEAP</td>\n      <td>100</td>\n      <td>-</td>\n      <td>0.635417</td>\n      <td>0.623032</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>423</td>\n      <td>DEAP + LR</td>\n      <td>104</td>\n      <td>100,20,21,25,29</td>\n      <td>0.880682</td>\n      <td>0.759459</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>760</td>\n      <td>DEAP</td>\n      <td>80</td>\n      <td>-</td>\n      <td>0.670139</td>\n      <td>0.566970</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>760</td>\n      <td>DEAP + LR</td>\n      <td>83</td>\n      <td>71,26,67,78,65</td>\n      <td>0.811869</td>\n      <td>0.679232</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>57</td>\n      <td>DEAP</td>\n      <td>100</td>\n      <td>-</td>\n      <td>0.537879</td>\n      <td>0.529964</td>\n      <td>poly</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>57</td>\n      <td>DEAP + LR</td>\n      <td>102</td>\n      <td>29,6,7,31,62</td>\n      <td>0.880682</td>\n      <td>0.756697</td>\n      <td>poly</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>769</td>\n      <td>DEAP</td>\n      <td>70</td>\n      <td>-</td>\n      <td>0.750000</td>\n      <td>0.589892</td>\n      <td>rbf</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>769</td>\n      <td>DEAP + LR</td>\n      <td>74</td>\n      <td>20,35,23,58,30</td>\n      <td>0.929924</td>\n      <td>0.706711</td>\n      <td>rbf</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>920</td>\n      <td>DEAP</td>\n      <td>90</td>\n      <td>-</td>\n      <td>0.648359</td>\n      <td>0.534245</td>\n      <td>poly</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>920</td>\n      <td>DEAP + LR</td>\n      <td>95</td>\n      <td>91,63,55,28,59</td>\n      <td>0.854167</td>\n      <td>0.709887</td>\n      <td>poly</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>226</td>\n      <td>DEAP</td>\n      <td>100</td>\n      <td>-</td>\n      <td>0.647096</td>\n      <td>0.571803</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>226</td>\n      <td>DEAP + LR</td>\n      <td>104</td>\n      <td>2,3,69,91,46</td>\n      <td>0.823232</td>\n      <td>0.654239</td>\n      <td>linear</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>196</td>\n      <td>DEAP</td>\n      <td>100</td>\n      <td>-</td>\n      <td>0.693497</td>\n      <td>0.671707</td>\n      <td>rbf</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>196</td>\n      <td>DEAP + LR</td>\n      <td>105</td>\n      <td>2,83,92,43,19</td>\n      <td>0.857955</td>\n      <td>0.672328</td>\n      <td>rbf</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload\n",
    "from ea_train_cosmic import run_deap, run_logistc_regression\n",
    "from nn_util import setup_logger\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bnn_feat_len_deap_dict = {\"seed\": [], \"classifier\": [], \"num_feats\": [], \"top_5_feats\": [] ,\"cv_score\": [], \"test_score\": [], \"kernel\": []}\n",
    "\n",
    "cxpb, mutpb = 0.5, 0.2\n",
    "n_gen = 1000\n",
    "\n",
    "for seed in exp_seeds:\n",
    "    print(f\"Running seed {seed}\")\n",
    "    logger = setup_logger(None, seed)\n",
    "    start_time = time.time()\n",
    "\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    cv = StratifiedKFold(random_state=seed, shuffle=True, n_splits=5)\n",
    "    gamma_means = np.load(f\"{exp_dir}/bnn_disc_mean_s_{seed}.npy\")\n",
    "    gamma_means_idx_s = np.argsort(gamma_means)[::-1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.3, random_state=seed, shuffle=True,\n",
    "                                                        stratify=y_df)\n",
    "    # X_train, X_test = X_train.to_numpy(), X_test.to_numpy()\n",
    "    # y_train, y_test = y_train.to_numpy(), y_test.to_numpy()\n",
    "    idx_sig = np.load(f\"{exp_dir}/fisher_idx_sig_{seed}.npy\")\n",
    "    X_train_sig, X_test_sig = X_train.iloc[:, idx_sig], X_test.iloc[:,idx_sig]\n",
    "\n",
    "    feat_len = bnn_feat_len_svm_df[bnn_feat_len_svm_df[\"seed\"] == seed][\"num_feats\"].iloc[0]\n",
    "    kernel = bnn_feat_len_svm_df[bnn_feat_len_svm_df[\"seed\"] == seed][\"kernel\"].iloc[0]\n",
    "    gamma_idx = gamma_means_idx_s[:feat_len]\n",
    "    X_train_sel, X_test_sel = X_train_sig.iloc[:,gamma_idx].to_numpy(), X_test_sig.iloc[:,gamma_idx].to_numpy()\n",
    "    y_train, y_test = y_train.to_numpy(), y_test.to_numpy()\n",
    "\n",
    "    X_train_sel, X_val_sel, y_train, y_val = train_test_split(X_train_sel, y_train, random_state=seed, shuffle=True,\n",
    "                                                              stratify=y_train, test_size=0.2)\n",
    "\n",
    "\n",
    "    hof, val_score, test_score, train_preds, val_preds, test_preds = run_deap(rng_key, X_train_sel, X_val_sel, X_test_sel,\n",
    "                                                                              y_train, y_val, y_test, cxpb, mutpb, n_gen, logger)\n",
    "\n",
    "    X_train_sel_ea = np.concatenate([X_train_sel, train_preds], axis=1)\n",
    "    X_val_sel_ea = np.concatenate([X_val_sel, val_preds], axis=1)\n",
    "    X_test_sel_ea = np.concatenate([X_test_sel, test_preds], axis=1)\n",
    "\n",
    "    clf_log, log_best_params, log_cv_score, log_test_score = run_logistc_regression(X_train_sel_ea, X_val_sel_ea, X_test_sel_ea,\n",
    "                                                                                    y_train, y_val ,y_test, cv, logger)\n",
    "\n",
    "    # Check the top 5 feats acc. Logistic Regression classifier\n",
    "    top_5_feats = \",\".join([str(idx) for idx in np.argsort(np.abs(clf_log.coef_[0]))[::-1][:5]])\n",
    "\n",
    "    bnn_feat_len_deap_dict[\"classifier\"].append(\"DEAP\")\n",
    "    bnn_feat_len_deap_dict[\"seed\"].append(seed)\n",
    "    bnn_feat_len_deap_dict[\"num_feats\"].append(feat_len)\n",
    "    bnn_feat_len_deap_dict[\"cv_score\"].append(val_score)\n",
    "    bnn_feat_len_deap_dict[\"test_score\"].append(test_score)\n",
    "    bnn_feat_len_deap_dict[\"kernel\"].append(kernel)\n",
    "    bnn_feat_len_deap_dict[\"top_5_feats\"].append(\"-\")\n",
    "\n",
    "    bnn_feat_len_deap_dict[\"classifier\"].append(\"DEAP + LR\")\n",
    "    bnn_feat_len_deap_dict[\"seed\"].append(seed)\n",
    "    bnn_feat_len_deap_dict[\"num_feats\"].append(feat_len + len(hof))\n",
    "    bnn_feat_len_deap_dict[\"cv_score\"].append(log_cv_score)\n",
    "    bnn_feat_len_deap_dict[\"test_score\"].append(log_test_score)\n",
    "    bnn_feat_len_deap_dict[\"kernel\"].append(kernel)\n",
    "    bnn_feat_len_deap_dict[\"top_5_feats\"].append(top_5_feats)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    elapsed = datetime.timedelta(seconds=(end_time - start_time))\n",
    "    logger.info(f\"Done for seed {seed}. Time elapsed - {elapsed}\")\n",
    "\n",
    "bnn_feat_len_deap_df = pd.DataFrame(bnn_feat_len_deap_dict)\n",
    "bnn_feat_len_deap_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "            num_feats  cv_score  test_score\nclassifier                                 \nDEAP            87.00  0.625142    0.574845\nDEAP + LR       91.05  0.859722    0.697121",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num_feats</th>\n      <th>cv_score</th>\n      <th>test_score</th>\n    </tr>\n    <tr>\n      <th>classifier</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>DEAP</th>\n      <td>87.00</td>\n      <td>0.625142</td>\n      <td>0.574845</td>\n    </tr>\n    <tr>\n      <th>DEAP + LR</th>\n      <td>91.05</td>\n      <td>0.859722</td>\n      <td>0.697121</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_feat_len_deap_df.groupby([\"classifier\"])[[\"num_feats\", \"cv_score\", \"test_score\"]].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}