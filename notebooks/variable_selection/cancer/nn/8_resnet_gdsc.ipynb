{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\r\n",
      "Collecting jaxlib[cuda112]==0.3.15\r\n",
      "  Downloading https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.15%2Bcuda11.cudnn82-cp39-none-manylinux2014_x86_64.whl (162.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m162.7/162.7 MB\u001B[0m \u001B[31m13.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25h\u001B[33mWARNING: jaxlib 0.3.15+cuda11.cudnn82 does not provide the extra 'cuda112'\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from jaxlib[cuda112]==0.3.15) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.9/dist-packages (from jaxlib[cuda112]==0.3.15) (1.23.1)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jaxlib[cuda112]==0.3.15) (1.8.1)\r\n",
      "Installing collected packages: jaxlib\r\n",
      "  Attempting uninstall: jaxlib\r\n",
      "    Found existing installation: jaxlib 0.3.8+cuda11.cudnn82\r\n",
      "    Uninstalling jaxlib-0.3.8+cuda11.cudnn82:\r\n",
      "      Successfully uninstalled jaxlib-0.3.8+cuda11.cudnn82\r\n",
      "Successfully installed jaxlib-0.3.15+cuda11.cudnn82\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mLooking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\r\n",
      "Collecting jax[cuda112]==0.3.17\r\n",
      "  Downloading jax-0.3.17.tar.gz (1.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m62.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[33mWARNING: jax 0.3.17 does not provide the extra 'cuda112'\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (1.23.1)\r\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (3.3.0)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (1.8.1)\r\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (4.3.0)\r\n",
      "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.9/dist-packages (from jax[cuda112]==0.3.17) (0.6.0)\r\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax[cuda112]==0.3.17) (5.8.0)\r\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax[cuda112]==0.3.17) (3.8.1)\r\n",
      "Building wheels for collected packages: jax\r\n",
      "  Building wheel for jax (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for jax: filename=jax-0.3.17-py3-none-any.whl size=1217849 sha256=b3c9a976db17c191e23c35f155901cc69d126540cdb47daccbd700a3e9ae6c78\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/36/cd/88/2d90379f7549c27d5654e893f74210f30f0c645c23a71e6f56\r\n",
      "Successfully built jax\r\n",
      "Installing collected packages: jax\r\n",
      "  Attempting uninstall: jax\r\n",
      "    Found existing installation: jax 0.3.14\r\n",
      "    Uninstalling jax-0.3.14:\r\n",
      "      Successfully uninstalled jax-0.3.14\r\n",
      "Successfully installed jax-0.3.17\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting optax\r\n",
      "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m154.9/154.9 kB\u001B[0m \u001B[31m27.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.9/dist-packages (from optax) (0.3.17)\r\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.9/dist-packages (from optax) (4.3.0)\r\n",
      "Collecting chex>=0.1.5\r\n",
      "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m85.3/85.3 kB\u001B[0m \u001B[31m26.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from optax) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from optax) (1.23.1)\r\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.9/dist-packages (from optax) (0.3.15+cuda11.cudnn82)\r\n",
      "Collecting toolz>=0.9.0\r\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m55.8/55.8 kB\u001B[0m \u001B[31m18.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting dm-tree>=0.1.5\r\n",
      "  Downloading dm_tree-0.1.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (142 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m142.7/142.7 kB\u001B[0m \u001B[31m39.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (3.3.0)\r\n",
      "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (0.6.0)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (1.8.1)\r\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax>=0.1.55->optax) (3.8.1)\r\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax>=0.1.55->optax) (5.8.0)\r\n",
      "Installing collected packages: dm-tree, toolz, chex, optax\r\n",
      "Successfully installed chex-0.1.5 dm-tree-0.1.7 optax-0.1.4 toolz-0.12.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting dm-haiku\r\n",
      "  Downloading dm_haiku-0.0.9-py3-none-any.whl (352 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m352.1/352.1 kB\u001B[0m \u001B[31m29.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (1.23.1)\r\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (1.1.0)\r\n",
      "Collecting jmp>=0.0.2\r\n",
      "  Downloading jmp-0.0.2-py3-none-any.whl (16 kB)\r\n",
      "Collecting tabulate>=0.8.9\r\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\r\n",
      "Installing collected packages: tabulate, jmp, dm-haiku\r\n",
      "Successfully installed dm-haiku-0.0.9 jmp-0.0.2 tabulate-0.9.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting tensorflow-probability==0.17\r\n",
      "  Downloading tensorflow_probability-0.17.0-py2.py3-none-any.whl (6.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.5/6.5 MB\u001B[0m \u001B[31m62.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (5.1.1)\r\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (2.1.0)\r\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (0.1.7)\r\n",
      "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (0.4.0)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability==0.17) (1.23.1)\r\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow-probability==0.17) (1.14.0)\r\n",
      "Installing collected packages: tensorflow-probability\r\n",
      "Successfully installed tensorflow-probability-0.17.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting git+https://github.com/blackjax-devs/blackjax.git\r\n",
      "  Cloning https://github.com/blackjax-devs/blackjax.git to /tmp/pip-req-build-_wcbxo7l\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/blackjax-devs/blackjax.git /tmp/pip-req-build-_wcbxo7l\r\n",
      "  Resolved https://github.com/blackjax-devs/blackjax.git to commit eb2ee2680fa0754a4ad49c9dcbd8ed9ba68cf494\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting fastprogress>=0.2.0\r\n",
      "  Downloading fastprogress-1.0.3-py3-none-any.whl (12 kB)\r\n",
      "Requirement already satisfied: jax>=0.3.13 in /usr/local/lib/python3.9/dist-packages (from blackjax==0.9.6+78.geb2ee26) (0.3.17)\r\n",
      "Requirement already satisfied: jaxlib>=0.3.10 in /usr/local/lib/python3.9/dist-packages (from blackjax==0.9.6+78.geb2ee26) (0.3.15+cuda11.cudnn82)\r\n",
      "Collecting jaxopt>=0.5.5\r\n",
      "  Downloading jaxopt-0.5.5-py3-none-any.whl (132 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m132.3/132.3 kB\u001B[0m \u001B[31m25.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+78.geb2ee26) (4.3.0)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+78.geb2ee26) (1.8.1)\r\n",
      "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+78.geb2ee26) (0.6.0)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+78.geb2ee26) (1.1.0)\r\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+78.geb2ee26) (3.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.13->blackjax==0.9.6+78.geb2ee26) (1.23.1)\r\n",
      "Requirement already satisfied: matplotlib>=2.0.1 in /usr/local/lib/python3.9/dist-packages (from jaxopt>=0.5.5->blackjax==0.9.6+78.geb2ee26) (3.5.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+78.geb2ee26) (2.8.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+78.geb2ee26) (21.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+78.geb2ee26) (1.4.3)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+78.geb2ee26) (4.34.4)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+78.geb2ee26) (0.11.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+78.geb2ee26) (3.0.9)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+78.geb2ee26) (9.2.0)\r\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax>=0.3.13->blackjax==0.9.6+78.geb2ee26) (3.8.1)\r\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.9/dist-packages (from etils[epath]->jax>=0.3.13->blackjax==0.9.6+78.geb2ee26) (5.8.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.1->jaxopt>=0.5.5->blackjax==0.9.6+78.geb2ee26) (1.14.0)\r\n",
      "Building wheels for collected packages: blackjax\r\n",
      "  Building wheel for blackjax (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for blackjax: filename=blackjax-0.9.6+78.geb2ee26-py3-none-any.whl size=126440 sha256=ad3ad8b2409c7c7a414449faae2e5e69f22ad09a03496ff1ab5e2263f771a6c6\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fk1q1ano/wheels/e6/1e/f6/a6e0408a4e374b9cdb789b1769716b4ed61eef520a2dd702b1\r\n",
      "Successfully built blackjax\r\n",
      "Installing collected packages: fastprogress, jaxopt, blackjax\r\n",
      "Successfully installed blackjax-0.9.6+78.geb2ee26 fastprogress-1.0.3 jaxopt-0.5.5\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "Get:1 https://deb.nodesource.com/node_16.x focal InRelease [4583 B]\r\n",
      "Get:2 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease [18.1 kB] \u001B[0m\u001B[33m\r\n",
      "Get:3 https://deb.nodesource.com/node_16.x focal/main amd64 Packages [774 B]   \u001B[0m\u001B[33m\r\n",
      "Get:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]      \u001B[0m\u001B[33m\r\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\r\n",
      "Get:6 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 Packages [29.5 kB]\r\n",
      "Get:7 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [972 kB]m\u001B[33m\u001B[33m\u001B[33m\r\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\r\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]m\r\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\r\n",
      "Get:11 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2358 kB]33m\r\n",
      "Get:12 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1779 kB]\r\n",
      "Get:13 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [27.7 kB]\r\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB][0m\u001B[33m\u001B[33m\u001B[33m\u001B[33m\r\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\r\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\r\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2827 kB]\r\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1275 kB]\r\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [30.4 kB]\r\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [1894 kB]\r\n",
      "Get:21 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\r\n",
      "Get:22 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\r\n",
      "Fetched 24.7 MB in 2s (11.1 MB/s)[33m                   \u001B[0m\u001B[33m\u001B[33m\u001B[33m\u001B[33m\u001B[33m\u001B[33m\r\n",
      "Reading package lists... Done\r\n",
      "Building dependency tree       \r\n",
      "Reading state information... Done\r\n",
      "111 packages can be upgraded. Run 'apt list --upgradable' to see them.\r\n",
      "Reading package lists... Done\r\n",
      "Building dependency tree       \r\n",
      "Reading state information... Done\r\n",
      "The following additional packages will be installed:\r\n",
      "  fonts-liberation libann0 libcdt5 libcgraph6 libgts-0.7-5 libgts-bin libgvc6\r\n",
      "  libgvpr2 liblab-gamut1 libpathplan4\r\n",
      "Suggested packages:\r\n",
      "  gsfonts graphviz-doc\r\n",
      "The following NEW packages will be installed:\r\n",
      "  fonts-liberation graphviz libann0 libcdt5 libcgraph6 libgts-0.7-5 libgts-bin\r\n",
      "  libgvc6 libgvpr2 liblab-gamut1 libpathplan4\r\n",
      "0 upgraded, 11 newly installed, 0 to remove and 111 not upgraded.\r\n",
      "Need to get 2701 kB of archives.\r\n",
      "After this operation, 11.3 MB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 fonts-liberation all 1:1.07.4-11 [822 kB]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libann0 amd64 1.1.2+doc-7build1 [26.0 kB]\r\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 libcdt5 amd64 2.42.2-3build2 [18.7 kB]\r\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 libcgraph6 amd64 2.42.2-3build2 [41.3 kB]\r\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgts-0.7-5 amd64 0.7.6+darcs121130-4 [150 kB]\r\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/universe amd64 libpathplan4 amd64 2.42.2-3build2 [21.9 kB]\r\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgvc6 amd64 2.42.2-3build2 [647 kB]\r\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgvpr2 amd64 2.42.2-3build2 [167 kB]\r\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/universe amd64 liblab-gamut1 amd64 2.42.2-3build2 [177 kB]\r\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/universe amd64 graphviz amd64 2.42.2-3build2 [590 kB]\r\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgts-bin amd64 0.7.6+darcs121130-4 [41.3 kB]\r\n",
      "Fetched 2701 kB in 1s (3395 kB/s)     \u001B[0m\u001B[33m\r\n",
      "\n",
      "\u001B7\u001B[0;23r\u001B8\u001B[1ASelecting previously unselected package fonts-liberation.\r\n",
      "(Reading database ... 78556 files and directories currently installed.)\r\n",
      "Preparing to unpack .../00-fonts-liberation_1%3a1.07.4-11_all.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  0%]\u001B[49m\u001B[39m [..........................................................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  2%]\u001B[49m\u001B[39m [#.........................................................] \u001B8Unpacking fonts-liberation (1:1.07.4-11) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  4%]\u001B[49m\u001B[39m [##........................................................] \u001B8Selecting previously unselected package libann0.\r\n",
      "Preparing to unpack .../01-libann0_1.1.2+doc-7build1_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  7%]\u001B[49m\u001B[39m [###.......................................................] \u001B8Unpacking libann0 (1.1.2+doc-7build1) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  9%]\u001B[49m\u001B[39m [#####.....................................................] \u001B8Selecting previously unselected package libcdt5:amd64.\r\n",
      "Preparing to unpack .../02-libcdt5_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 11%]\u001B[49m\u001B[39m [######....................................................] \u001B8Unpacking libcdt5:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 13%]\u001B[49m\u001B[39m [#######...................................................] \u001B8Selecting previously unselected package libcgraph6:amd64.\r\n",
      "Preparing to unpack .../03-libcgraph6_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 16%]\u001B[49m\u001B[39m [#########.................................................] \u001B8Unpacking libcgraph6:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 18%]\u001B[49m\u001B[39m [##########................................................] \u001B8Selecting previously unselected package libgts-0.7-5:amd64.\r\n",
      "Preparing to unpack .../04-libgts-0.7-5_0.7.6+darcs121130-4_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 20%]\u001B[49m\u001B[39m [###########...............................................] \u001B8Unpacking libgts-0.7-5:amd64 (0.7.6+darcs121130-4) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 22%]\u001B[49m\u001B[39m [############..............................................] \u001B8Selecting previously unselected package libpathplan4:amd64.\r\n",
      "Preparing to unpack .../05-libpathplan4_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 24%]\u001B[49m\u001B[39m [##############............................................] \u001B8Unpacking libpathplan4:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 27%]\u001B[49m\u001B[39m [###############...........................................] \u001B8Selecting previously unselected package libgvc6.\r\n",
      "Preparing to unpack .../06-libgvc6_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 29%]\u001B[49m\u001B[39m [################..........................................] \u001B8Unpacking libgvc6 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 31%]\u001B[49m\u001B[39m [##################........................................] \u001B8Selecting previously unselected package libgvpr2:amd64.\r\n",
      "Preparing to unpack .../07-libgvpr2_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 33%]\u001B[49m\u001B[39m [###################.......................................] \u001B8Unpacking libgvpr2:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 36%]\u001B[49m\u001B[39m [####################......................................] \u001B8Selecting previously unselected package liblab-gamut1:amd64.\r\n",
      "Preparing to unpack .../08-liblab-gamut1_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 38%]\u001B[49m\u001B[39m [#####################.....................................] \u001B8Unpacking liblab-gamut1:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 40%]\u001B[49m\u001B[39m [#######################...................................] \u001B8Selecting previously unselected package graphviz.\r\n",
      "Preparing to unpack .../09-graphviz_2.42.2-3build2_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 42%]\u001B[49m\u001B[39m [########################..................................] \u001B8Unpacking graphviz (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 44%]\u001B[49m\u001B[39m [#########################.................................] \u001B8Selecting previously unselected package libgts-bin.\r\n",
      "Preparing to unpack .../10-libgts-bin_0.7.6+darcs121130-4_amd64.deb ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 47%]\u001B[49m\u001B[39m [###########################...............................] \u001B8Unpacking libgts-bin (0.7.6+darcs121130-4) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 49%]\u001B[49m\u001B[39m [############################..............................] \u001B8Setting up liblab-gamut1:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 51%]\u001B[49m\u001B[39m [#############################.............................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 53%]\u001B[49m\u001B[39m [##############################............................] \u001B8Setting up libgts-0.7-5:amd64 (0.7.6+darcs121130-4) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 56%]\u001B[49m\u001B[39m [################################..........................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 58%]\u001B[49m\u001B[39m [#################################.........................] \u001B8Setting up libpathplan4:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 60%]\u001B[49m\u001B[39m [##################################........................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 62%]\u001B[49m\u001B[39m [####################################......................] \u001B8Setting up libann0 (1.1.2+doc-7build1) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 64%]\u001B[49m\u001B[39m [#####################################.....................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 67%]\u001B[49m\u001B[39m [######################################....................] \u001B8Setting up fonts-liberation (1:1.07.4-11) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 69%]\u001B[49m\u001B[39m [#######################################...................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 71%]\u001B[49m\u001B[39m [#########################################.................] \u001B8Setting up libcdt5:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 73%]\u001B[49m\u001B[39m [##########################################................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 76%]\u001B[49m\u001B[39m [###########################################...............] \u001B8Setting up libcgraph6:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 78%]\u001B[49m\u001B[39m [#############################################.............] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 80%]\u001B[49m\u001B[39m [##############################################............] \u001B8Setting up libgts-bin (0.7.6+darcs121130-4) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 82%]\u001B[49m\u001B[39m [###############################################...........] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 84%]\u001B[49m\u001B[39m [################################################..........] \u001B8Setting up libgvc6 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 87%]\u001B[49m\u001B[39m [##################################################........] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 89%]\u001B[49m\u001B[39m [###################################################.......] \u001B8Setting up libgvpr2:amd64 (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 91%]\u001B[49m\u001B[39m [####################################################......] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 93%]\u001B[49m\u001B[39m [######################################################....] \u001B8Setting up graphviz (2.42.2-3build2) ...\r\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 96%]\u001B[49m\u001B[39m [#######################################################...] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 98%]\u001B[49m\u001B[39m [########################################################..] \u001B8Processing triggers for libc-bin (2.31-0ubuntu9.7) ...\r\n",
      "Processing triggers for man-db (2.9.1-1) ...\r\n",
      "Processing triggers for fontconfig (2.13.1-2ubuntu3) ...\r\n",
      "\r\n",
      "Reading package lists... Done\r\n",
      "Building dependency tree       \r\n",
      "Reading state information... Done\r\n",
      "The following additional packages will be installed:\r\n",
      "  swig4.0\r\n",
      "Suggested packages:\r\n",
      "  swig-doc swig-examples swig4.0-examples swig4.0-doc\r\n",
      "The following NEW packages will be installed:\r\n",
      "  swig swig4.0\r\n",
      "0 upgraded, 2 newly installed, 0 to remove and 111 not upgraded.\r\n",
      "Need to get 1086 kB of archives.\r\n",
      "After this operation, 5413 kB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig4.0 amd64 4.0.1-5build1 [1081 kB]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig all 4.0.1-5build1 [5528 B]\r\n",
      "Fetched 1086 kB in 1s (1553 kB/s)\r\n",
      "Selecting previously unselected package swig4.0.\r\n",
      "(Reading database ... 78769 files and directories currently installed.)\r\n",
      "Preparing to unpack .../swig4.0_4.0.1-5build1_amd64.deb ...\r\n",
      "Unpacking swig4.0 (4.0.1-5build1) ...\r\n",
      "Selecting previously unselected package swig.\r\n",
      "Preparing to unpack .../swig_4.0.1-5build1_all.deb ...\r\n",
      "Unpacking swig (4.0.1-5build1) ...\r\n",
      "Setting up swig4.0 (4.0.1-5build1) ...\r\n",
      "Setting up swig (4.0.1-5build1) ...\r\n",
      "Processing triggers for man-db (2.9.1-1) ...\r\n",
      "Collecting smac\r\n",
      "  Downloading smac-1.4.0.tar.gz (202 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m202.9/202.9 kB\u001B[0m \u001B[31m27.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.9/dist-packages (from smac) (1.23.1)\r\n",
      "Collecting ConfigSpace>=0.5.0\r\n",
      "  Downloading ConfigSpace-0.6.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.6/5.6 MB\u001B[0m \u001B[31m43.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.9/dist-packages (from smac) (1.1.1)\r\n",
      "Collecting dask\r\n",
      "  Downloading dask-2022.12.0-py3-none-any.whl (1.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m86.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from smac) (1.8.1)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from smac) (1.1.0)\r\n",
      "Collecting pyrfr>=0.8.3\r\n",
      "  Downloading pyrfr-0.8.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.5/4.5 MB\u001B[0m \u001B[31m14.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0mm\r\n",
      "\u001B[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from smac) (2022.7.9)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from smac) (5.9.1)\r\n",
      "Collecting distributed\r\n",
      "  Downloading distributed-2022.12.0-py3-none-any.whl (925 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m925.3/925.3 kB\u001B[0m \u001B[31m92.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting pynisher<1.0.0\r\n",
      "  Downloading pynisher-0.6.4.tar.gz (11 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting emcee>=3.0.0\r\n",
      "  Downloading emcee-3.1.3-py2.py3-none-any.whl (46 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.2/46.2 kB\u001B[0m \u001B[31m15.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from ConfigSpace>=0.5.0->smac) (0.29.30)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from ConfigSpace>=0.5.0->smac) (4.3.0)\r\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from ConfigSpace>=0.5.0->smac) (3.0.9)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from pynisher<1.0.0->smac) (63.1.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22.0->smac) (3.1.0)\r\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (2022.5.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (21.3)\r\n",
      "Collecting partd>=0.3.10\r\n",
      "  Downloading partd-1.3.0-py3-none-any.whl (18 kB)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (5.4.1)\r\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (8.1.3)\r\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (2.1.0)\r\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.9/dist-packages (from dask->smac) (0.12.0)\r\n",
      "Collecting sortedcontainers!=2.0.0,!=2.0.1\r\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\r\n",
      "Collecting locket>=1.0.0\r\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\r\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.9/dist-packages (from distributed->smac) (6.2)\r\n",
      "Collecting zict>=0.1.3\r\n",
      "  Downloading zict-2.2.0-py2.py3-none-any.whl (23 kB)\r\n",
      "Collecting msgpack>=0.6.0\r\n",
      "  Downloading msgpack-1.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m322.4/322.4 kB\u001B[0m \u001B[31m60.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting tblib>=1.6.0\r\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from distributed->smac) (3.1.2)\r\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from distributed->smac) (1.26.10)\r\n",
      "Collecting heapdict\r\n",
      "  Downloading HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->distributed->smac) (2.1.1)\r\n",
      "Building wheels for collected packages: smac, pynisher\r\n",
      "  Building wheel for smac (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for smac: filename=smac-1.4.0-py3-none-any.whl size=262348 sha256=9d31b81e8130cec4276bd62053578435b55225ba77b4d5f6fa24975264bd624c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/cc/e7/d683d9404760c4701ea2f64faaf689a8de718f701de63e71ea\r\n",
      "  Building wheel for pynisher (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for pynisher: filename=pynisher-0.6.4-py3-none-any.whl size=7026 sha256=e86463328883aa7b4419ee7126edf0b7daa891c594c7287407ae88fc27e35075\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/1d/de/5e/d4947b76b76ba27581d1e09f395eca1583a802203a41c04873\r\n",
      "Successfully built smac pynisher\r\n",
      "Installing collected packages: sortedcontainers, msgpack, heapdict, zict, tblib, pyrfr, pynisher, locket, emcee, partd, ConfigSpace, dask, distributed, smac\r\n",
      "Successfully installed ConfigSpace-0.6.0 dask-2022.12.0 distributed-2022.12.0 emcee-3.1.3 heapdict-1.0.1 locket-1.0.0 msgpack-1.0.4 partd-1.3.0 pynisher-0.6.4 pyrfr-0.8.3 smac-1.4.0 sortedcontainers-2.4.0 tblib-1.7.0 zict-2.2.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting graphviz\r\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m47.0/47.0 kB\u001B[0m \u001B[31m12.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: graphviz\r\n",
      "Successfully installed graphviz-0.20.1\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting gplearn\r\n",
      "  Downloading gplearn-0.4.2-py3-none-any.whl (25 kB)\r\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from gplearn) (1.1.1)\r\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from gplearn) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.23.1)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.8.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (3.1.0)\r\n",
      "Installing collected packages: gplearn\r\n",
      "Successfully installed gplearn-0.4.2\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U jaxlib[cuda112]==0.3.15 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install -U jax[cuda112]==0.3.17 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install optax\n",
    "!pip install dm-haiku\n",
    "!pip install tensorflow-probability==0.17\n",
    "!pip install git+https://github.com/blackjax-devs/blackjax.git\n",
    "!apt update\n",
    "!apt install -y graphviz\n",
    "!apt-get -y install swig\n",
    "!pip install smac\n",
    "!pip install graphviz\n",
    "!pip install gplearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xabush/miniconda3/lib/python3.9/site-packages/chex/_src/pytypes.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"False\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "SERVER = 0\n",
    "\n",
    "if not SERVER:\n",
    "    %cd /home/xabush/code/snet/moses-incons-pen-xp/notebooks/variable_selection/cancer/nn\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "tfd = tfp.distributions\n",
    "import jax\n",
    "import haiku as hk\n",
    "import numpy as np\n",
    "import optax\n",
    "from nn_util import *\n",
    "from optim_util import *\n",
    "plt.style.use('ggplot')\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JAX_ENABLE_X64=True\n"
     ]
    }
   ],
   "source": [
    "if SERVER:\n",
    "    data_dir = \".\"\n",
    "else:\n",
    "    data_dir = \"/home/xabush/code/snet/moses-incons-pen-xp/data\"\n",
    "\n",
    "%env JAX_ENABLE_X64=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### GDSC Cell Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Tamoxifen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(406, 37265)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdsc_dir = f\"{data_dir}/cell_line/gdsc2\"\n",
    "gdsc_exp_tamox_data = pd.read_csv(f\"{gdsc_dir}/tamoxifen_response_gene_expr.csv\")\n",
    "gdsc_exp_tamox_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "X, target = gdsc_exp_tamox_data.iloc[:,:-1], gdsc_exp_tamox_data.iloc[:,-1]\n",
    "# change to -log10(IC_50) to make it comparable\n",
    "target = -np.log10(np.exp(target)) # exp b/c the values are natural logs of raw IC_50"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "     symbol method_of_action        cosmic_moa intogen_moa    gene_id\n0     ABCB1              Act               NaN         Act  SIDG00064\n1      ABI1        ambiguous       TSG, fusion   ambiguous  SIDG00145\n2      ABL1              Act  oncogene, fusion         Act  SIDG00150\n3      ABL2              Act  oncogene, fusion         Act  SIDG00151\n4     ACKR3              Act  oncogene, fusion         Act  SIDG00205\n..      ...              ...               ...         ...        ...\n778  ZNF814              Act               NaN         Act  SIDG42334\n779   ZNF93              LoF               NaN         LoF  SIDG41755\n780   ZNRF3              LoF               NaN         LoF  SIDG42403\n781   ZRSR2              LoF               TSG         LoF  SIDG42422\n782    ZXDB              LoF               NaN         LoF  SIDG42467\n\n[783 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>method_of_action</th>\n      <th>cosmic_moa</th>\n      <th>intogen_moa</th>\n      <th>gene_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ABCB1</td>\n      <td>Act</td>\n      <td>NaN</td>\n      <td>Act</td>\n      <td>SIDG00064</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ABI1</td>\n      <td>ambiguous</td>\n      <td>TSG, fusion</td>\n      <td>ambiguous</td>\n      <td>SIDG00145</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ABL1</td>\n      <td>Act</td>\n      <td>oncogene, fusion</td>\n      <td>Act</td>\n      <td>SIDG00150</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ABL2</td>\n      <td>Act</td>\n      <td>oncogene, fusion</td>\n      <td>Act</td>\n      <td>SIDG00151</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ACKR3</td>\n      <td>Act</td>\n      <td>oncogene, fusion</td>\n      <td>Act</td>\n      <td>SIDG00205</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>778</th>\n      <td>ZNF814</td>\n      <td>Act</td>\n      <td>NaN</td>\n      <td>Act</td>\n      <td>SIDG42334</td>\n    </tr>\n    <tr>\n      <th>779</th>\n      <td>ZNF93</td>\n      <td>LoF</td>\n      <td>NaN</td>\n      <td>LoF</td>\n      <td>SIDG41755</td>\n    </tr>\n    <tr>\n      <th>780</th>\n      <td>ZNRF3</td>\n      <td>LoF</td>\n      <td>NaN</td>\n      <td>LoF</td>\n      <td>SIDG42403</td>\n    </tr>\n    <tr>\n      <th>781</th>\n      <td>ZRSR2</td>\n      <td>LoF</td>\n      <td>TSG</td>\n      <td>LoF</td>\n      <td>SIDG42422</td>\n    </tr>\n    <tr>\n      <th>782</th>\n      <td>ZXDB</td>\n      <td>LoF</td>\n      <td>NaN</td>\n      <td>LoF</td>\n      <td>SIDG42467</td>\n    </tr>\n  </tbody>\n</table>\n<p>783 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_driver_genes_df = pd.read_csv(f\"{data_dir}/cell_line/driver_genes_20221018.csv\")\n",
    "cancer_driver_genes_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "cols = X.columns.to_list()\n",
    "driver_syms = cancer_driver_genes_df[\"symbol\"].to_list()\n",
    "sym_list = [sym.strip() for sym in cols if sym in driver_syms]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(406, 768)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_selected = X[sym_list]\n",
    "X_selected.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "seed = 739\n",
    "X_train_outer_df, X_test_df, y_train_outer_df, y_test_df = train_test_split(X_selected, target, random_state=seed, shuffle=True, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xabush/miniconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:2583: UserWarning: n_quantiles (1000) is greater than the total number of samples (324). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer, PowerTransformer, RobustScaler, MinMaxScaler, Normalizer, StandardScaler\n",
    "\n",
    "train_transformer = QuantileTransformer(random_state=seed, output_distribution=\"normal\").fit(X_train_outer_df)\n",
    "# train_transformer = PowerTransformer().fit(X_train_df)\n",
    "train_transformed = train_transformer.transform(X_train_outer_df)\n",
    "test_transformed = train_transformer.transform(X_test_df)\n",
    "\n",
    "X_train_outer_df = pd.DataFrame(train_transformed, columns=X_train_outer_df.columns)\n",
    "X_test_df = pd.DataFrame(test_transformed, columns=X_test_df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X_train_outer_df, y_train_outer_df, shuffle=True,\n",
    "                                                              random_state=seed, test_size=0.2)\n",
    "train_indices, val_indices = X_train_df.index.to_list(), X_val_df.index.to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "X_train_outer, y_train_outer = X_train_outer_df.values, y_train_outer_df.values\n",
    "X_train, y_train = X_train_df.values, y_train_df.values\n",
    "X_val, y_val = X_val_df.values, y_val_df.values\n",
    "X_test, y_test = X_test_df.values, y_test_df.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "class ResnetState(NamedTuple):\n",
    "    params: hk.Params\n",
    "    opt_state: hk.Params\n",
    "    net_state: hk.State\n",
    "\n",
    "class ResNetBlock(hk.Module):\n",
    "    def __init__(self, act_fn, dim, init_fn, dropout_rate, name=None):\n",
    "        super().__init__(name)\n",
    "        self.act_fn = act_fn\n",
    "        self.dim = dim\n",
    "        self.init_fn = init_fn\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def __call__(self, x, is_training):\n",
    "        key = hk.next_rng_key()\n",
    "        subkey1, subkey2 = jax.random.split(key, 2)\n",
    "        z = hk.Linear(self.dim, w_init=self.init_fn, with_bias=False)(x)\n",
    "        z = hk.BatchNorm(True, True, 0.9)(z, is_training)\n",
    "        z = self.act_fn(z)\n",
    "        if is_training:\n",
    "            z = hk.dropout(subkey1, self.dropout_rate, z)\n",
    "        z = hk.Linear(self.dim, w_init=self.init_fn, with_bias=False)(z)\n",
    "        if is_training:\n",
    "            z = hk.dropout(subkey2, self.dropout_rate, z)\n",
    "        x_out = self.act_fn(z + x)\n",
    "        return x_out\n",
    "\n",
    "class PreActResNetBlock(ResNetBlock):\n",
    "\n",
    "    def __init__(self, act_fn, dim, init_fn, dropout_rate, name=None):\n",
    "        super().__init__(act_fn, dim, init_fn, dropout_rate, name)\n",
    "\n",
    "\n",
    "    def __call__(self, x, is_training):\n",
    "        key = hk.next_rng_key()\n",
    "        subkey1, subkey2, subkey3 = jax.random.split(key, 3)\n",
    "        z = hk.BatchNorm(True, True, 0.9)(x, is_training)\n",
    "        z = self.act_fn(z)\n",
    "        if is_training:\n",
    "            z = hk.dropout(subkey1, self.dropout_rate, z)\n",
    "        z = hk.Linear(self.dim, w_init=self.init_fn, with_bias=False)(z)\n",
    "        z = hk.BatchNorm(True, True, 0.9)(z, is_training)\n",
    "        z = self.act_fn(z)\n",
    "        if is_training:\n",
    "            z = hk.dropout(subkey2, self.dropout_rate, z)\n",
    "        z = hk.Linear(self.dim, w_init=self.init_fn, with_bias=False)(z)\n",
    "        if is_training:\n",
    "            z = hk.dropout(subkey3, self.dropout_rate, z)\n",
    "        x_out = z + x\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class ResNet:\n",
    "    def __init__(self, block_class, num_blocks, hidden_dims,\n",
    "                 optim, init_fn, act_fn, dropout_rate):\n",
    "\n",
    "        self.block_class = block_class\n",
    "        self.num_blocks = num_blocks\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.act_fn = act_fn\n",
    "        self.optimiser = optim\n",
    "        self.init_fn = init_fn #TODO try d/t values\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self._forward = hk.transform_with_state(self._forward_fn)\n",
    "        self.loss = jax.jit(self.loss)\n",
    "        self.update = jax.jit(self.update)\n",
    "\n",
    "        assert len(self.num_blocks) == len(self.hidden_dims)\n",
    "\n",
    "    def init(self, rng, x):\n",
    "        params, net_state = self._forward.init(rng, x, is_training=True)\n",
    "        opt_state = self.optimiser.init(params)\n",
    "        return ResnetState(params, opt_state, net_state)\n",
    "\n",
    "    def apply(self, state, x, key, is_training=True):\n",
    "        return self._forward.apply(state.params, state.net_state, key, x, is_training)\n",
    "\n",
    "\n",
    "    def update(self, key, train_state, x, y):\n",
    "        params, opt_state, net_state = train_state\n",
    "        grads, net_state = jax.grad(self.loss, has_aux=True)(params, net_state, key, x, y)\n",
    "        updates, opt_state = self.optimiser.update(grads, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return ResnetState(params, opt_state, net_state)\n",
    "\n",
    "    def _forward_fn(self, x, is_training):\n",
    "\n",
    "        # First layer\n",
    "        x = hk.Linear(self.hidden_dims[0], w_init=self.init_fn, with_bias=False)(x)\n",
    "\n",
    "        if self.block_class == ResNetBlock: # If pre-activation block , we don't apply non-linearities  yet\n",
    "            x = hk.BatchNorm(True, True, 0.9)(x, is_training)\n",
    "            x = self.act_fn(x)\n",
    "\n",
    "        for block_idx, block_count in enumerate(self.num_blocks):\n",
    "            for bc in range(block_count):\n",
    "                x = self.block_class(self.act_fn, self.hidden_dims[block_idx], self.init_fn, self.dropout_rate)(x, is_training)\n",
    "\n",
    "\n",
    "        x = hk.Linear(1)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, params, net_state, key, x, y):\n",
    "        preds, state = self._forward.apply(params, net_state, key, x, True)\n",
    "        preds = preds.squeeze()\n",
    "        nll_loss = jnp.mean((preds - y)**2)\n",
    "\n",
    "        return nll_loss, state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### NN Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_act_fn(name):\n",
    "    if name == \"relu\":\n",
    "        return jax.nn.relu\n",
    "    if name == \"swish\":\n",
    "        return jax.nn.swish\n",
    "    if name == \"tanh\":\n",
    "        return jax.nn.tanh\n",
    "    if name == \"sigmoid\":\n",
    "        return jax.nn.sigmoid\n",
    "    if name == \"celu\":\n",
    "        return jax.nn.celu\n",
    "    if name == \"relu6\":\n",
    "        return jax.nn.relu6\n",
    "    if name == \"glu\":\n",
    "        return jax.nn.glu\n",
    "    if name == \"elu\":\n",
    "        return jax.nn.elu\n",
    "    if name == \"leaky_relu\":\n",
    "        return jax.nn.leaky_relu\n",
    "    if name == \"log_sigmoid\":\n",
    "        return jax.nn.log_sigmoid\n",
    "\n",
    "    return ValueError(f\"Unknown activation function: {name}\")\n",
    "\n",
    "def init_nn_model(optim,\n",
    "                  block_type, num_blocks,\n",
    "                  hidden_sizes, init_fn, act_fn, dropout_rate):\n",
    "\n",
    "    if block_type == \"ResNet\":\n",
    "        block_class = ResNetBlock\n",
    "    else:\n",
    "        block_class = PreActResNetBlock\n",
    "\n",
    "    model = ResNet(block_class, num_blocks, hidden_sizes, optim, init_fn, act_fn, dropout_rate)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_nn_model(rng_key, data_loader, epochs, num_cycles, lr_0,\n",
    "                       block_type, num_blocks, hidden_sizes, init_fn, weight_decay,\n",
    "                        act_fn_name, dropout_rate ,show_pgbar=True):\n",
    "\n",
    "\n",
    "    act_fn = get_act_fn(act_fn_name)\n",
    "    total_steps = len(data_loader)*epochs\n",
    "\n",
    "    schedule_fn = make_cyclical_lr_fn(lr_0, epochs, num_cycles)\n",
    "    optim = optax.chain(optax.scale_by_adam(), optax.add_decayed_weights(weight_decay),\n",
    "                        optax.scale_by_schedule(schedule_fn), optax.scale(-1.0))\n",
    "    model = init_nn_model(optim, block_type, num_blocks, hidden_sizes, init_fn, act_fn, dropout_rate)\n",
    "\n",
    "    cycle_len = epochs // num_cycles\n",
    "    init_state = model.init(rng_key, next(iter(data_loader))[0])\n",
    "\n",
    "    state = init_state\n",
    "\n",
    "    # print(f\"Total iterations: {epochs*num_batches}, Num Batches: {num_batches}, Cycle Len: {M}\")\n",
    "    states = []\n",
    "    val_losses = []\n",
    "    step = 0\n",
    "    key = rng_key\n",
    "\n",
    "    if show_pgbar:\n",
    "        pgbar = tqdm(range(epochs))\n",
    "    else:\n",
    "        pgbar = range(epochs)\n",
    "\n",
    "    for epoch in pgbar:\n",
    "        for batch_x, batch_y in data_loader:\n",
    "            _, key = jax.random.split(key, 2)\n",
    "            state = model.update(key, state, batch_x, batch_y)\n",
    "            step += 1\n",
    "\n",
    "        if epoch > 0 and ((epoch + 1) % cycle_len == 0): # take snapshot\n",
    "            states.append(state)\n",
    "\n",
    "\n",
    "    return model, states, val_losses\n",
    "\n",
    "\n",
    "def eval_nn_model(key, model, x, y, states):\n",
    "\n",
    "    if isinstance(states, list):\n",
    "        y_preds = np.zeros((len(states), len(y)))\n",
    "        for i, state in enumerate(states):\n",
    "            preds, _ = model.apply(state, x, key, False)\n",
    "            y_preds[i] = preds.squeeze()\n",
    "\n",
    "        y_preds = np.mean(y_preds, axis=0)\n",
    "        rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "    else:\n",
    "        y_preds, _ = model.apply(states, x, key, False)\n",
    "        rmse = jnp.sqrt(jnp.mean((y - y_preds.squeeze())**2))\n",
    "\n",
    "    return rmse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "outputs": [],
   "source": [
    "import optuna\n",
    "def objective_resnet(trial, seed, x_train, x_val, y_train, y_val, epochs,\n",
    "                  batch_size, num_blocks, hidden_sizes, init_fn):\n",
    "\n",
    "    lr_0 = trial.suggest_categorical(\"lr_0\", [1e-3, 5e-3, 1e-2, 1e-1])\n",
    "    act_fn = \"relu\"\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-10, 1., log=True)\n",
    "    dropout_rate = trial.suggest_categorical(\"dropout_rate\", [0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    num_cycles = trial.suggest_categorical(\"num_cycles\", [2, 4, 6, 8, 10])\n",
    "    block_type = trial.suggest_categorical(\"block_type\", [\"ResNet\", \"PreActResNet\"])\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    data_loader = NumpyLoader(NumpyData(x_train, y_train), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    bnn_model, state, _ = train_nn_model(rng, data_loader, epochs, num_cycles, lr_0,\n",
    "                                                                block_type, num_blocks, hidden_sizes,\n",
    "                                                                init_fn, weight_decay, act_fn, dropout_rate ,show_pgbar=False)\n",
    "\n",
    "    rmse = eval_nn_model(rng_key, bnn_model, x_val, y_val, state)\n",
    "\n",
    "    return rmse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-31 18:13:06,560]\u001B[0m A new study created in memory with name: no-name-697efa14-e31c-4452-9a69-4f752b779415\u001B[0m\n",
      "\u001B[32m[I 2023-01-31 18:13:56,467]\u001B[0m Trial 0 finished with value: 0.9390432238578796 and parameters: {'lr_0': 0.1, 'weight_decay': 0.0018197136825097908, 'dropout_rate': 0.6, 'num_cycles': 10, 'block_type': 'ResNet'}. Best is trial 0 with value: 0.9390432238578796.\u001B[0m\n",
      "\u001B[32m[I 2023-01-31 18:14:41,397]\u001B[0m Trial 1 finished with value: 0.5342723727226257 and parameters: {'lr_0': 0.01, 'weight_decay': 0.032854033879743144, 'dropout_rate': 0.1, 'num_cycles': 2, 'block_type': 'ResNet'}. Best is trial 1 with value: 0.5342723727226257.\u001B[0m\n",
      "\u001B[32m[I 2023-01-31 18:15:27,790]\u001B[0m Trial 2 finished with value: 0.5291550755500793 and parameters: {'lr_0': 0.01, 'weight_decay': 1.5295816507885268e-06, 'dropout_rate': 0.0, 'num_cycles': 6, 'block_type': 'ResNet'}. Best is trial 2 with value: 0.5291550755500793.\u001B[0m\n",
      "\u001B[32m[I 2023-01-31 18:16:15,859]\u001B[0m Trial 3 finished with value: 0.5546969771385193 and parameters: {'lr_0': 0.005, 'weight_decay': 3.420976960866209e-05, 'dropout_rate': 0.3, 'num_cycles': 8, 'block_type': 'ResNet'}. Best is trial 2 with value: 0.5291550755500793.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(sampler=sampler)\n",
    "init_fn = hk.initializers.VarianceScaling(2.0, \"fan_in\",  \"truncated_normal\")\n",
    "study.optimize(lambda trial: objective_resnet(trial, seed, X_train, X_val, y_train, y_val, 200, 32,\n",
    "                                              [1, 1, 1], [512, 512, 512], init_fn), timeout=180)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr_0': 0.01, 'weight_decay': 1.5295816507885268e-06, 'dropout_rate': 0.0, 'num_cycles': 6, 'block_type': 'ResNet'}\n"
     ]
    }
   ],
   "source": [
    "resnet_config = study.best_params\n",
    "print(resnet_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/optuna/visualization/_plotly_imports.py:7\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m try_import() \u001B[38;5;28;01mas\u001B[39;00m _imports:  \u001B[38;5;66;03m# NOQA\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mplotly\u001B[39;00m  \u001B[38;5;66;03m# NOQA\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mplotly\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__ \u001B[38;5;28;01mas\u001B[39;00m plotly_version\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'plotly'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[0;32mIn [112]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01moptuna\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvisualization\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m plot_param_importances\n\u001B[0;32m----> 2\u001B[0m \u001B[43mplot_param_importances\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/optuna/visualization/_param_importances.py:138\u001B[0m, in \u001B[0;36mplot_param_importances\u001B[0;34m(study, evaluator, params, target, target_name)\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot_param_importances\u001B[39m(\n\u001B[1;32m     75\u001B[0m     study: Study,\n\u001B[1;32m     76\u001B[0m     evaluator: Optional[BaseImportanceEvaluator] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     80\u001B[0m     target_name: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mObjective Value\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     81\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgo.Figure\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     82\u001B[0m     \u001B[38;5;124;03m\"\"\"Plot hyperparameter importances.\u001B[39;00m\n\u001B[1;32m     83\u001B[0m \n\u001B[1;32m     84\u001B[0m \u001B[38;5;124;03m    Example:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;124;03m        A :class:`plotly.graph_objs.Figure` object.\u001B[39;00m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 138\u001B[0m     \u001B[43m_imports\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m     importances_info \u001B[38;5;241m=\u001B[39m _get_importances_info(study, evaluator, params, target, target_name)\n\u001B[1;32m    141\u001B[0m     hover_template \u001B[38;5;241m=\u001B[39m _get_hover_template(importances_info, study)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/optuna/_imports.py:89\u001B[0m, in \u001B[0;36m_DeferredImportExceptionContextManager.check\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_deferred \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     88\u001B[0m     exc_value, message \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_deferred\n\u001B[0;32m---> 89\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(message) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc_value\u001B[39;00m\n",
      "\u001B[0;31mImportError\u001B[0m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."
     ]
    }
   ],
   "source": [
    "from optuna.visualization import plot_param_importances\n",
    "plot_param_importances(study)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 200/200 [00:57<00:00,  3.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "rng_key = jax.random.PRNGKey(seed)\n",
    "epochs = 200\n",
    "num_cycles = resnet_config[\"num_cycles\"]\n",
    "# num_cycles = 6\n",
    "batch_size = 32\n",
    "lr_0 = resnet_config[\"lr_0\"]\n",
    "# lr_0 = 0.005\n",
    "hidden_sizes = [512, 512, 512]\n",
    "num_blocks = [1, 1, 1]\n",
    "weight_decay = resnet_config[\"weight_decay\"]\n",
    "block_type = \"ResNet\"\n",
    "dropout_rate = resnet_config[\"dropout_rate\"]\n",
    "init_fn = hk.initializers.VarianceScaling(2.0, \"fan_in\",  \"truncated_normal\")\n",
    "torch.manual_seed(seed)\n",
    "data_loader = NumpyLoader(NumpyData(X_train_outer, y_train_outer), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_data = NumpyData(X_val, y_val)\n",
    "\n",
    "bnn_model, state, val_losses = train_nn_model(rng_key, data_loader, epochs, num_cycles, lr_0,\n",
    "                                    block_type, num_blocks, hidden_sizes,\n",
    "                                    init_fn, weight_decay, \"relu\" , 0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.05866165831685066\n",
      "Test RMSE: 0.5023300051689148\n"
     ]
    }
   ],
   "source": [
    "rmse_val = eval_nn_model(rng_key, bnn_model, X_train_outer, y_train_outer, state)\n",
    "rmse_test = eval_nn_model(rng_key, bnn_model, X_test, y_test, state)\n",
    "print(f\"Val RMSE: {rmse_val}\")\n",
    "print(f\"Test RMSE: {rmse_test}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "outputs": [
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x7fe2f700f070>]"
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAGdCAYAAADqsoKGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAADm1klEQVR4nOy9e5xdxXEn/j13HnqNNKP3DNYiLIPAWcsQNooX4mwwxAnLsnbsgGHtxN4Q8PrjELy/bOzdkNjBjiGRWTshq3jDGhwbOwYUDCEiIMfB4JBAbOMHID8QIGOBpNGMpLkzmofmce/5/XHvPdPnnO6q6u5z7r0znPp8/LGYut1dp7qqu7oe3UEYhiEKKKCAAgoooIACFjiUWk1AAQUUUEABBRRQQBZQGDUFFFBAAQUUUMCigMKoKaCAAgoooIACFgUURk0BBRRQQAEFFLAooDBqCiiggAIKKKCARQGFUVNAAQUUUEABBSwKKIyaAgoooIACCihgUUBh1BRQQAEFFFBAAYsCCqOmgAIKKKCAAgpYFFAYNQUUUEABBRRQwKKAzlYT0GwYGRnB3Nxc5v2uX78ew8PDmffbLrDYvw9Y/N9YfN/Ch8X+jYv9+4DF/415fF9nZydWr14t+22mIy8AmJubw+zsbKZ9BkEQ9b0Yn9Ja7N8HLP5vLL5v4cNi/8bF/n3A4v/Gdvi+IvxUQAEFFFBAAQUsCiiMmgIKKKCAAgooYFFAYdQUUEABBRRQQAGLAgqjpoACCiiggAIKWBRQGDUFFFBAAQUUUMCigMKoKaCAAgoooIACFgUURk0BBRRQQAEFFLAooDBqCiiggAIKKKCARQGFUVNAAQUUUEABBSwKcLpReM+ePdi9ezfK5TI2b96Mq666Cqeffrrx90888QTuvvtuDA8Po7+/H+9617tw7rnnRvhvfOMb+OpXv4r9+/djfHwcn/jEJ3DaaafF+piZmcEdd9yBxx9/HLOzszj77LNx9dVXo6+vz+UTCiiggAIKKKCARQbWnprHH38cd9xxBy677DLs2LEDmzdvxo033ojR0VHt75999lnccsstuPDCC7Fjxw5s374dN998Mw4cOBD9Znp6GmeddRbe9a53Gcf9/Oc/j29/+9v4nd/5HXz0ox/FyMgIPvnJT9qSX0ABBRRQQAEFLFKwNmoeeOABXHTRRXjTm96ETZs24ZprrkF3dzceeeQR7e8ffPBBnHPOOXjLW96CTZs24corr8SWLVuwZ8+e6Df/4T/8B1x22WXYtm2bto/JyUl87Wtfw3ve8x687nWvw5YtW/D+978fzz77LPbt22f7CQUUUEABBRRQwCIEK6Nmbm4O+/fvjxkfpVIJ27ZtMxoX+/btSxkrZ599Np577jnxuPv370elUon186pXvQrr1q0zjjs7O4vJycnof1NTUxEuCILM/ofBl1H98ucx9uUv6PH7n0X48G7tuJiZRrjnXuDIQW3b8PvfRfj4w/p+x8cQPnQPUD6ub/utf0b4vX/Vtz0+jPDBvwGmJvVtH/sHhD96Ot6mTj8OHUC458tAZS7db7WK8B//DvjJ82ZefO0BhheHDLz4DsLHv2bmxYN/A4yaePEYwu99g+bFySktXeE/fQXhs8/o20a8qBC8eEHf9oUfmXkxfZLmxd5vI3ziEYYXI2kcgPCb/2TmxbGhiBfatv+0B+Gzewle3KvnRaWC8Kv3AwcIXjzy9wQvvgwMHdbz4plvI/zXR2IyKuFFEAQ1Xjz1LZoX0wZefH0PsM/Ai4M/4Xnx0n592+d/gPCRB428GPubzxG8eBLhvz6q7/fEaO17xsoGXnzdzIujR+q8OKnnxaMPAc99X9/25RcRfuU+oFolePHjWH8yXkzV5GJ4UP89T38L4Te+7saLb3wd4dM+vPiBkRfVr9yLULd2VuZSvIjhn/sBwkcNvDg5VdsLTLx46lsIv2ngxViZ5sW/PorwmSdpXsxMp/ig+73P/2zAKqdmbGwM1Wo1lcfS19eHQ4cOaduUy2X09vbG/tbb24tyuSwet1wuo7OzEytWrBD3c9999+Gee+6J/vvVr341duzYgfXr14vHlcDkj3+EYw/dg/H+V+GUX/31FP6lq/8zAKCvtxcr33JlDHfi7+5C+cufA778Ofybv39S0/YPAQBrzzgLS8/eHsON/O0XMP7ALpS+9gBO+et/iOGq0ydx8P99AgDQf8dD6Fgb/+bhT9+Ek995Akue/z42/Mn/i+FmX34Rg3fsBACc8rdPIOjqinD9/f049Pv/DdXBg+g5OYnV7/+fsbZT3/xnHL37NgAwfE+dF32rsfLSy+O8+NsvMby4ocaLM1+Lpdv+XYIXd2D8gb9B6dEHccoXvxLnxckpHPx/N9fo/8IedKxZF8MP/cXHMf3db6Br/4+Am/4v+vv753lx4McY/MJf1Hhx/78i6Iyry6Hr34vq0GH0zJzE6vd9MM6Lb/yTiBer165Fz3/81Tgv7vtrhhcfBQCsO/OnsOR1Px3DHb/3c5h48Mvo+KevYOCOB+O8mJpEpcGLv/4HdPStifPi//wRpp/6Jpa8uA8bPv4XMdzsT17A4Bc+bebF712N6vAR9MxNY/V7/0ecF088iqO7bud5sWYtev7j22O4sS9/AaNf/jzw5c/TvDjr3wL9/bH5O37PZzGx5z50/PM/YOBzfx/nxeQEDn7mf9d48aV/REdvX5wXt9yA6We+jaU/eR7r/+j/xHAzLz6PI1+s8WLT3/0rgo4EL/7nb6J6bAg9lRmsvuZ3YrjJxx/BMQkv1q1Dzy//SpwX93weo5/bSbT9WI0XP7UNS86KHySP/81nMfGV+9DxL/+Igb/aHefFxDgOfqYWyu+/62F0rIyv10O3/CGmn/kOlr60H+s/ekucF/v34chf/18jLw5+8L+iOnIMK6tz6PvND8R58S8Pa3nRmMMGL9Zs2IgVb/7PcV7s+hwjF380z4szXxfnxd2fwcRX/w4dT3wNA7ffH+fFiTEcvK3Gi4G7vobSylUx/JE//TBmvv89LD34Itb/4Z/GefHCjxRefANBR0ecF7/7HlTLxzFaraD/quvivHjsH0VysWZjP1ZcdGmcF3d/FqP33gHcewfJi/X/9mx0n/FTMdyxO2/F5MMPoPMbj2LgM/fFcJUTozh0+6dqvLj7EZR6VsZ58cnfx8wPn8bSQz/B+o98KoZT9bDZ4JQovBDgbW97Gy69dH7yG9be8PAw5ubmMhsn7D8VAFAZPIjDLzwPLF+h/V35aw9ifPsvxP5W2f989O/Dhw8bxzj6T/+Ijg2bYn+b+8FTAIBq+XiqbTg5Ef178Bv/jNJP//t42+88AQCYfuY76baKcXr4qW8jeNVmBEGA/v5+DA4OojJ4EAAw/pX7cPJt7461rR4+GP370AvPIzDwYvRrD2L8370x9rfK/nnPHcuLdafEv2fv92rjjxxLf8/EePTvwW89jtLr48bh3He/AQCYfupbtd8MDiIMw1rbwwlenHJqnOah2ljjX/lbnHzrr8VwMV7sfwHBsuXa7yk//BBOnHN+vN8X5r2PFC+GH/tHdKyNLx5ze79b6+PYUKxtEARYv3xp9N+D3/wXlLb9TLztU98EAEx/9xtpPsZ48R0Ep/ybOM3DRwDUefGf3xnDVQ8pvPjxfgRLl2m/p/zIQzhxznnxfoW8OPrY1/CqnzonNn8RL4aPpL/nxHwO4OC3Hkfp38aNw7lnvg0AOPmdJ2hePP1dBP1x3awcGwIAjP/D/Th56X+J4aoHX47+fejFHyNYshQ6KD+yByde/4Z4v88/Oz8uyYuHUeqNG++N76kMHU5/z1g5+vfgt55A6bVnJ9p+BwBw8sl/oXnxzFMINsZ1szpyDABw4h/ux9Ql74jjXn4p+vehn7yI0pKl0TrTmEMAGHl0D8ZeF5fVyvM/mh+X4sU/fw2lVWsT31PnxeDB9PeMjsz3++QTKJ31+njb738PAHDym49peDH/34ef+V6aF+XjAICJh/8eJy95R+wbqy/P55geOvATBF3d2u8ZeWQPxn4qfqirPCfjxfA/P4JSz+r499R5MXfopfT31OkFgMEnn0BwZtxQnvvh0wCAk9/4p6ituleo3+cLnZ2dYoeEVfhp1apVKJVKKe9IuVw2ViH19fWlkohHR0etqpb6+vowNzeHiYmJ2N+pfrq6urB8+fLof8uWzS+kYRhm9j8sXRYZMmH5WBofMel4GrdmXtmqUxNWbYM18xOspauB0427ep25reLqU7+n8dsI5uY0vFhOj9vAjep4MU9TdWrSzAttWw9e9Ma9FXFeKH8fIeZ2ZjqNWzZv0FFywfLi5EmCFyNWvIh9p44Xygk9LRclpS3Bi5NTadzyHnpcCqfyYtrMi3D0eIpuWi4YXqxYaW4b0xHz92BSo9MrVoja6uRC1fmqTuaItl46ospysm1JkQtKR8bH0rieValx1TFImlS5mJ0heGGnI+y4S4h9ROUFMbdVnf6sTPPCSUdmZy3bynhR1bVVDK9kG11fPv+zASujprOzE1u2bMHevXvnP7Zaxd69e7F161Ztm61bt+KZZ56J/e3pp5/GGWecIR53y5Yt6OjoiPVz6NAhHD161DhuU6G+MapWfgqU01AEioJgVIOvQzim6bd33uIOZ2eIcem2FIQETSzoxm2Arl/19E601dEUxHgxS4zrwQvqezig+KjrVzEOaV6kcTFezOXFi7Lod+JxG6DrV9lQtXii3xgvKhViXB8dsZULxVK21ZEV88YhxQtWR/Liha2OqKkSpFxocCuUsJCPXFSrVm2RCFMawXq9UJhhqyNqWOjEaBpP0BSsEvLCQy6aDdbVT5deeikefvhhPProo3j55Zdx2223YXp6GhdccAEAYOfOnfjSl74U/f6SSy7BU089hd27d+PgwYPYtWsXXnjhBVx88cXRb8bHx/Hiiy/i5ZdrrtlDhw7hxRdfjDxCy5cvx4UXXog77rgDe/fuxf79+/HpT38aW7dubQ+jphFaODll/s30SboPqq0OpxpEtm0NoRDZuHp3uaitBOfSVv2eZvKisyv9N2lbCqcmxtm2VQ0iSuamM+ZFSbiU6Mal+g0YPIVTv8d2XB9eSBMbrXXEQy5U45CSi6x5IQVrHZG1DTlezNjyQh9SF40rBWsdKdF4iqblytxSh2MfuWgyWOfUnH/++RgbG8OuXbtQLpdx2mmn4frrr4/CQEePHo1lK5955pm47rrrcNddd+HOO+/EwMAAPvjBD+LUU+fzE5588kl8+tOfjv77z/7szwAAl112Gd7xjloc9j3veQ+CIMAnP/lJzM3NRZfvtQMES5YhBJqr1KWEICeS+8i2qgt1bhaBaXM2GTWcgYaaAhmX9pA4EZjGpXAxXkzGXLkx0C0WiocorBC5VloDYikwTnhDGv2enDTzgsvvsuWFmqh5cioWRmHbqnJRrSAodaR/Q7Wdmkj/PQGkXFCLqmlcpd8UJHmhhMHYfmO8qCIwGW0mHZGsBeQhyMPw17XtTPLCsDlr5TwecjFWo+jadncDM8y8AvYbuRSvwynFDzg5FT8IcDQphzprXnR0AtQaQ7WV4Fzadi+J402HVk5HKF40GZwShS+++OKYp0WFG264IfW38847D+edd176x3W44IILIk+PCbq7u3H11Ve3jSETg4bS2xo1aqzQ2stDtVVwmrbB0qXzvzg5BfQoSs7RtHQZ4e4Vfg/T1HrBVmlO4VWaNLxYMs+LcHLSjqYly4DxE3o6SZoYyEsuYt+jk4tlilycjG98ErkwGjXNkItsdSQWDp0+GT+VcjQtXUbQquQ4UAYe09Zr4yN1hPEKT5+M84aTiyXLzEaNmtdE0awLi/isFyqkdEQg5w2YmQGWKEaBRC4mDOuFVC50RlGM5sk0vgGSaEEspGTBi9mZuIHUQijefsoCXI0aBUJKGD2sc+1ioZ7AbRcE9aRCxeZ9eGHrHhfiudN8dcpyQVgaP8270MQBudj78EJymrfpN3GCdaGJhSae5mPeS2sdiZ9gnWjioJm86O4m8SRNS5vAiyZ6NgJ14562XLOzCN1z0EyPl2rQ+dCcMRRGTRZQV1x2o6CgmcIobqtR2tgJ1iNM5EpT1m55BUIqfMKdVHwMMQpyMnbZnA2KV8zmlRcvcjPwPHI2+NP8tBNNHJC88NARfb8euTxLEqd5B5o4aK5cCHnBGXhUQcMC4UUgzOVpNhRGTRZAeWqWxvMUjEAJBROHbapSS0+wLC8cPRtcfN6DF6SnJmteSE/ztsYFN64E59I2GZu3ahsvDc2Mpla1lXo2tDI1fwhqCS84mbI9yEhP81oDIoPk83bSEdVT43NYyZKmZrRtMhRGTQYQXaDFxqOpUxuffGsEUqmZfskKGZ+2DC+oE6xtDooCPqGrkDBqtP0GUs+GLnSlLHCUoeYhF1rPIUVTbFyPE6yPXLie5pnkc1IuPDZyvVwoy6rtHKi8oJLIqe9hks9JueDkjcBreaF6Z23nQNER8noCimbmEETLhQcvdDj1hmHbcdXwrqtccJ7qvPaRJkNh1GQBlKcmmWlvAk7gFsUJ1uPUpoAzLxge054aAseNy/HCNjYvBZ+5bebpN3aC9Rg3S5pa1VbKi8V2mmcPhHmFdxeIjjQl1F14agpowFLBPTVavEVVSNKzQWXpq3s+Uy2UPnVb0JQal+pX3pYdN3map6oOYv3SFTKpnBqOJuEc2Mfm5ZVTqdg8STPXr4dckLxg5ELKC87YTfFC+betvJHzx9HkIRfJ6wlibS14kUzi91kvYnJhWSEonAN9Ej9V0GChI8lQN0mzBY8z1RGqX8h5wdGc4oW0WtJ2H2kdFEZNFtCIoVMuUg6vSxxTczaotq64hdRWVeqceEEm73H3yfjwYpbom6LJZ1xqTG7cvGhi2zJhFSp05dFv63SEoCuvcdm5zYumNpRHVm/daSLDaXnKFJWfmRdNTYbCqMkC6sYHuSkCdL4At5FTbZnNmAzXuPbr3dZyQVDL0Mm27jSFVJ4PdzGcK00AMGdJs1pR5ywXHE0+i31ecqHpV9ER0qhxxbnQlFlbS5pVj1decpHbeuGjIznNgQ9NXnxsVVsPw5Ebt4lQGDVZQCNvxkO5yAWZacuOS90n49EvacR5KJeWF2qJratihlXybh16U+S8PPks9qF2cVQ3L8c5qFToajyPhbO5cqE8LkkapbSR1hLDP2vjUA1deRj+JC9aYVxwbXV8VJO1PTZyWi58jIB2bOtjpBWemsUFjddKW2Wds65B1xNsjgu2rXdCimdPV4RhSVVKtOw0n6dXxDHslStNPp42j0WZDMm0yMCjZDWvE3kY0iEKr0NQToZ/Xp6NsMocCNvP28IZ/s5rdqVCX7bK6VcToTBqMoCgYdSwsV0iwVXXVj0lUEmQKUFNnC5s3M1kvz40MW1V0Lal8EKadG3VBD3qpG9tsDLfGuOF5RyQdBH9cm2bQZOtcUHRlOg6taCTbZM6YrHYc3JM8tGjrQq26wUlF0lvBJV8br0pCufAeq2xkXOCF7aHLzFNtgczH5qofhm6KL1OyoWr4d9kKIyaLIAKP8UW3Sa65cm2QuVrqlveQqlzyo9I8zHOJ2e3vI/rVvs98vmLN7PYyK09NVKDNeONXO3GNfykxUs38iZ68FQ+scZhs7xlPsaFEGdNEyCmizLejX079OvbljNc8hqXwIdSnW8yFEZNFtAp9NTklcHvM25uVRQtcsvnZRB5ueU98nHYqhAfY9dxDpjcpNbl4+QUrqHmgHXL5yQXXnlaOa0Xc3P0TeFeOWs+BlFeuYwtOhDmVaWU5z7SRCiMmixAmijc1NO8EJ9XbL5N8y7IBY57gsE5ya41nqf2OjkLcJ40ZeupUXGtycdZMFVXKrhumi2rFvLI/WO9ZY7l0165SS0y/Ivw0yKDyKjJM1s+n428LbPlfRZsD7c8b9S4buQ5bYo+NLFtc5q/HBdOv8W+BZtmq8qn2zGJ38e4yNNgbUXKQK7reYvGbSIURk0W0Ag/5Zgtn9eC7eeWJwSZKxn2WKTyq6IgTvpc30z4oiVu+Zad2nzc8h7hGjLRu0WLPSPnC658OjfPbhvS5NvWdY3L02vl5XkqPDWvHOgS3vxLJv4xyW42GfxWWeuWba0qSpLuV8cE1+S4XjSZ+ch6aqhxs5wDr2oh4ZhAmlextsTccXRRNGn7ltJE60h6/iyq4gia0lVVybZE35Q8hiFzCLIMX6hdkzQR8qbFK7+0DR+SOqL8u6rL03Jc/5K/tKEpCcT6SPara6sC1Za9Q8r9e9LzJ6yY0+KpflsHhVGTBahGjWvpYTPdhjbluV5hFQuaM6PJPWcjtSly5eA247om2WmrhRxpsll0fSpzbE/OPqX+ajdZejOtynPzOs23KLRoc5rPVKZceaHBucpUVhWCXN8sLxIGbVvsI8T6WHhqFhcEpY75q+tdwzWzM4wrOp88Ei+3vEdljpdbPqekXP3tvXG6nGgC3PNmvN6fyTGXx+v9IFcd8QhL+uSR5FUtxLbNKfyU45tGuVX1eH1PXjTlqV/u+4gzTXk+ndJEKIyajCDoWlL7h09CG+mWz2mx96kWysvLw+Ym+ZzmLTw1Fn17vftFtfW5yZN1ReeUm5SXcViZI93yPjdCt88dUirOJzcpp9N8O1by+eQmtaqgIa/1ol2LVZoIhVGTEQRKBZQRmul+jeFbQBPbtg3d8lSiKUdTO3p5WpZMmqcLnDD8vUq62yQ0LO0XyE0u2vMOKaJtq+6QyjMEmNuBsEVy0UQojJqsoBF+opSLEjYO74prFU1sWyZ05cwL937DvGgCPOfAsW3LaPLhI9eWSOhtGU35fM8rTy5aMH8tk4s2XLPzpKmJUBg1GUHQWffUUFUjKaFIuAkrRPJsst9YxnsSlxgmS5qoflMVPxY0p6prKJqJLP3k91jQFFLVWhxNVHUGR5ctHyFsy/VLzb01Hykc9z1Cmhi6wnahiZyfxE995s9Vr6l1JnOaKJyPTHF9O/bLrH8hyWNuXDMfUxEj1/UvgU+Fopq2j7QOCqMmIwgiT01engCibVilSwA9ThtkPofXqTq/UygZU87tJMl8j9WmadE38z358aINvRPMcwYtoall3swFRhPbtlVeq3xoytcr3IZeqyZCYdRkBZGnxsKzYYP3EbiW0eTR1urEl824XMw/5QnIiCZ2gfOZP1Iu3L+H5JWHTLF5F65tWcPfhxc+OkLkoOSlI6zh7z63+emIR5EFMz90knHz16Fa3y1Ysz1klaWpiVAYNRlB0MF7aor4uhDH9Z1XWx8DoWUnr1bF19svX4DcULm+W+FJY9u2oXepHdcwtm1+nlAK/IxdH4+/Ix/D0MPjX+TULDoIugSJwj4WtscJlhbynE5Avm19TgWO47JeAoomr1M1N67H/LnyMc9T6ILTkRadqr3kIi8vT35ykZeXJzd59KEpz7Z56QgpF0VJ9+KDRvipFVnrbNu8aFpcJ1ivk347nqo5/EKrVGHwuZ1g21BW2bZ5eR0XW/UTO+7i8ma2bB9pIhRGTUbQCD+lKzCoipJEJ6mqBKKyg6wcsMh4T8bXLWjiKgesTl4W1TWp+DqZpW9TYZG4uCrLygGqmoiTi0yryHKiiar4sZJHRi4ouri3a1x1xLaKjHyLJ6PKNg7P8nHWjLOStwRkWTll8z1KW7bih5oDdv2jcE2qBLPZR2xoSuEzlIsmQmHUZATR5XuvqPsu2pAmti1DM/Wadstc64srvk7fJ9OqihKibbVKv7Lu8T1+1YUecjHnXi1JJxm/gjwbuVZaLrB9pKh+WoTQSBR+JVUatSNN3uO2YQ5Kq3I2XNt65Wl5VD9xN7z65ETllmScV56WjzxyfHStqMsp/4vBs2FlV/0KwxwPQTnxIq81mzP8mwiFUZMRRJfvFfF1Ydt2Pam0Y7XQAmvbjqdq77Z5eTPbcL1oVd4F6T3K8dCQl2ejZRV1bbhmNxEKoyYrkDyTkNfJmGvrc1psx+qndjzB5umd8FpoWuA5bFFlh88dN15z0DIdaZUn1FE38zIQWjnuAtOR3KqfAH4NbBIURk1GEGTw9hP9GnMRXxe3JUMjXFvXE7nHhWZeNBWn0AbwJfktOMHm6mFow9BwK/pl27bIa7XQaOLaFp6aVxZERo2VYnLvLMWrlKim8Qx+i0z6FF0WNCXpsqraoSuYyGovthrFTBNbRabygv2eBA1qTJmiKQmMXKSMUtc3cUKmX5Iui4qfZHzd6w0ZjmbH6hpOLsi23NtPKi5ZXejx3lESrKoLKTxNU0jqvF2FjMqL1BmAmlt27aRosqlS4uaWWh8ZnW/Gu14+NCXHtd1H2qQCqjBqsgJRTk2LTtXtWGnUrvdd5JUYndsptFVykZMs+1Q/sZ6aBaabXDJpTvk47LtsPp5QKpnUw7NBzr2HZ6N4r0/Yr6TvJkFh1GQE0TMJecXXF1ulUTvSxOGL+LpCE7fAFXla831Tm0yrbqnOycBrmY740FS81yfC+8hjE6EwarKCrixuFG7VRu6xwOUVX8+NpjaMZS/EaiGvTTOnu1nyognwmwOPPK3W0NSaQwN9f1Gbrn+5rdk5yUWr9pEmQmHUZASB6JkEjxdhfRapPE8qzndWcEnGed0pwnknfBY4Rxd4rsl7eZ34fDbNfLwTITV3HE15VT/5tG1Xj2RuxoePjiww76xPW59Kyzz3kaL6aXHB/CvdFhsUl0yauMI/jksmUKptLZJUGZpScVQqCY/9ngSevLiKaJv6nuTzDB40EYpJJqmm+rZIrE3G122uNmcTay0MIioBNikXVpuMTdI092wHQTOTfE59T6aeDfIKf4u23CGoovZrm+A6Z0SlaUrgM/M6cgm9RL+50cS0tShKsDqsWCafu9PEJE270qSjq0VQGDVZQZchp8ZKWHMq++Xen8nL5Wh1yvF5NyU7mkjjsFXxdZuKH4ImfuPjxrVo61OZ4zgul9Rp81RFuuLHfdzMckW83haKQ3bPdhAHCtu+KR2wSqy109sYL6x1hDoEWVQaJYHaR3K8PsJOLizXxyZBYdRkBPOempzi4G1b5dKCtq3ajFvl0m9ZPk6zjLRs2vLPJORFUxvmROWaj5NPpZHfYSWncHWr8nEW2z7SRCiMmowgyqnJq3qjXU7VWY672HINcouvO1bPcP3mmVOTVy6Pl460Y6VRTu8h5Zqn1YLqJ7atT2HBIqs0akeamgiFUZMVSG4UBtyVj02sbdGpuhVehLyqTXzb5pYMnNPJOM+E6xZ4ERZd9ROHb0ePZDu2bZUXLs+rGjz2EWeaipyaVxY0PDX8kwStqTSiILcXYfN6r4qNr7dKqW2SjBPgfPLyiK/nmXdByUVe3iXO8G+V968VngAfr1WrvBN5efC8jItWeUJzmr9cvfZF9dPigk5pTg2Rmc5U9dDVNTb9etBk9cRChqdQii6bK+CzPA3aVB2wc+vDR6Wtz/MMFtVPudHEtrUx/G14bGHg2TwNwNDEV9QJvTzJMcOQrqizqpa0eF6DrcyxmVuP6kIVUs92+FQaJf4zp2ohq/f6yLUxzcO44W9XmUhC4alZXDCfU8OVzFEK1KT4uhVNTTp52VQGcPis3PJZ0pTlS9zOVRRNNPAWmjzmVJHlRVMKz5RA2/RN6o9Fv1zJcFY0AUwYlqAJkHu5M6x+ytQrkuk+4qojzLUiRU7N4oJA7KmxOBXY4FsW584r096n7UKscvFZ7F3lYnFVP7H4vE6h7Vj9xLbNSy5a9T2tqhZqw/WiHeWiiVAYNVmB5EZhoDUx2HakicO3Kr5O5j8ssvh6XjzmaGqZPFI0+cxtTrljgPvccvjcclBylPO8PKEt05EFtmYX1U+vLIg8NR6naq9LvMiLq0KPF2GZy5pysuz5F2EdTxvJ+LpVvxwv8jnxWcXXLfpFhauoW2Cnap9x2YTrFpyMvdu2wjvhTlNu6x/btlXezAXmIfehqYlQGDUZQSAt6c6rEsLLi7DITrB5LQjtEl+36ZuNr7tW1NH95lY55eVpI9qGIfNsh7tc5FZdmNfmxRn+HnpL0pyrXLhv5LlV1OXmUc7Pg+dcadlEKIyarKBD6qlR8RbvAyXxVMWPTvDESs0lfDrSpKPLatOk2hI4bdIg8TZUZosUVyFD0ZTEWYxr/f7MPD79NEA2RlpKHK1CBWZ50y6wVHUNJxeuCdc2Os9WTiXHlW5eljpPvY8GMDRbbKjkHNhWkRFt8ywAEK/ZOVbUWdAUUjQl29ruIz53cTUJCqMmI4g8NdQJh8Pn2TZ0bOtDEzVmCp9QoBbxkSyDbde5lbbVLVKu4yZxKV4Ri1+qXws+U/1ybVl5JEqG20FHktPXKpkKCZlabHLu05bVkTZcL6zWbMtxmwSFUZMVlJjwU6nOap2l29EhbKvBU/0GyvTato1oMmwirjTF+vZpq8FLaDK1de23KW0d5kDSr09blqaMZUrclqBZd5Js9Mu1JWly0PmmyJSjzpva+siFiCaf78l43RWvFy40EW19ZCqvfYSTC27umwyFUZMRBI2JN1mrDUHWnTRLwrbaxYToN4Ai6Jq+RTSZNhFBW+MG5EiTdFwdH4PSvJFny8e8aFLxFE0cH21pVo1d2/nLhBc5trXWEWbzarmOeMiULc0NHEDrZqvkwpYmn3FVXpDz57CGUXtFJnPr0ZaaW8Dte5sMhVGTFUThJ5Mg1/DaChnuFNpBhLYa/bIKpBuXuFun0S4M9UmDUVsCx9CkpZl77ZziFcVjta12XL5fVKv6PA5JWyeaZDJl7RUJwMyfTGb0vCBopvqN9U3QxOoIPX8pCALh91I0GTYRcn6YcLWr3nJ0UW2DYN7Is50/oc77rBdeOkLqvG6jFnpqbGUGaqqCj4547CO2c8AZeJw8NhkKoyYjCFiBIhYL1rKnFhouHEBtuAKauHFzo8nhtEHRBDDuc+KU06F6NixPKhxN9bZ6Y5dz6RO8KhGGI0eXlBe6+HpEM3WqNm3kpXq3DqdQileNfl08hxI+hYZqoQx0JLSlSaWLGJeVC9JzSM27wdiV6IhJLkT65SAXpAePMXYz0Hkn72wr9pEATHiK+d4mQ6dLoz179mD37t0ol8vYvHkzrrrqKpx++unG3z/xxBO4++67MTw8jP7+frzrXe/CueeeG+HDMMSuXbvw8MMPY2JiAmeddRauvvpqDAwMRL85dOgQvvjFL+LZZ5/F3NwcTj31VFxxxRV43ete5/IJ2QNnYesWhIbymzbbxtqgFfQwjosptbKoiFyOBE3JccPEuGKaFLxksUjxguAVRVOo44XFHKjtGjQrXiwxTclvTbbV0eTk0if61bWN0Yx4W+p7gJrcqP8tpslisZfQZBxXQJO0LUUTUONzY+FPyaPN3DrqiGpHiHTEkFhb6gAwFx83+b3cISisAkFCLlwMhISOhJUKgiRNTuGnRL+UPFYq8jU7SVPdyx2oXh/UDsAhQxN/CKoqvBDQpNJFrNkhpSPVqn0orgVg7al5/PHHcccdd+Cyyy7Djh07sHnzZtx4440YHR3V/v7ZZ5/FLbfcggsvvBA7duzA9u3bcfPNN+PAgQPRb+6//3489NBDuOaaa3DTTTdhyZIluPHGGzEzMxP9ZseOHahUKvjIRz6CP/mTP8HmzZuxY8cOlMtl+6/OAQJ2MyYSiVnrXODG1vWrnjZ0C5yEJnZcS5oAWvm4tqSXh3PLOyZ9dii2v+38cTSRSYPptrFTMMUr3fdETQPhadGBF7Y0qXRRvKJoUtrqPV5UyAV6uUjpCLOR2+qBOIRkSZNKl+38xMalwhuakuGYXBC80iZrZ6HzhjJm23BNpF8BzUcJTSl8vW/qSR3jAbdZ+wjjtbLdR1oA1kbNAw88gIsuughvetObsGnTJlxzzTXo7u7GI488ov39gw8+iHPOOQdvectbsGnTJlx55ZXYsmUL9uzZA6C2SD/44IN4+9vfju3bt2Pz5s249tprMTIygm9961sAgLGxMRw+fBi/8iu/gs2bN2NgYADvete7MD09HTOOWgrSDdUpjtqC8FPsFOoYfnJxv7LuZEe3bwDziRAMTg25WOegCKtCnMJpklCPy7hCXtiGEoThJ5oml7ZSHbENIbVaR7i5tfyeIBDqCBOWJHNQLEOWsbY5hZ+yXrMZHSFTFfKiiW2b0z7SArAyaubm5rB//35s27ZtvoNSCdu2bcO+ffu0bfbt2xf7PQCcffbZeO655wAAQ0NDKJfLeP3rXx/hly9fjtNPPz3qc+XKlTjllFPw9a9/HSdPnkSlUsFXv/pV9Pb2YsuWLdpxZ2dnMTk5Gf1vamoqwgVBkP3/lOQv9e8R1I2eoFpN4ztkbaG0pXBxfE0Yg9CybalUW+TqNDf4BiRpVvtN4CommjpTNAUJnB0v7PiozkE0sLLopnkxv3lJ+djgnZGmIN6WlAuGj5RcBDqc8r3xcc39pmgy9a37Xo/5SdNkMbcpHNM2Jo8UzQmaWF5QcyunKSkzPuuFVt4ARi7mNzZSLkgdIfhY3xRNek3KBcNHhOb1gpULcs0maIKFXHjpSHb7SKCjCdDuI+k1uxr1qY6fxf9swCqnZmxsDNVqFX19fbG/9/X14dChQ9o25XIZvb29sb/19vZGYaPG/1O/CYIAH/7wh3HzzTfjPe95D4IgQG9vL66//nr09PRox73vvvtwzz33RP/96le/Gjt27MD69euFX2sHldGR2j+qVfT390cTcaijAxUAnd3dmAOwasUKrKznCp1YtQplAJ1LargOBLE8opcCAOF829WrVmF5HV9e0YMTSr/dHR3YWMdVujvRmI2O7m5UAKxdvRpL6vijy5ZhSmm7tLsb6+q46dFjGALQ0dGJSkcHMDeH9WvXAgD6+/txpLsLM0rbFcuWYnW97eTqPhxTcKhW4nlRDV4sIXjR3eAF4ryo/3+j7ereeV6M9KzAuIEXc10lHAYABHpeLK3zYsmSOi+65nlRHq7xorMTlVIHUK1g47p16FizDgBwpLu7xos6TT3LlqGvwYu+1QleVOO8KCV40dOj8GJlnBfBPC/CMMTLDV50a3ixos6Ler9dnQovhgZrDYMAHV01XqxbsxrdES+W1nlRa7usuxtrG7wYGcIQgM6uLswFARCGcV50xeUixouGXCzRy8XBjhKqSttVPfNyMbZyFUah15GwWk3xom+VhhcNuejsxIYGLzqAw6itLaXurhovVmt40d3gxZJ5Xhw/Ms+L+vgb169DR9+aulyYeTHR14fjhFwcLNV5oZGLsZUra7yot+0MFF5U5uZ5UW+7pncVlhl50THPi6A6zwtSLmo6smzJvFycPHYYwwA6u7rnebFuPTp6+wAAg11dmFVo6lm+XOHF6hQvgNo6E5OLetvenh70mHhRUngxO5uSizW9vQovlsd0JCYXqNR4UQoQdHWhCmDdmjURL4aXLMFJpW2MF0cP1XjR3R2Xi1VxXjQ8NSuXL0NvUi7q/QahQS7qffeu1PBiiUYuZmdIXhxfvgIT0OvIbDiHwbpcaHmxdGmNFw0dWbIEa+tz15jDVoBTonCzIQxD3H777ejt7cVHP/pRdHd342tf+xp27NiBP/7jP8bq1atTbd72trfh0ksvjf67YWQMDw9jLuPrnIMgwIYVy6P/PnzwYCS4lbq7bq4efxwrlzF+uLbNVut5SHPVGq4yO4PDdRyAKGbZaDty7ChG6/jK+HgMN3NyKmobjhyLuqjUw57Hho4g6K1tQJW616rR9uTE+Hzbo8N1uueixMfhwUEMrNuAwcFBzNbznBptJ8bGcLLxPSMjMRzCEIcOHowS5Sr1hxcb3ztWHpnnxdhYrG2KF3VotB05eozgxcn57zk+3GAmKnX8saEhBKsOx3lRbfBiYr7tsWN1XlRqvKhWcOTwIQTTtWvi5xq8qLcdHxvFVMSL43FeVKtxXtTjzxEvRhRejJp5oebUaOViYiLW78xUTS6CIMC6hl82DNFwFB89cgRBz2otL6ZicnG0hpubqydQzuHI4cPzvJidjdE0Pjam8GIk1i8qFRw6dCjSyWo97BDpyIhGLiIdmZ6nSXF3z9W7LhM6Mj2l6MjRIxE/GzpydOgIgp6+WtuTJ2NttbyoVGon2EpdLqama39P6EhMLsoJHalUYnJerSR0ROXFiROxb52bUeRCWdMabY8fPYpSUi50vBgemudFvY+jR44gWF47aKblIq0jc9VKrRw8rNZ4MTkV50VDR0ZH03IR8aL2DYODgwjDMMWL0ZHjOGFYL+ZmFLlQnnFo4I8fHU7zoqrhxVCdF9X5R0OOHjmCYNnKWtuGXDR4MT4vF9Wjx2I4ADhy6BCCiTov6jrS8GycGB3FZFIu6m3DuYRcVOM6Mnp8JM2LaoMXilzMzuelkryI5GIyzYswRFj3yxw9MohgaU+cFw0dGT+BwcFB9Pf3R3OYFXR2doodElZGzapVq1AqlVLJueVyOeW9aUBfX18qiXh0dDT6feP/R0dHY8bJ6OgoTjvtNADA3r178e1vfxt/9Vd/heXLa8bDli1b8PTTT+PrX/86fuVXfiU1bldXF7q6urQ0ZcnsCJT4ejg3G7/nAEDjxuFwbjYaP0xWC5keT4vaqvgwhlPbzv8miOgI5+bS13Q3+lXKMGPjNzLi64tEHJced77CYl6swrk5IDkPjaoD5XuinqN+6dLQsDKrjGumSf8987wIk4mOJl50dABzs3E+pipK1HETNKF2mkaQ4EVJx4sEH+dM36ORC00lREwegFjuRJwXSZoqaZoQ1GLolbpcGGRKOweqjjQMAk3bsKLjRfp7UvOT+B4RTTDxIj4HISVTlUpCLpLjVsxyEVZRrVRSFTI6HUlXgjFyTummUefT60WE1eltmJCLuapBLgiaGzTV74Fq/C/1PdR6MWf6nrQOifio5OCp35PUEejW5EbfSblocKpDJ49pmqpKOCf5Per6N89HTi40a7bHPkKt2ak5bCJY5dR0dnZiy5Yt2Lt3b/S3arWKvXv3YuvWrdo2W7duxTPPPBP729NPP40zzjgDALBhwwb09fXFfjM5OYnnn38+6nN6unYCKiVL44IgsmBbDsakwcSCrk2U4xIoJUmdFm01G58WyORZQZWEqa2uXylN5CVePsmxmkQ5zcJKVywQNHF46dxqNnJ5wmF683JKOJSUxlvdfSSY+yxklU2sVefeclxbml10RGPgacGHj7al16EG78MLMslYiAvTeG1VHKsj/JpNloOn6BLsBcnS+CRdovWP3gvIC2A99hHjPVBNBuvqp0svvRQPP/wwHn30Ubz88su47bbbMD09jQsuuAAAsHPnTnzpS1+Kfn/JJZfgqaeewu7du3Hw4EHs2rULL7zwAi6++GIANcPkkksuwb333osnn3wSBw4cwM6dO7F69Wps374dQM0w6unpwc6dO/Hiiy/i0KFD+MIXvoChoaHYfTethChRGLDfvNjbiB2NCyCWbGrVbxDQgi6hyURXpyNNaltrA89QltgA9uIqx/njNi9JeWfWNAGp5MzM2lI8jumI5fdKdcTH2LVd7NVnSKwrZBhjVyQXhg3I+cbaIOaxNLbl1hodXUIdCXVlwZnoiCVNXN/cA8ZE24D6nsW2j7QArHNqzj//fIyNjWHXrl0ol8s47bTTcP3110dhpKNHj8ZcZmeeeSauu+463HXXXbjzzjsxMDCAD37wgzj11FOj37z1rW/F9PQ0br31VkxOTuKss87C9ddfj+7ubgC1sNf111+Pu+66Cx/72MdQqVSwadMmfOhDH4pCVC0H4yk0gdduqB4lp+wFbYIbKF0WewlNxnE9bsUkHwnMyfOk4qnv4e4yceWFy/dQ/QK0EeDzaKWEJiNeQpODN5M6ccfwPjri+FaScVwfT5qPjvC3VDuVDEv6NdElaetyGzHVL8Csfz7XBNjoiD5cvaD2kSaDU6LwxRdfHHlaknDDDTek/nbeeefhvPPOM/YXBAGuuOIKXHHFFcbfvOY1r8Hv//7vW9PaLAiCetyxWrU/Ufh4CTI5hWbs0i/VS5rD0F4JOBdq1mGGOgQdnQghcCdbPwzK3N+Rxfc4PfTn+EwCR5eEJhPeQ0eCUv2WVp+Qi8sp1HX+pOEnDU3kt8Zoor7HR78sbqmW4DjPbiY6kvWhwWOtkRq7i2UfaTJYh58KIECyucWu3K7/v7JYxJKrogQwXb5Hol9T3kXWJxWCplAbX7fNu0hvtrqEUH2ugWajjtFEzAHnCRCdnDUx8CCg6dJtqLnRpMFrbwjV4dRxBR4VXew9YB5M1Mo5ITM6mnTjUjSZ8CROlUfJ9+ieSTAYuza5SUaabHWEydPyysGjvifNi1BHl+U6FKeJmD+XQ4OEJkBmxFHfaqKLWhOofmNtm7yPNBkKoyZLIK3Z+ezyFIivW7fsV4n5k9fHm0rcBVeB65PDgvnF3vaK+OiyrJB+2NCWT0pbbdxe/NyEef60/XI0U1exKxeH0S+l236PSrMlTRwvIrkwtXWUC031U7yto1wIv4fdyF2u/3ddL8TJy814kqAOgTBnTTsHpXljV8dHH533eRqgRMgq+T2qnGs8NVROjXLpqfM+4sULeh+xbtsCKIyaLEEnVClvjIMr2jUZEdB7VMSPp9XL+JzGtfPGpE5PAH0aybqygzpJArIYedauW61c6E7VwrwY1uOVxFmEKFInWJuHNAXyGOOFo1wwsho38Gy9ExlVyEQ44uTcoCkMDcauRFaZChnxiZzxBCT7NepIyYyXyEXM8JdWCBKeC+m4NnMbJnCsUeq4j3CGv88+4hK6ajIURk2W4LpRqA+g2QoNu0i5VgspbZ2qDqhseUG/pr5F32M6PRE0+SSiUlUf0raZl4MLaXI6/XoYEBRdku8x4mWLvZ4m3jDRf08gGzfrOchNRziahJ4AW5pUvLVnV2jsetBk/Ugqg294aozGbhb7SGjLC9k+4lQO3mQojJoswTE5LGA2r4A40VE4AORpkExYA5y/pzau2bIPJP0a+g4I5QsobwpDkzjJjqSJO3lpaJbywlWmXORC3NZOVgGAykGR64jd/DVFR3Q0eySiknPAJFxT8uhFUxN0RLfRt05HzLki8rbZHoIy2UcouchDR5oMhVGTJbhWyATKNNhuuOyiS3ggxOXTVFufcsgml4NLTsaZhdM040qTY5NjGts69itt6/Q9Dq51G5pMeCq8wRm7klwQn3Jwm0TUqC0fyjGPm5eOSA8ywgTyWN+UHgjLwcnvdfDOZlFkQRlpDoegTK5McNpHHGlqARRGTZZAJkESOQ4B6KQ0rcKHPA6qFW1JE6AI8lwar/1WLvs/fZ13Chrl4GpbbVWBPU36EwXVVgGyAoOgyYQnb4dN0GSii5o/sbGrtk3Sa5ALa1lVgEz6FFRnqHi22kQ4tyKa1LlV8QSvGN2k9UBgaKltuZt9o5wMjqYM7oHS6QFFk4IP5wgdmSN03kSXdv4E6xBAr39inaeMQ9PaKdEvgiYVH5NV933ES+ebDIVRkyVQFitZLaS0tc5BoSpkuJi/sLLDtm3AjEvGlAPhgkDwKVka3wDR9xhuI3Ytn45VUVBtTbygSjhdbwUOaJcxdZKM5V1Y0qT2bS1TSoWMdTIpwwsvHaFo4nSEkClq/oReKxePJBmGyOLGWlMuj20JdITjbql21xHn4gCmEiwgw0/SNdslB0+wjzjljjE60mQojJosQSdwksoOta1rEiRAJ8qR9+P4LFIuSZAETeJxPXghdc3GaBIkyrEJlA4VMkm54LwESZxaIRPzwrlepT/flqyKs8lBSb4hk7VBpMiquBxc8k6POq4HH+0NSybp3atCRmBMVTV8ivXtXiFonYiquwfKmib93Sy0sStc/2Jt631TJd2A+z7C3QPluq5CfYTTYW9rMhRGTZbgGrNk7jUQxVi5cW1xsdOGy421GeQa2J66KV74VnZIvDxZb8bquNY5KIxcULlWPt6JLHJQdDRJ54/Nu7DNU5DpiPzBRF3fZprMBp5jIqrKi2SFTGwdcn+SQG/sSpNjqbuPHLzckrk13gMl8Hhxic+WhR+1toKcqLz2EZOx65On1WQojJosgRIoj2oUuXfC1hUtW6ScTqE+Ro1Q+Yw0mfBZJMf6PEngYgSQm5fQ2HVd7L2eSbDw1Ej65ejySazNKznWp0JGahzaGlNciMJH5yUJ16wRkPUhSEATkK2x60MTh+e8mVnsI7pycEm/pn2kyVAYNVlCXhuFOGkwY8V09RIAjNvX5WrzRNvcvBMuTxK4V0LIE2ttFxqfvAvGwPORi7weTJToD2Cg2YOmLHRemjQtHddLR/LyZkoPDT46YimPueoIcaOwV6VlBvsIqyOWXqvCU7OIQaSYsmqh+HtHRJWLrkKGq4SQ0kR6npjKKV22vPRGzVR1gLSqQFoho6ui0OBImpS2LpUQ0rYUXbaVECpQ1UJUtZaxbZImk1xIvkdDk7YtN7eJftW+s6KJqmThKqds5SJGV7KtsNrOZb0QV9Tp2gr6BaL1Ilb9JF4vbNepRL+mtllXgkU0EXzi+rauOm3VPtI6KIyaLEFrsTaEJoMqCmOWvuHUYKywSNKkt/rJ2K/01llxDopa3k55eaiKkg7m3RRL16zyECN5sZ/4bSHLuQ2CtEzpkn2pe050bXX96mgyVYLpZCpMyBSXp+WVX2T7NABxG7Gaq5M5TVRbIS+4BwbJfA+GF863VOt1xKsSzCdnLaUjwnG1cqFZh7zWv/T8kZ4ao45kuI+IeaHS5L6PNBsKoyZDaGTL218lHdCKK62QcX2SwFgO7v9Mgi5RThxTduKFqS2d+DxvwDEVMplVQli2tZ2DLCpkuEoIr3yBbJ8koF36wrJ6a5oyqpBxqqhznL+gNG/4kzpiH3IRyYVJbxvXPHisNfZhSe6OG5/KNgLPVj9JdN59H3ELP3nsBU2GwqjJElzzYtS2zaw04hLlJLF5p7yYLBZ7jwRKNuE64wRKKglSnKeQdYWMgMemBxNd83w4ml3CG0mcV54ChbMoB0+2dcpBEW6aPjLlWlFnrJDx0RHqcjfhWmObiM966agcPHodkj3P4FMhmPE+wpWD++RmNhkKoyZLkLjH2QRKFyNAElNmLmuyVSAfwyQvI42jS2zgWW4U4u9xSY4VbBR5bV7GcR2NJQ7flMXe0cAz0ZWBYdJW5eBUv1yFTAbl4G4VgvwctKwc3ENHSC+3xz7CyoXrPmIydpsMhVGTJVBJndbJsbq2Dte8U225RDnRosvRJExw1ballMtlXOHmZTt/XHWGKNnX4QQrTla0pJm7el4iUz6Gls1TFVHbDCpkKJyJLi9j1zLBVds2r6cqiG81tfXR+WityXi98FqnBN8DWBvZ5NM1gJ/O++xBIr1m9pE2eKm7MGqyBIm3hc14193+KhTkZLKbFqdpp+J1NGkTQglcrK0q5MlkUuHJS0uTKYGSuF1UW4GRGBMwnEY8Lp+ibmKlaFLpEp/ahHMvqXJR8c2gSasjxLtEKk3aK+81lR86HZHorYqnaFLxYh3R0Gx7oSPXL4W3rdRL4k001RqbadJU7YTUuCa5sPb+UXKhgM6Lo+WF5ZrdqVv/fHRER7NmbsX7SIMuTlYT7Uz4JkNh1GQJ2qukEzinMINjRYnpnZ5IMblEOWqxIJJQA+49HWrRFVYLaePCDF3SRDldJYSkcsC4WAgSubVzq8Prqp9041J85BIKuUoIwoCQviGjNQI4Y9cxQVnLxwYu0CduJmkC7OdPKqu2OsImgedUjcfKBWXschV1Epr0aw19qON0JEFXqOIEa7IOz1T5BewbWjp5TOBcwu9Z7CO6wyT3/laToTBqsgQyBkucCLiENe6Ze9fcCUpxuTJL8Q28Lm55S4NIgucWc64k1YMmkYHnlAQpvCTPNgmyRFXICGlyuSaA1BFmk+HySJxzJ4gwg7eOSHJQLLx/Ub/ExhbD6zYvRi4a0MzbiMW5YwTNTjoioMnUViJv7Nxafg9Xhu6zj0hCv6ZxmwyFUZMluF5pD+jdr6m2Dorps9iTVn8dx1XIeCT7ahPlfPIUpLywNgLm+US+m5JTEqRXsnbWRoBPFZLYwHNJRHVMjo2NaykX0goZF5qyODS4GHiUseshb0G0/hE5Gz7J9F5J0zoDQph3pvPU+LyUnkUiftYFGlxYsslQGDVZQmNy55qY7AtkdMrxSA6zTkSVbl4O5Z0kL+gEykC0UfgkUFK8oOWCTNbWyRs3LrWJABAljFI0haHhYUP/W1r1yaRSHTEnfRp54ZuIzz6Y6PA9okOQCy8YuXBNEucqZFwT/AF6vZAaeOSazXjXXROuc032beI+opaDF0bNIgPreCaVHKaJC3PJsbrkMDZbnkisJa/VJmLKpnGjGzWFSp1FEiRHk8pn6lIsalxdPo4uQU98ArK8OTZGk9q3R4giRZc0aVCTgyK+bp24sTZGs05HhHJhS5Oub25uk/kPJpq1iai1/7PXEQWnTabX4YU06cb10hHNHESGpUYPXJKMo36JfKlY3+bigJDTTVJHNBd9NtYZ1ctNyWqMJpmOhDntI8bSeO4trCZCYdRkCYRbcX6Rsl8s6AWOSvpk8hCkCYc6xaS8E8Zk0mS/HE2WvPB8ksCc+xKQRkCMF5a5PLRccLfoEgsymRwLwWlRkHBNfY9xXImOuNAkTETVzq0wEZU8wXI6ksBLn4xwoInWEci+h83Bs0vE53XEUeeZ+SPz2cjkWOZGdYDgI3cbu0eSsYenWqxfXvtIYdQsLtDG3onyzlhbaoFj2pJVBzqPiXBciTvZ1JaiSXuiU/FEQpu1h0HYLwCUEkqtLWNmeJFXAqUtTTF8jaZQ6tEChF4enQePMHYBQw6KVEc8yuptw5K23j+KTxzeJ+fJK6/J/UZo0mvF6oj5e92eSXCkSYsXenFieEKmNDpCGrtGmpM64kKTj45I95HinprFBdZhhgYE9IndoxKCTDoDzG7DgMAB8wmDJjy5KXpUyNgucPO9sicV0RxwiXK2NItznlw2oJxu/pXQZBrX1UBgPSYeVS6uiagcTT68kF6gl7GO+FXj+Rh4jm9ziWnyMEyMHiLHNZvjRV46It5HXG4+L3JqFieQsdD5SScrZEir36VCJoNycDbUk0/1U1MrO9RxbY2AALKF1YEmuhxcepOxg7FrewqNOg7cK2R8jF3bvBi1X6lnw6cc3NYIEG8i+VQIeq01HuXg5JtgTnor1ZEMvFZCmshQHEeT9KZp8i2/PLzcDK+aCIVRkyVIstYB2hL2eCYhpDL4PSpknCqnJK5MU4WMsIpCC41EOYeqg0Awf/wbMtTCStDEloO7V0LoXfruBhFbORXJnOXcKvJEV8i4eLx8qq4c54Azdj0qBMmr9r3yLnJ6kiAoKRUyBM26dYrSS9CPR/Jrp5kmcj0AaL0m5UI18HRt3XWelAtx9ZMDL7i+mwiFUZMh0GETTXIYFfuN5U4wGe/J05VN7kTqRCGs2tH1Lb1unY0pU1fa62jyeSZBd/uo9JkEYWWOtCqErF7LoqJOAY4XSbyUJiDNK6k8qjpCPZio6VfrbdHE/EPye9w9AeIKGa7qKqkjpnugbOVNVy1kS5M6LmGIyR/S1FXXKJtiGMfFDH/rijoHz5POa0XqiJAmnZfbVkekOu+yj/hUCJrWhCZCYdRkCZLKAIA+mZH3WZiqXAxKEDDVCtJKCJeYMrkRUNn/Ct6JJsdnEkCEITKrhGDkQjcudaU6+T1EfJ2rTqP65qp2qL7Zih/O2HV8qoKkCfLvsaWJrILxeKoCkOkeq7eWRoB0rTEau456HdORZBUZQRMYnEJTyptpUyFI0KQ38IRVVy6Xnorkgq6oS9HM0cTNQZOhMGqyBKl3wjppUOhtES8WGVR2cHRJEyjJqgMPXmSSHGtTLUTx0fLBRF1bHU1sAiXlSfOIkQu9ItbVeHklUMbGtTUCQM8fm4PimDAf0xHb0zxHE+954kPOPjpimVMjzkGhdISbWwcvt+09UFLvIHk/WKv2EaKylxu3yVAYNVkCeeIjEuW8s9bzWOwZLw9HlyTR1Ih39AAB5hyG2LsozEnSOlFO6IFgS10zTriWLHA+SeBeRoDU2FVpdn+SwM8wsV3sNX1bl/pLdcRh8zLNH/eWVYxmjwoZLx2xnL+m6EgT12ydsTtPVMv3EWPOYROhMGqyBDIZUb1K2jIBjEsOcw4hqTRrksO4hFAy0ZHABcobMtrvzSJRzrJf0HyUJ8pZzl+gLlKOTxK4zI9HAqXc42V+kkDuwdPgbZMrxTTl+YSJZQJsiZELr+RYj41P9D2cMUUk4lPFDqa+XRN2AdkcZF0cEBuXWrPzkousPP5q30VJ9+IEzlqVuKIpnOkNGZ/Kjixc0bYVTBxe2K+2QsYrvu7zPYJKFq0HT2lr7U4WlkDb5nAxNIvLwW29PCVhObjD3AauNDF9i8vBSZnS5SkwnhrXfgGZx8Ql5CLWectSZXUjd9YRD5p81guvqzQImkzjStZzWy83R6+KL8JPiwzY5MtEopwuM11XnRFLGmwIuu5ND5eNPJEoZ5Mtn0r8S+PMlRCU8iWS0nS36NY6N+N1FRbkVfqIXs8l35AxxciTvNLQZKyQSdIlrjZpvCFT1b8hI5ELphIstKVJxRNzwMqFVg8Si71WVtXSeOm7UZYVMtq3n0xykeQVV53WePwJtLyKaNLPbfqVaG5uZTqkr0CjKnN0cqF6Hxq8CNIbrrhqkV7D6HugdO9GqXizrAa6trG+HQ103UvpQpq4yimRXJj0lno3r8lQGDVZAmnNBrT1Lont6vBkbJfJ9YCyWJB35xgy3k2ntoDpl32LR+D10NGl5s3YeoDYyhzHS7w4mii6bKqFUhUYKh81sXfXJEivijqKJgFdYh0h5oCSN6OOEB5YrnTXpPMKzlwhI5gDSkd0FTIqTdYXzhEVMpLbiE34GE0uOiJL2E15drm1U+ptsaXJt+rUOU+L8HJz+VTifaQwahYXEO99GPENkBgXPm0rhpOka9UOINtwpW5f27d2uHEdDJPUMwl5VkJI6ZLwyTRuimaLk5d1wqFHRZ3Xi+YJnAlv+z1ZvP2kjuulI5r58+KFY8IuoPGYaMbN/EkChq4kn3Q0mehK0hTzesjWP2tjNzausBJMSpfk4FzrPI0X3VLNVXMV99QsLhDHlBMJXqrlTt2oaeo7jwTK2CkmhwRKUSKqDqeE4mwS5WxOg7ZJkByvqH45uiR8MrV1TYKM0eQgU3kksUq9E0aabY00Td+2NAE0rzJJRCXCJiY8tQ55JZMS3yqsFtImCks9sC5y4ZqIL34axcRHSi6keUC6fcRjvRAkcjslxDcZCqMmS+BO5BIrWmdBq++m2L5cnQVNbK6Bw7gUnhq3IwNehCZPDZFbIT6FEvPH8UJHl4RPHN6FJkoeOwicineSR4HXkfVOUEmfedHkwguhXDRVR+aLErQg8lrlwUeCLkoeWR3xoMlHR8j5y2nNZnXEQ1Y5XjURCqMmS5C6om0vAAvUcnBHt7ypKkRCk0O2fCYPJpqqQqgKGdeHJQE+Qc+EY9o25cFE1xwU9iSZ1z01PnfCMBt5Mx9YzeueGg4v8TCY2kpo8gib+HyP09tqEo+WaVwRjx1k1TVHiMNxdElk1YTPK8zaZCiMmizBerEn3mNBIpEr9e6NBkdVqnAPJhI0xZVH+s6IxrKXVtBQuSBauoRv7TA5T3QlhIbeWOyemANtzoamrW0+js4FTsmFCpyxK0oaFC72Ck2B1Aig5FGH0xm74rfVaIM1zQuLXCui4od9r0qUO2HgRQMk+sXlD0nzAtmKuuQc6MZV334Szr2ETwC9Poo3+Qxoio3rUF3oqiM6Y1dbOUXtIwaaAuZ7mwiFUZMlUC46NidAWBVCZuknY/6gXY5sxrtrVQiTJ0Ld7stWhVB0MZU5kbvYUBVirGCS37SqfzfFwzth5DEgnnsdjjN2nU+hFE0MH72qQii8kCZtVQhkp3mXBHJxXhP1PUwFmnOisH4NM959JLmxm5iDhrFrvuPLRBe31pTMGy4lj1wFEygvN0cTPS55oGCrvaiDDOW1IiqYuIq5GE1F+GlxQTNcjtbl4MLLmnLK4Bcv9i4nFVuaGbd86uTscjdL1q5oykPHjSs5GQPuMsVUMOnv+9HRqztpCqtCFCA9bZIchlrnaZpsq0J0NJOeAKERoPHi8OEayxCTkicieh1c58VhdCTMOqSZLDNPGqaim88Jr6/O2BXSZPZUCwyTGP+F70aR667w+RPrAy5z0WeToTBqsgRp7oTWSnZtC9mGyuGzjqNmYQQYQy4uyhfENy+tUhMXSOVCU4KuZhqWpLErpIk1WDMwdlWa2ORm6rJBQa4VYLnYC/TW44oB5w3I+c6RICEXlkYcS5PAMDF5/0Q64pPcTPAJ0CT8etypBChP0GR5CBJ4uX33EdPr4JxuNhEKoyZDYJNjXRO8gHTuRAwnUC4TPotHArM4hWpp8ljsXRIoXV32MZpcFikPIyCLu4+oUnLbjU3F29IEeIWfyKR3Lx2haMozUdjjPhmbu490OG5cl1Bcbom17kn8YrmwXbO573GlKTauyz7ieFjh5IJb75sIhVGTJUgNExehEJ34mDhqXlUhzTyFcngJTQa8aFP0MkxcDDxhW1sjQLxIZbx5WSdBCnEcXe1YFZKJNzNjI6CDkwuP6kIBTfFnEoRtfaoLc9MRxtBypQnw3Ecc83FYL3eRKLw4wfYUGquEIKpYAI0SyDLegxKRKMfQxFYLJWlib2FV8WaayNwWHV3UzZYqTlcV4nWbrYYXmjnQ9mtbFZJJJYTaTmPstpqm2LgNmnQ4k1zYGAEWNzNLaDLNLfVmkXYD0sirxONlyrWS5IeZbuDVfa+Px8Q29EFW5mj69ag6ZQ9B1PpI0iQ9pArzytS2JE3CfURaYebjzWwyFEZNlkDeBMnElCkcFY+mbrZslLhSdEkWTm1ViJAm6+9h7uRRadZ9L2UskXii6iD2oJ5tKE7zGJ+prWbcQMpHbQWGmaZA9zCekCZubgOKVx46wvHCaf4CpV9t3wGtIwHxrZyck98Ds5EtufnXRBc3LsUL9r03oXFhLRfUuCqOyXcjddO2Wkjp10FHzGucYL2QyGPW+4iQF9p8myZDYdRkCToh11jCXslhVNmvNO+Ce2U4OSaHpxTepSpEWgJNKlfGbt8GLmRKoF1CFBEviEvlIpoM3gni9DRfFWKgyzaUoFRn6KtCLMNp0jt7bE+wUppibXVeK0mVi75fMs9OrPPVOkkaz65XHkmajwF3qEiFaOV3s0iq08zVXGaagtS8M55dCmfy7LqG4kyealEuHOPZlXi5RXRJcIzXqgg/LVKQ5j8kM+nVUw539Tx1JbfXtdrEycvUVnT1PLfo2uagMHd0mGgKeLoCZwPPkaYGXa6lyBxeGl9PySNoeVR5QT2M55FflL5HiG8bkDrSyNnIkI8xPhmqQiiZEj6ToK824XSE103tnUocXUI+pY1dGU3sHV+UPLo8ScAZeCa6uDXbZ012bUu9Ci/pm8DFvdwOOYdNhMKoyRJyzVqXnAp8Ko3o5DBr71ImNHF89Dj92hpiWdz3Yzw9STYRS7c816/at21Vjy7vQof3SaxteqWRwLvEJRlnWQIN0HObRZJxU0ugZf2GzEOn2ZZAK/3mlWTMGrvuOqL37PrvI27l4IYDUgugMGqyBGkJNJXgGikAE2awSQ6zqTQyXtA2l+6bSnbTJhnraBIodSpsQiRJUsluJrrqEFAGT2wjp9yv0qRB6XXrwpMXlficSUKvpt8YXkqT0DAhdUQ/t1aXJ9pc0CbWESq8KwxduV7hn/wekmYicVZLV05PErDJ53k9SaDDZRGu0dDEtXWiKbmP6GgS7iPUJZM2F31yeUBNhMKoyRKkiX+2p9BYshsV+ki2bSQKG/DSpxugOSFRNLFJg0JXpu2JIXH9P9lW9z15lHfGXNi2p2oJH4lxKVxsXJsTXyIsaeOZYp9J4OSRwHFtfXREEjbh2mqTL6lDEDe3Qi+cizxK1hpOLqx1pB4etL4CwmatIdYwXSgHoNdlyU3TGnwAYg64MKv4uR1uH7HRzaSOOIS9mgiFUZMlZJE06JD4xyaHWT2MFyN4/t8Z398hejAx1/CTblziRlruvp/caLI8hUpp4saV0GTEC2jyCsWZvFaC+XO6B0po7Lq+sp51SCyGd8h/EF3/z8iFqxHgE4rzevk9ax3hDDyBsZR1sm8M7zAHrvtIk6EwarIEZbGYT5ST3ongodTSTVN8V0YNAumpwHZji+HtaIrjdQaeuxGQqqKI3clTYsrf3Q0Tq0v/UmEGSTjN0Nb1Ii5dVYi0EoIJxdGGv3t1TbpChqHL9m4WE97VWAI0c2Bx309eRoCET6Zxm3WQMVX52YaQ1HF9jF0yTCRdz6X3T7kfjsWHIEn4vYXQ6dJoz5492L17N8rlMjZv3oyrrroKp59+uvH3TzzxBO6++24MDw+jv78f73rXu3DuuedG+DAMsWvXLjz88MOYmJjAWWedhauvvhoDAwOxfr7zne/gnnvuwU9+8hN0d3fjta99LT70oQ+5fEI+oApyWAWCjjjetFGwruj5vsNKJSrkSY3rkZSbDi8pLsdKxT1RuP4wXuwuECFNXJghrFTNvNC5xwHRYk+Wxs9V9a5b0Sm0VhUSBCrVsjdxvG6atk4UpsNEkbFbqdiful2fm+AM7FjbJiXlBsF8VUhVIxfquA40BaWO2lZmuwExodSo34w9h9HdR2FIh4apJFVDojDJC49nEoIOol+FLmcvHECHUp0ME2IfaVnBCTNuE8HaU/P444/jjjvuwGWXXYYdO3Zg8+bNuPHGGzE6Oqr9/bPPPotbbrkFF154IXbs2IHt27fj5ptvxoEDB6Lf3H///XjooYdwzTXX4KabbsKSJUtw4403YmZmJvrNv/7rv+L//J//gwsuuAA333wz/uiP/ghvfOMbHT45R1BdjovtSQLdYiOuFiLKWdvIFU16TDi6xK7ojOVCMXaNbfOsCnE9wRofxnPnRUDRnIEnwKsqhJFV8u6jvKoLbQ1Ltd+sK9CUNcz97iNuI884FMd5ua09lnVg1iFZGoP9ISh1q70NXQs5p+aBBx7ARRddhDe96U3YtGkTrrnmGnR3d+ORRx7R/v7BBx/EOeecg7e85S3YtGkTrrzySmzZsgV79uwBUPPSPPjgg3j729+O7du3Y/Pmzbj22msxMjKCb33rWwCASqWCz33uc/j1X/91/NIv/RJOOeUUbNq0Ceeff77Hp+cAUle0JLZrqmaQuMctQhRSF/j8gk49SaChyUSX1SLFXYdPVFFwrmgpTVq6iH65K8bJ6/81/WYd84+NK6lUMfSdRbVQjCYi5MLRZBPStAlRUDTpxtX1S9Fkoot83kRWwRSKdMRUOaWRC87Ao+hKbXwGXoQ6eUxW5hDrUBKSNNuEugWVfKF0zdb1S9FU93KzNGkrmGiawqz3kYV6+d7c3Bz279+Pbdu2zXdQKmHbtm3Yt2+fts2+fftivweAs88+G8899xwAYGhoCOVyGa9//esj/PLly3H66adHff74xz/G8ePHEQQBPvShD+G9730vbrrpppi3Jwmzs7OYnJyM/jc1NRXhgrrbOMv/AUDQOR/NC6rVWJghCEqxk1kQBLHQSaAkOap9Nto2vAhBHd9oHJSCWIJkrG2QvkCqhtf1W6c3Nm68siAWNlHazo/bGDaI8yJM8KI0328Q0RQRNc+LSlXDi/nr8FP9xngcpyn6HsMc1HjRmeCxQlOQplnpOHZxX63foIGax2naqjQFiflBECDoTH5PnI+p+VP77YzzcR6ZmNswSbOmX5UmJdyWnAOUSmleBEq/pI7My3I0B4qWmPqttVXnLy7LQYmQi+h7TDoSmOUCcT4GidCi2lbPC0UuUjpSSstFbH4ac6tbL3Q6Mt8WBpqA+NymZFknb0k+GnQkzuOK+XsABNU0H1NyoepmJ7F2lnRrXKNpCeiMr28BlH5jc5tcLwi5SMhUSkd064VRLsLU9xjXIVjsI2FiH4nJBb2PJNfsIIjrfPQdGf7PBqxyasbGxlCtVtHX1xf7e19fHw4dOqRtUy6X0dvbG/tbb28vyuVyhG/8zfSbI0eOAAD+5m/+Bu9+97uxYcMG7N69Gx/96Edxyy23oKenJzXufffdh3vuuSf671e/+tXYsWMH1q9fL/1ca+gfOAUv1/+9cf06dKzqw8FSCVUA69evx3jPSowDWLl8GXoHBjC2ciVGUTPiVvX34zBqCjAwMIDq9EkcbPTb34/jy5djCkDvyh70DAzg+PLlmADQ07MSS9evxxCAziDAwMAAZqszGERN2AYGBjC4ZClmAazp7cWygQEML1mKkwB6+3oRdHXjOIDuzk5sGBjAySMvYRhAV2cn+gcGcLCzC1XUTiP9/f0Y7OrCLIC1a9Zg5ngvRgEsW7IEawcGMNHbh+MAlixdgnWnvCriRf+69SitXBXnxcoaL3rqvBhd2YOxBi82bozz4uSUnhc9CV6s7MGS9esxDKCzVOfF3DQGUTO+BwYGMNi9pMaLvjovli6p8aK3NzptRLwYPFDjRVdXnRedqAJYt2YNugcGMNjZOc+Lo321uVzSjTUDAxjv68UIgCVLlmLdqxRerF+PUs9KvFwKEAJYv2FeLnqWL6/xomdljRcrlmPlxv7aXIZhjRdTkzFeHFu2rEb/ypUxXqxcuRLd69ZFvOjv78fsgR8DKi+WdCfkosaLvr5eIAhqc9nVifUDAzh5eG2dF50xXqxfswZdAwM43NWJOQBr16zFTG9vnRdLarzobfBiCdadMq8j/RvWo7S8By8HDV5swImentpcrliekIsVCi/qcjE5rvBiI47W5y/SkWXL5nmRlIuZSa1crO3rw9IYL/qAarXOiy6sHxjA1ME1OKqRi4gXnTVerFm7FtO9q2r0L03zYu3Aq+bpX78epWXLI7nYsH49xlaurPEikot5HenZuDEuF+Mnor4G+gdwNCUXy1Jy0VUK0D8wgJnpCRwBUOpI8qIXSwcGMLSkG9MA+vpWI5ydqdFfl4skL17u6KzN5VodL3pr9CfkYunSpVhzyikR/RvXr0Np6bJILjZs2ICxFQm5qPNixYoV6NlQ40WpwYsTY/O8GOjH0WXLa3O5ciVWDAzg2LLlmASwcpXCi45SjRcnx2u8qMvF4SXdNfrrcjG0ZEmdF30IZ6brvKjLxUs1XnR3d2HjwABerufrzPOiA40gfs+qpFysmufFwDwv+jesR2nJ0jgv6jqycsVyrBoYQLmnBycavKjLRQk1XlTGluFQxIt5uZjnxbIaL1auQte6dbW5LNV5MTmGIwA6OjpqvOheEudFd00uVq/uQ7WzVKO/uytam1oFTonCzYZGjPXtb387/v2///cAgPe///143/vehyeeeAJvfvObU23e9ra34dJLL43+u2HtDQ8PY27OcGulIwRBbdMYHBqK/nbk0CEEE1Oo1t18w8PDqJ48CQA4MTqKycOHUT1xAgAwOTmJk0eP1b51roLDhw8jnJmO+hocHER1ZhYAMDoyghOHD6MyOQkAGB8/gYm68Tc3M1NrOzRc6yus4vDhw5iruyKPHx1G6fBhVOp0jJZHgSVLAADTU5M4fPgwqsePAwBm5+Zq/90wkisVDA4OYna2Rsex48eBiRoNU+Pjtd+O1uiYPjmNw3VDFAAGDx9CMD6BapXgxZjCi2MNXtRoCKdPxnkx2+DF8TgvToxjYiTBi+HanFSrdV7UaTg+fLTOixqfR0dH0bduHQBgZmqq9j11OmZnZ2v/XT/WHD1yBMGSFZEcHTt+HJicqNE/Po7pw4dRLddyzKanT+KwIheDhw8h6FmFsFqT6eEhlRflGi/G67yY0PDi5GSCFzUaRo/HeXHixAkEkVxMY3BwEGvrvuYULxpyMV3jRbk8Gp3KpicbctHgxVyMF8NHjiDoXo65SC6ORXIxOX6ixovRBi+mcbgumwAwePAQgp6VkYt9eGgI1fp8NOSicmK8LhcTaV5Mqbw4gq46zZFc1L2zMV5MT9flYljLi2PDcR0pl8vRCTzixchxrVwMHxlE0LU0kovjx44hbPDiRJoXg8OKXBw6iGB5TyQXQ8PDCi8ScjE5ialjx+O8mByP+jo8eBjVuaRczPMCddmcjXSkzotKgheRXMzUeTESncQjXhyL8yJsrLODgwg6lsR5MdmQi/EYL06ePInB4Xm5OHL4MLB0WbTuDw0NoVpfA06U63IxXvveiYkJTNXXrOpcnYY6nwDg8OH59aJ8/BjGDh9GpS43J8YUXkyfNMhFWJeLoYSOlKMQT1IuZmYavCjN86LUHdt3xuvzMS8XY1peDB46iGDp8nm5GBpCdXpeLiaSvKjPR7Wup+H4mMKLw9F6UT5+vM6LhlyMAaMNXtR15OhRALX0j7iOxHkxMlIGpmrr38m6vA8ODupzoxyhs7NT7JCwMmpWrVqFUqkUeVAaUC6XU96bBvT19aWSiEdHR6PfN/5/dHQUq1evjv3mtNNOi/1m06ZNEb6rqwsbN27E0Trjk9DV1YWuri4tLktmp6BeFRJWK7GYZIgwVvYWhmEsnBkmcWrbcL5t2EikCxs4KLF3U9t5t686bohQefm4nqxoaBtWEwl8YajQXO+3QRNQj7uUanHhSsXY7zxNCr7Rb6j7HsTiwiY+NRIOI3QQxPFJXoRhrPqp1m+ibzW/yMSLSjX2PbWpmnefhpW5hFwgLRfhPDZk53aephQu0VaF2BxU4nNQE6lGvw25UGhS5aIxt/OMNMpyCESbno6PMR2pJOcg4HWkIyEXjXE1vIhPHyEX0OhAqNCkykVKzoEwUfkWKjSFSmm8XkcMfAwxjwsrGl7M61CY4GONFUkdCRufQ8uFstZEcpHSESV/xUCTrt8YL+Z0OhJfL2L4hkxV47oX8Tk1f41+w1oISsPj+TmYz4uJywXSukes2TEdUdrOr52qjsjkIrn+Afw+kpILZR8R80KrI422c/Nj5bnPEmCVU9PZ2YktW7Zg79690d+q1Sr27t2LrVu3atts3boVzzzzTOxvTz/9NM444wwANXdaX19f7DeTk5N4/vnnoz63bNmCrq6uWIhrbm4Ow8PDuYaTnMC5BFCYHOZR/aR9K0RaaUSUJbJJg64XcRkrIfIqgZ7PNdCCTXKsArF8AuuqEGmyr0MSeAvuu6jlKUjKSptHE0uztELQtQTahWYmOdbnvh+viz6zKIF2rS60LYFW22Zc/cTSRfJJLQdfaFW0C7D66dJLL8XDDz+MRx99FC+//DJuu+02TE9P44ILLgAA7Ny5E1/60pei319yySV46qmnsHv3bhw8eBC7du3CCy+8gIsvvhhAbZG75JJLcO+99+LJJ5/EgQMHsHPnTqxevRrbt28HUIshv/nNb8auXbvw1FNP4dChQ7jtttsAIApHtQ3YlLqqG3ayBDq5l6eUIEzjTJUQNtVPSSNCUlFiqgqRVARxJdBhNU2TVQWTvq2uKkR8QRsxrrbCgqNLwuNQWAJN8iIBSWM3TOPYS//yKoGmaNJ4FQEgKM0nz6YaU5U3UppMlVMiHaHl0fqCvaSxZFovqGo7br3IogQ65p1IzIGCixm7Wh0h5MLawKPkIgHE3KZKoNk1W9Ovhib2ok/JPmKQiyCvfYSSmSaDdU7N+eefj7GxMezatQvlchmnnXYarr/++ihEdPTo0Vi28plnnonrrrsOd911F+68804MDAzggx/8IE499dToN29961sxPT2NW2+9FZOTkzjrrLNw/fXXo7u7O/rNr/3ar6FUKmHnzp2YmZnB6aefjo985CPaJOGWgtHaDehFiip1TVQOJLs1W9/1eTBu1gxNCl3ay/mMb6okxtXRZRw3iPPC5nSlVvTYeicChqbYuJY0qePaeCfUShQOb3URV4Mmh1MoN7cUTQ3o6ADmZj11JEmXkCbWwEtebmkjUxqaTAZexMcSUDHQRc0BZ+wa5QKsEaDSnKo78bgHynzRpzIud4khtTbqvNyqXOjkjVqvqXE5mgB6zTbKlCIXuos+pTqi83JTa5zNPqLDc96yJoJTovDFF18ceVqScMMNN6T+dt555+G8884z9hcEAa644gpcccUVxt90dnbi3e9+N9797ndb09tUIE/OiROfCrEL2ipI3UYscdlLL42T0hRra3n5HkeX5G4WU9+O7v74uJoLs9i7WXxc0YI7RyQP43EncoomzgunpYmbWwcXOHFiF82tAU962sQeBuIeFC5E6xIOpeiSvBxuGjevkGaSXtYjKcQ1xp2bpddHlhcCzy6FM+iI9iZ3qWeXuE+G9mbOMR5YZh8JCWPXNiSm4CNeqF6rUq3SS2tgNxmsw08FMOAS11ctaMA9Hu3k7mc8DKlLvDT9esWjmc1Ld6oT5eNoTmwcTa6n0IA68QnokobibPMuXB/G42iKtXXI0xK9GUbQpMOrbV1oykBHvG7gdTVMAEsvHUFT0qvodWggPCq2hjLpRQU4Y1eUd8YarIS3zMXY9cnHMa01NvuI12HEYQ9qIhRGTdaQV6KcVxKkwAjgFNN2E+HoknonmngKVS8000JuCXrSU6jluHltqICfPLourAwvyCRWKnQYwztsQDl5DnOTi1yfSSA8vz7GrqRfn7ZOT6MwXm5RorCDl9tDR3JPFDa1bSIURk3W4PgkgbgqxPFhPBeaVLzOUxPktGlmUhXiwwuHjVxcFWJtmBBeK59+AdqwlFaF5GUEuFSFUEapl6cmx8Xe2VMj9U54HGRIwyQHI4AMxQkNvKy93FlUF4oShS3okhwWTXiPfUT2TEIRflp8IFA+7TtKCr4mNMnYbjKrXVM9k7pTRE4TWxUiCW+wlVNqvzbhGodKlUr67ght21g1g9RrReRdSKuFyBi5QhOVkA2kjQCdXGRSFWKYW62BR3wrRxdBU7oqJFn9lJxbdUxaVsnHTCk+AZr5IWQqpZsEXVRFSZDcvBiaqKpFU4UMudYIjQCdfnFyoeMVKauW4Seqyi8JVkanXEfS6wVDs3TclKeG4zExtzY0cd6/JkJh1GQNVJ6CpCoEoDPeKRwQt5S5GLlKE7NIhbpXusWnUHM8Ol0V0hjXMR6t8oJKlLOpClGrM3Q0UzSxlWDzNKW9YcnKqeT3UNUMaoVMoipEkgsirApJ333E0ASYjTRJ3gVFl9EwietPrCrEhiZrD15AyCpTFcfkbCTfUkoBNQeuIVqqulCSj8PKhcCzofUYK8auTbUQlTtWJ4k0dp29mUQ1XmKtsd1HKC83HQ0IiPUvqPetpymQ5OA1EQqjJmtICY26gHKJm5TbV+D64/BamhjFpGhmE5QtknJT5YfUSYUKfai8sOQjdYqRjuuUa2DBRxea6uOmSzwznFuth4Hzwpl1xFwV4kizygudsUu1DcybiIrXJ9NL5YIwdk3ySNElkQvj3UcWPDZUCOov+hTygpgfkVyYqhYpmjQ3JAMQ6ogpnCbQEa4ttWZLPcoUTVKcFm/xPU2EwqjJGigvAncPgOiUw5X9WrY1ekQsqp/Eiqlpy53ItSdNgRcHSCg1f18MmyPkcw+KxNPG5mxYnH4Bj6oQ+f1FbvkrFsauShPTt/pCsZEmwD73xSPXSs5Hh3wc6h4UCU1Ags8Sj4kwv4ikyTanhvAiSOiS5NGZ6KLWR+HculWO8mt22sDLaB/hLvq0qTptARRGTdYgKivlLHvLxLLkbcQmfG40cdn/ljRxdEkMR8By01RwupOiD03Stty42nCa0Ni1HZf7HopXPnLhw0fJ3Lq09aKJkXNJxVbmOiI0dqlN0SirkrWGPsjow7uEF4ejS7L+cXgXuWhrHcl4H+EM/yZCYdRkDa5Z6xxeYiWbxpWcVNj4uk9OjcMJtgUl0NbPJEhxgPv3ADKPF3siJxYxMl/AwwvnVf3EyaMmn6Aogbbr16dtHiXQEm9Zq0qgrfLOPGkCPPcRiYc8432kCD8tYiAT2hJCQd0Om6oMMAt5OjnM4ubYpJAbKiEiF6o2LmyI+ac2cptKI4Uu9l0bQyVERRdfJ+aANfAkRoDQMBG+icPSlaKJ4EWKJoqPjKxa3XfhUf1EVa9xNBnfVqN0RBAGMuU1kTckG3LHUhuFw23fFd16kZRHQ4VMpWpVIWifZKyrxjPIhZU8WlQakTRlqSMe1U8ua7bLPkLlRKW83B40tRAKoyZrME6umndhWf1EvRVCVQ5wmfRUFn4DqHGpHBMAxoQ10fcI8mY0lUbWJdAKUfOX79ll/5M0ReNSHiLDySo5f1a3EQeJEmhLQ4zKaQI0D+PJaAJAzg9r7BJ0kTSlSqDn23E0cQu2US64N9sAstLI6HnyqTSCoh+mts2ufsrzbTXuTTejd8KCJnb9082tXi8DNi9GXd88QpoUnwC7fUTy5l4ToTBqsoZkYq32NVnGgCDcvsbsf5sSaJtXoI3lxjBvthKaU99q/y6RliaOLkkoLiqBzv6tJH1ViPD0a3t64uiSbEDJEmhjW4fTPHX6FVdOEf0qYF0CbXHfj1/4yZEmji7nEmiGptQ6Y3Hfj0d4lzxQpOgyeLklJdAu772J74HStBXriNJvbqE4xtiVeMiL8NMiBJcYeRbvEpGLvWABS46b8hLYbSIxvE88mjpd2eSvCO5mCWK8sFkQHGlq0CVepMzzRz+MZ6CL8HjFXNE5PoxnRVMMr/PydNbJdVjsJYaJ6b6fDPJxtMauNKRJyKPVISh5RxS5KWquCBDSZP3oa+w+LYf1j81fsTcCAos7vvzy6Jq3j7C32pPhd4YXTYTCqMkaJJ4Ar0ThDE58ujEBcrEPda9050WTSpdX0qAHL2znj80RcjwZq21dEvRcDUuTKzqJ99lERPk4SXw+ngARTYAhjCfYgFzuqZGGNG03II4usY7YekLpjY+8UqFVXm6RsetRAs3oiJbmDPYR1sttqyNFTs0iBlEylSRp0OKZhNi4msQ/m6oQE026JEnbqhCrC9qUzdomkTFFF5PUqQubqH1Tbcmr530SQj2SILnEaFtchLehiUiKjtEsSRRO4qkwQ3JjM4UhNGEGmwdWbRLXWZqI+ZNeYiiiKYGP0cXJqgZnwpM6z8gqUVhA6o+2rZAmHV06HEVThJcXd7A0NWsfoeTRiqYi/LR4gUwadDyFShKxtONyzyQoNOnwMZo0bal+AULQLa6Pt/FOiMIB5u+JEoWBxBzY0GTghZMr2ueZhCRdmrCJ5KRvwmdCk04eHU+hAZFrQMqFREccPXjkNfw2CaG2balxGTxAy2PMg5fRaZ5LiOdoAowGYBAQaw3Fx0hHBBWPSbwoedlxDbPaRzTfY6QJiXFt1j/B5YhNhMKoyRjsHsZL4nOoClFxoofx9DS5JApn8iK2LvdFHGbIOFEui5CY030kLhuQAJ/FiTzjuWVDLlmENF3DDIB78qWLu1/MR598HMcQEod3uftIlMjtUvHjEdLMS0d8wu/ifaSJdx+pRRa6XKsmQmHUZA2pt0KIe12SIHl/xqTUVDVD8q2Q2Ht+TAm05A0ZnzwSY9hEolzcuDqlNtMUq4TQhSg8aAocaUrhU/cIWciU8d0oQ1VIAyh51M0f81ZSkJOOsIY/RVeSJtPdR7ZzL92AMs/H8UmsFchqA8+GWYX0AvQax+mIOrc2b5yl6HJ9W43SEV24Wvg9urAlqyMOXu4UzZZrNseLJkJh1GQNLlUhCdeg1isiPZETJxWrqpAkTdRJxFgOLjg9cYlyNnfciMYlvkcZ127+1HamqhDJidy0AXkkk/ok9JKeAEmFhcP8UC77GN7BCBB5lwxVIbrFXvS6NGPgOdJUG9ewAcXu7LE5kQcErvEToYFnE64maarTxW3kVDk/m8TvEjJL3PdjWzzg43lndYQwTDLYR1ivlanvJkFh1GQN1CYjXuzpUkrrcb1ct0K3fF7vpjgt9h7hDck7MdRbLka8v2FCVlh4vEuk7ZdrK8FxVSEebyXp+g3ExqFPhYwlzYrM0CXQHjpCyRv3VpKljsTv+7HVEQ/vkVTnnXREEFbhSqAzXv8C0T7CrZ06D2xOfFT2AmNlVZOgMGqyBqt8AZsr4An3eHJcY7UJd1IxV0LoPTWJUxuXLW/zJEFssbCoJIq1JUIu4mvrVZqTJyDi6nkTzVRVCDe3Eppcrmo3jRuTRweaAGir10T3oDjMbXIxp/TAhqYUXRbymCyBtqqcknozCZp0shob10xTKJGLJFiVQDMypX1uwhCupjywXIWgpARapCPJfiXezLqx60BTW+0jpYTOtxAKoyZroDL8nR/Gk1xPTlS5cFUHkuQwrhLCJn4bENnykuoMqqoKcPM8URUYNtUmKbyEJsJlb6Kpgafc40DEK+2lcawr2iHkkrxuvUq01dHsnGSsPnOhkVXTuJIr4NVxrfJmbK7h18gjm1PjSJM6roWOBCkdsZhbFZfAp54GsAmbJCvqbDxP1NxLLiKk6FL7ZddOyzVbfKGjRt4ommLjWqQiqDQBCHWPHzcRCqMma5Bk0ntVJLhUhfi7fcnqJ8C9KsR682Jw6rjeTxLY0MTE11tBE4fnxnWdP9cSaHVMl1eg88idkNAlrpyyHNcr50m41niFaCkDnQnRkm8pZR2ullY/+ehI1lVkLsZSAy/ZR7i2lvsIV0XbRCiMmqyBermaEajmlUA7XLals74zLYF2oCkPIyC2YORwKZnTZqwuykQYQdtWdUVzFxE60EyF0wCZEaeronAxTJpSAm0pj1mWQHMvLmtpysEIiBl4GeoIdX2E1cO7FjQp44ou+rShKxUONemIwICw2EdkOuLgeSLWbDa/qIlQGDVZgyjezIRNnKzzrE98kreSSrISQAeazPf9SN5KModN7B7GM+B0L3wnH8aT0NSgi/FO0A/juXu8xJfV5f0wnpDeGt5sBLAVP614ksDLm+l2kAkkF7S5hlnVcX28mTbfa3X5qIt3wuV7BCEzCc407kLcR2I5Xq2DwqjJGkSJclWmKsThpBKFifI5hRoF1dUVrVj1dg/jQb4BkXz0qQpxCFFkMLdunidHDwPXVloV4moEeHkz7Q2TwPmBVRrHGruS5EsXXngYJrmUQNcInv+3bVg5C4+JV/jJJRwqCOUA7iFNJyNNegjyCWkWOTWLC8SuaE3VgbrYs++TUK5oyxAFmfHOZMtbuX2JqpAkxMImHE36trKqkGTf/IkvNI2r0kW+tZNoHBm79aoQU4WMU7WQxSJlUwnBGbsxuhyqZ6KqEDlN6URhizdxbLwibJVfVjoyb2ixdx9x1Wlk6MpxrZEYAcq4QTIR3OZ9NCYUJ3oMU1ItlICUsWusCJKs2Sq9ipfbWkfo9S9wpKmGN9PEe7kb62MRflpcQIQonF2OAcybrc07PjZVIQmaUptXVB2goSvLaiEbFyr5fkkS58ALq3wcA002rujk+zOUXJjmVnvqJioddHTZ0qSOq6vAkFT5meiiQhQmD0OCJm1ViDTMYHPHDcUrVlYTvAg1+iV6HdywXlBGqYkmqhIsQRNr7OrCXpJE4aSX24ImTkdCXfjdxZuZoElbLUTSBdk+YjR2CXmT7iNWXm5BKK6JUBg1WYNkU+TwmVeqZNDWxS3f6qoQ6uIqG8MkNa6PK9qhKiQLmqyM3QRdXrH55lWF8GGTFlSFcH1Tc9CMfBzqMsE8qgtJmikdYe5B8aIp59CVSyWfdB/RXXKYV16M0MtjvMCySVAYNVlD0jWrexMH0As69UaJ9H0gSTKpxRtA8xUJhjip69tQ3MV9oreSTIuFzwakhpAs3O4cXZK5ZWkmaJIs2LZv4kje4jGGvSzmQKUrUDdyTVvR+0AZXHtgM/dSlz6ZWyY4BFnRlOF6QeFN76M5lJLTISQbHSFwHE3JtmId8eQjh7PZR5olF7ahrSZBYdRkDZKFM4lPuQY9vC1ZvYkjvaBN5OVxqAoxhhIEF7QR1U88Lxw2IJHr1hQyswhLOj2GKaDJ5XtYD5GjEWDy1HBhLyg5NTZyIaEJkIU3svaEct5MUajAwiMpCZVyeCo8yNFl1C9Y6EjGHhNjuCaJb+KabfL4t3wfYWSuSVAYNVkDVRUSSw6zvGCKqdoh373hqkII7wX7VojjmzixqpDQ0qPC0CQqgebexNHQxL8t5Bhy4a4Y7zDTxFf8uPPR9U0cFi+hicM3kyZuXFZHHGlWDTwyzOAyt4K2Jh3pIPA+OiINV1O8cqFJksvj9D2MTLnKhZeOSGlyWGuChle/SBReXGBKGmxALF5NvT9jrmDSvhVChiikSm2maT7GSrgcbd61ibXljAADn0z9xiosoMexFT8Cw4QKBVm8S5QugXblU7op6dK3yhewqDZJ0sXJRaxfxdjVzr2ZJm5+AkeaYuNqaLIqgeYqfoxhBu7UbdEvENch62ougcfLGMazmIPYhXPJkItF6EO6Dolynqj1wrbSyEIudDRFeAea2DWMqJxi95GipHtxgSl8YVMRlMIpIRcgIVTUuzZcpYpNJURSUKmqEO6tEKoqxKL6KfkKtM+lcYnL+UJbmkx4CY8pugDCrRt/MTlWCUFWpyXH9El81tCkjqsLbxhDcYlxLeYvCILYZYLxqhCJjpj6bYxLhSW5zSttBKTeO9JUo6RKoJPfY3MbcaKpXQl0clzCKBUnn9tVP6VLoCU0ydY/7UWf0osItSGzRL9O6x+xj7DGLtGvUbeIy0lT7ws67CNNhMKoyRqsTnwWbbmYciaVKua25sv3BO7KLGPzKs44riD5smXVTxYGkQ1NJrok+SsMTW6VELz7nPVm2ubycFUhLiXQCbyWZitPqJ4mqxLo1LiU4W+46FOSj8PQpA0zOBkBibZsvhvVltYB64s+OS+cV96Z2/pnNnb5tuLvcfHsFs8kLFJICZTHZWi6dhzeJTmMCrlIBdnXCKAuIjTRZML7GCbkBXrSU2g+RgBJUwNvookKUbgsUjahOLZfC7qIOYgShQF3I8Ckt073QCX6zroEWkITwBi7DjTFwrsWfJLS7JNwTdEECEqgLdZrBS8y/G1CTCkjOuN9JOnltqDJKvm8BVAYNVmD2JrVuEmt4qgGvI0rmqSpgfPx1EjLO23CRArOOK6DKzo5rlVlRwNvYQTY0CVNGrQtq+doEp34GO+E04mPmD8JTRzexzChXPoup/lMDg0ETdy44lCOB006uqw8NTYXw3EHQkcjwElHpN5Zl8Okq45ksY9wnt0ip2ZxgdQVTS6s3OaVsYXdqlOo62IvfRjPq7w9p1O1jyuaDT9ZGqWcsZtzSDPrxT5QjZrcvJlNMExifdt4vDQ0cW2zLoH2uYhQ7Alw9M66jOuSIyShCRAeOBiPckv2EYdqySZCYdRkDFYP41lUBqSqQki3L1MtlKRZQFO0KRrfNEq3TSfK2btftTSlEuUoPtpm/9sYWg6VORJXdGpuBRsboK92sDHSsqJJHZeiybdyykSvqa2EJjYXpErQ5FLxIwzFcfJoognQf6+IJoHOUzTpQCQX6QrBFF1WNDGJtTYPrJIVjyaaJIcGUyVstvtIytgl31az1PnoolYDvklQGDVZQx4nPuriuFQlC+GKNibKSTwMCZcilRHPXeKVbEtUC2krIaRVIeRdGExlhyVNcXyapoCiyUQXd2FgEPBVIdq2slCcqCqEm1ul74CqsJDSJbkcMdnWSlb1VYuiu49MIRedrKYqb5JtCboonU/SlMIL3llyosmnWijRL3c/DrX+UXxKjWsTZrVYz5n1LyB5xdAkuWBPO39MWNJyHyFpKoyaRQbiU+jCqgrhqjOsHsZL0kVsXk2vCpE87cDGlDOuCukQyoVt8iVr7Drm+cTwFE05VoXYzoFPdZq4oq55Idr03UcW4zI0kffyZJAonO21B7q7jyxoYkJxTsZuclwdzazn0HX948rB3eWCfVKiSVAYNVlDcmK9qkKSfQuEVSeoybdCXGhyigvb5ON4vGlkMy53e28seblJNAH0Rh4QNHHjBkq/Jnc+YKgKofpleBEQG1SQ+Fabt3oaLm6XhZXUkYaxW68KcdJbxi3vo1+6cADHC5E8UvNe1Ru7gUozcXmbK03c97AyRegI9y6bz9tqKVxyvTaE0SmajOFqt30kUO+5cZWLKDyoD10V4afFBmxCqCDkYjzlUJnpglO1Ca8Na0lpIk4M0tOvlKYaYTxdWhdqgiZuXCscEU7j2qYqpxxc4K4eCEPSYMC4z2M0+VTF2ZxCpe8SSTwqXN4FKY8eyeckTQ4eLy1NgQYv1ZFEaMpEl8hjTOut1d1HkvBvB2fgSUJMxNy6XPRJrdfKuFqaM9hHzBWrnvtIaDB2OT1oEhRGTdYgDrloEg5jbnkq94VK/CMSGU10CWiKFI9MrGWu1WaTJDVtQw1NCl1afIeAT6ZxRYm1TEIoSxPSdElolrw/Y3O1eWrzagOaYuNqTs4UTam+KR1xlQuCptAkF4Rei5O1iZNzqPmeJF1k0ifHCxuabHSES9Zutjxyxi5lBAjmNkWTRfJ5alzhPsLkUIYuc2vEM4Zlk6AwarIGU36DzzMJVHIYeYV/IvHS1FZCE5c0SJ1imFObU2xekjRtU94pSb50KYGWehiopEHD6SlIzb1mDnRGmvQuE/I075CDwj03kUroFeYLBIp3Qnv1fKOtrl+dd0IzBxKabJ5GYb/V4mkHm1weiQeWS6yVJOnbeHYlXg/A4M0UyLmJrqT3lqIpNS7ieGp+uOcmSJoc1k7KU22iS7IXGL2ZwiTxJkFh1GQNHol/Vg/j2eCsHsbT41hXZtZJg+KTl23IJZFf5ERTxmGg2LgO4Q0Xo1SlydTWNekWEBomDnPrcw+KZG4B9+TLjJN94/h8ks/Zzcs2edaHJuna2cx7k7jQfV7rRavainXELDdFTs1iA+l9Fxk9jJdqa1UC3aCZirHS3+P2MF4SrzPwHE58yX51VSHGEuhGW2ojoDcvbQm0gCZuXNbYlZRAOxm7gg0oz6oQ3fzlZQQYS6B5mnKtfpIYJj6HBu6CNmuDyN0wCTjDvxWGZRbGbtaGFsDkMmaxj9h48GoQcOt9k6AwarKG1GJhuMyOFGT3MllRCbTDGySo5vEwXoNmS5qUttQi5VQCLeFTs0ugqXyABF0kLnk5YpYP49m8AZTCOV7KqAPHU2iqBJq69M+SJq8S6NgGlJi/nDyHbAm0hE9Oxq7QSOMMf10OntSwNOVDcTSLdCSJFxwmnapO/fcR9h4oqoq2eCZhkYGLCzX13HweJz7LhTVJE2CXTyChyWWxl+QTaGnSVIW4nkKBeAm0T76UT1UI2zeDi/VN5ONQNOXxMJ7uvh/rPBLzuG4VMkTehWKwksZuTnc5xXMjNG19ZCqr+35sctaMbTPwclsaAcb7fqh8nBRNHgaezXM7dZrEt9pbzi3v5SbWsCZCYdRkDVYP41lcSw8kKjAMON19FhxdEppMeAlNptO82A3K3YJscXV5imYHLwEgOF1xzyQY+tby0SIfx/aqdvKUKqDJRJe62FOyqqVJUJmjq/pQx9V9D0UTR5dYRyw9Acm5NcgFb+xylWCGfo1rDcVHvt95Y5eYA+7ladOTLbY0qXjd+mhz6Z/VUyINXjBeboIm731EJxe57SNMvluToDBqsgbTpig5aRpx1D0oVJUEQ5ek2kRaFULRlKoK8aTJ1FZyBbyJLuraeo9KiCBVwcQtuppTt8splGobO80Ti5TLw3iiih/D/EiuW9fSy93NIqDJRJfIw8BUhbh4TEhe+TxJYKsjxPxRVX4munz0VqKbnFyQ8miQC+s1W7deuOoIvY/QD9W660jI6ojOq194ahYnSBOFm10V0spEuayT92L4rCswBEm3JrpsHsazGtdhsU/S5JR8KZgfDt9MmtRxbUNiHF0+VSECj0m+OuIQ3nDMx4nJhXV1oYdcNOOZC9v5E1cXUjrvs4/4rH8uuX+Md6lJUBg1WYNXHNXdCGCrQlxj5NKqEJdF1ytHKKeqEImxZBrXNVeHoymLnBqH5EvxRm7Li2bkCHnJo+P3GMeVyCpzN0tehknG1YWZyEUuOuJhBEiuiGAvq3Nsa5SLFhxwObo4PjYJCqMma0hutqZbdH0Uk3yvxcM6N1WFJN8hyYimeFWIRb4NR7MPL2KJcMlqE64SgqCZeucFQCAuM7erumrQHJraSt6f4YxdXfzdxhtmzLUiZMqpXJULaVK5PBZVISm8h3eCzP+S8sJybjm6JHzi2rrMbUDMX+x2Xi4Hj2hLjWu7ZkurhRgeOyefN30fKcJPixNSiXIJEFUz0FYyWQ7usthLS6CdT14OVSGGMELqFl1mXKVhGu/iinYNexkrO2y+x+P0xN2CbJnjwD+M506TvgTaMhdEa+B5nEK1OlDPw5KWQDvMbUDyUZdLpckFoTy7NrkTSRxr4Gn6dvJmCm6xlnp2XYwAUh4FNHH4ZtKk9u3jRSUPuIVRs7hAWgLNvYlDVu3oqgoIXApvQZMNzSaagHoJNPEOjCNN+heVVcUjvoegKdTRxNHVIeCTqSqE7FfmTtZXmxA8BhIn3CROMXbJChnKCKBpou/7MdNklAsJTbp5V+niDGXqHhRrHUnSa5IbxvDX0kR4L1Qd4ebANK5GpmIl0Oy7RMkxGWOXmgNp3lm1mnrvKOD0Szi3SVTc2KUOuIRcGPQ2yGkfCTqItZGji3tUtEnQ6dJoz5492L17N8rlMjZv3oyrrroKp59+uvH3TzzxBO6++24MDw+jv78f73rXu3DuuedG+DAMsWvXLjz88MOYmJjAWWedhauvvhoDAwOpvmZnZ3H99dfjJz/5CT7xiU/gtNNOc/mE/MBonVtUP5lO89R9CVRVAYeXxmCpe2yqBE1AYqESvFflkjth/S6RkCbbEyxFk6mt5P0ZI01mLwJbFSKqNKI8XnP2J/KYjlhW9VDyhtpiHyLhApe+OK/NUUlXhQTQQEcJqBjo0m24SZpSr0Ar/QLyCjPN94RSmmLfo5O5RL8VhRdJuahWLXQkMaakBNpAk7HKj1z/GD7aJhkneVFJ6ogFTSmj09Hjxa0XZKWlZXXhQvPUPP7447jjjjtw2WWXYceOHdi8eTNuvPFGjI6Oan//7LPP4pZbbsGFF16IHTt2YPv27bj55ptx4MCB6Df3338/HnroIVxzzTW46aabsGTJEtx4442YmZlJ9ffFL34Ra9assSW7ecBVhbQqOSyLBEpXFyrXtpmJjIAwHGBa7KmQGRWrNhl4yXHlobgUfqFUGvlUhYjd8k1OPncNJXjpiJAXWSef51VpZDR2s6NJXgKtG9dMk1/onjDgAOa+GPd9hLz7yKMCt9WeGmuj5oEHHsBFF12EN73pTdi0aROuueYadHd345FHHtH+/sEHH8Q555yDt7zlLdi0aROuvPJKbNmyBXv27AFQ89I8+OCDePvb347t27dj8+bNuPbaazEyMoJvfetbsb6++93v4umnn8av//qvO3xqkyC5eVld7maRNGjCSYwA6ycJBMrHbV6cEWBzLT0g24B8jIBKhQkzOG5AHJ6rvHGlqWJKOPQwalw33JiOaL6pFTQl8WQIyXZcgawChlwelSbTmBnTlBzX1K8xdNVCY9eUEC+tfrJ9/sTqMGlxwV5MRzTPDmRyGPE8TBpDmgZeNAmsjJq5uTns378f27Ztm++gVMK2bduwb98+bZt9+/bFfg8AZ599Np577jkAwNDQEMrlMl7/+tdH+OXLl+P000+P9Vkul3Hrrbfi2muvRXd3tw3ZzQWXh/Gsr4Bvwik0lnAoSbJjvBOuRoDvotsA1hUtwJnosnlWINVWQhNX5ZLB9+gu/WPlUSAXLgmU2pALQW+srWTj8zACTDKVWwk0pbdE2DFGk1RHdCEKh7WGosulBFpyIWeeXu5WrNmsjlA0CdeL3O7Tau3bT1Y5NWNjY6hWq+jr64v9va+vD4cOHdK2KZfL6O3tjf2tt7cX5XI5wjf+ZvpNGIb49Kc/jTe/+c14zWteg6GhIZbW2dlZzM7ORv8dBAGWLVsW/TtLaPQXBAFKpRKqpRJQrSJQlCQIkHhAsBo7pdVw9emoVmLrSxAEtf911PIFgmolsvkDBJq2jUdAdNVCaZoa/aJajS1sQSruX0VU8ZGgCdUKAgQ1+jCPqwYBEIYJXtRp1m0yQYIXlWpsviJe13MngrCKsEETkjyuzN/oC54XABAofERiDlRe6L6nQROqNZpDta061zpeqDQlv7dTM7cqLyK5qMbGjLeNL2BaXswnSCTkzSAX9baBkjQYICkXVQSon1GT32rkhW4OknzS80Kdn3kdIeTCqCMNfON7OiPdS9Gkjhtq5KLetsaLhFx0GniBtI4EAer6ldQRg1zEdMTEY71+6b2d9e/pZORCqyOI86KejxOqfOyY346Caqi0FciFhY6gsU415r5Tx8f03CJUvzctF6n5UfgYhGn9i+tIXS4imlS5UHmh2UeS30vsI3Fe1AwTdc0O1basjiCGV/PZst5jbcApUbjZ8NBDD2Fqagpve9vbxG3uu+8+3HPPPdF/v/rVr8aOHTuwfv36PEgEAPT39wMAXuroBKoz2LBuLQ7XJ3fDho3o3NCP8qpenACwYtlSBF3dtX+vWIHVAwOoLF+KQwBQrWLjxo1omIkDAwMIggDHVvRgEsCqnhWYWboMkwBWrlqFVQMDmJkYxREAHQiwbu262r9LHVGy9dCyZZgGsHrVKowv6a79e/VqLB8YwNTadTgKoLujA6v6+mr/7u7GxnrbQ93dqABY29eHYx2l2r/XrsWSgQGcWL0aZQBLu7uxZNXK2r+XLsW6etuXOjqAuTlsWLcOh+vfs2HjBnSu24jyqlXzvOjojPNiaXedFxVs3LAhxgsAONZT58WKFZhZtrT271WrsHJgADPjI7XvDwKsW7e29u8OhRdL67zoXYXxrhovGoZ639q1OFbnxcrVa2r/7tLwYvVqHOvoqPNiXZwXS7qwZGWdF8uWzfOi1FH7nvXrcai+Tm3csBEda9dHvOhZtgwIUPv3ih70DQygsqRrXi50vFixoi4XPZhZGufF9InjGAKgnPtivDiydClm6rw40dVV+3ddLibXrsMxAF2dHVjZ11fjhSoXXRperIvLxbLubnSvWlX799KlWBvxomb4b1y3Dofqu9vGjRvRsWYdyisVXlSrtX/3rKjxortTIxdBpHtLV6zAVF1Hppcurf27t7fGi9FjdV4EWLt2be3fHZ0aXvTiRHeSF3W56OxET50XS7q7sSGSiy5UAKxbvRpH6zqybt06dA8M4ERfX+37l3Sjuy4Xy5Ytw9qBAYRhiJfr87Jx/Tocqrv0N27ciI7VazGyahXG67wIu7swXteRvoEBzHWVajpVrWLDhvW1fwfzcnE04kXPPC8aclEexhCAzlKANREvVLlYhhkAa3pXYawuF2tWr8YyRS66OzvQ07e6zosl87yoy8W6NatxtNTgxfoaL1YrvFiV4EW1GudF498bN6KjdzVGVq6s8WL5coQdpdq/e2o6MtcRKLzYUF9rghQvent6cHLpkjov6nIxMpTiRWdnWkfW9PZirLOz9u81DV6k5aJbkYuDXV2oRnJR0xGgtleM9a3GaJ0XXStX1v7N8qIfHb19GFlZk4uVy5ehGoQJXgCHUTOkGrwIAoUXy+u8WNmDk0tqctHb24uegQFMHz9S+/5A4YVGR9b09mKsriNr1qzBMkXOUa1G+tgKsDJqVq1ahVKpFHlQGlAul1Pemwb09fWlkohHR0fnN5H6/4+OjmL16tWx3zQqm/bu3Yt9+/bhne98Z6yf//W//hfe+MY34tprr02N+7a3vQ2XXnpp9N8Ny3F4eBhzc9m6x4KgtqgODg7W8hbqLrqhw4ejuOPQ8BCCSojK1BQAYGJsDFiytPbviQmcPHwY4cR41OcRxfM1ODgIAKjMTAMAxkbKCOv9nDhxAhOHDyM8frz2m9kZHD06XPt3tYrDh2vqXZmrqdPIsaOoTtcSsEfKZYwePozq2BgAYObkFI6PjNT+PTMz37a+AR8bHkKl7tI8duwYgsOHUR2fAACcnJjAdL2fk9Mno7YRLwYPRyHloaFhBLPVOC+6uuu8mKzxYrzWF8IQR+rfD2Cepvo3jJVHIl6MnTiB8cOHER4fUXhxtPbvSmW+bf0bRo4eQ7WejF4uj2I5gHLEi5MYGanxdGZ2VuFF7SOODQ2hUo91Hzt21MyLkwleVCs4cvgQGsw4MnQEwcwcKlMnAQDjY6PRSWt8cgJThw8jPFHXn2oVR440eBHM01T/hrGREYQna/2MjY3VeDEyz4sGqHIxp8pFvZ+RkZG4XExNYaQhFyov6v3VeKGXi6mJcZys9zOV4kUVRw7Py8WRoSEE07ORXIyPjUUn8vHxOi/GyvWPqODIkSMNVmBwcBD9/f04qfKiIRcpXkzj2LG0XMxFcnEU1Zmal3ekHOfF9NQUZkdqNExrdOTo0JGIF0ePHkPQcxjViTovxg286Oiofc/heZ0/MjSE4ORMTC4aZ9+JyckaL0ZHIl4MHal7r0MY5KLBixMxXszNzODYsWNGXhw/Oi8Xx8tllA4fRnVU4UV5JM2LOp1HjxxBpdrgxTCCFb2KXEzg5NiJOi+mcPjwYUTl4NVqbP07cuQIgsmTqNRle3x0NMozGZ+oy0X5WJ0XczFP/jwvavM5OnJcoyPlOi+mI17MxXhRrfNiGNW69//48ZEaLzRyoepIw5+hygUQYHBwEBVVLk7U5WJKkYu6lzvGi6EjCCanIrk4MToK1HV7fHy8xouR2jeEc5WIFyHCeV7Ufz+qrBejY2M4kZKLozQv6jw9PlLnRf17wkplfi/MCDo7O8UOCSujprOzE1u2bMHevXvxsz/7swCAarWKvXv34uKLL9a22bp1K5555hn8p//0n6K/Pf300zjjjDMAABs2bEBfXx+eeeaZyIiZnJzE888/j1/6pV8CAFx11VW48soro/YjIyO48cYb8d//+3+P+klCV1cXurq6tLgsmZ3st2bU1M7FanZ5GIY1A8dwY20YhghL8y67UEkOi+iN+p2L2jbGDJX8BvXzGm0b+FjGe4NegiYAUYw2nEt/T6hmvDd+H2ponptLtY3F9aOpqn9PgyYjL5TviYbV8SL9PfO8UPiIOC6emBnq56AxbpIXiaTOeT52AHOzBl4o40alrdDwIjEHKZoa35Nom4ivx2hCfH6i743lzEjnFhpeaGSq1AFgLj23qlxUFV4k5xbxKosUTTEcEnJR1cqFTr+i7w10CZ8GuZjHpnVkHhlvW6lo9Uudg7AxRpjQkUQJNCmrQh3RVsiEGrnQttXoZiQXaVlNjVutGuRCI8tJXlSrCKs0TaGRFwa50OaoJHhRrcyvIaFGLhIHafOanWhbMemIPrclto9UubmdS9HM7SOiOajMxXnQZLAOP1166aX4i7/4C2zZsgWnn346HnzwQUxPT+OCCy4AAOzcuRNr1qyJvCqXXHIJbrjhBuzevRvnnnsu/uVf/gUvvPAC3vve9wKoeTkuueQS3HvvvRgYGMCGDRtw1113YfXq1di+fTuAmhtXhaVLax6O/v5+rF271vnjc4NYolxiYm2qQkx4p3JVKgtfmOxmW9LNtVX5lFSADoYXeSWEcpUQruWqHF3SqhDbcTuY7/FJCC0R82f1/kxy7jUGXpKmBt7Ur23yMofP9YoBalwFpxYhqDSxNOeVEJq+cC6Gt732APV8j7lZfv5SvOASroXrEHFZJ1kCLdH5ZNeKAa7NQGncfeQq52Fon1Qt1BHthZzcJYZNAmuj5vzzz8fY2Bh27dqFcrmM0047Dddff30URjp69GgsSejMM8/Eddddh7vuugt33nknBgYG8MEPfhCnnnpq9Ju3vvWtmJ6exq233orJyUmcddZZuP7669u7yokCyVtJVGUAYBBGQWmh08N4GmHkLmuS4Di6yKoQDS+8aNK0pcY1LVK692eiLGMhL2yrQgJmI6fexGHenNIvYgKaYniHxZ6iK3cdcZFVTb/W8uiim8oGlFyttTqiaeult4S8sTqi8orAxcbVJyinaE4aNYo301qvA84wkeitg3HIrjWC9bFaUQz9ZuqIWVZb/faTU6LwxRdfbAw33XDDDam/nXfeeTjvvPOM/QVBgCuuuAJXXHGFaPwNGzZg165dot+2BKhyZJ8SaJfbiK1oYhYaFyNAcreOS0kjyUdOMc0ek6DDjAMg/B6fzYvx1IhLewX0+tCk9i1+K0lIVxYl0M2kKTauCx8dDw1iXljqj4r3uW7Boa1aTWmkqVIBOhOpBdJ7oGz1JzaugxdOtGYzF31y658qByrO1NaLJoGhvBCNmgIYoMIMxNsn0VshYch7VAyuTNPbT2q5sbHfUNMvEBf0hMsxWoRMbSm6yHAMd/JquFh14Qs15KKhSfL+DHvyoly3+jkg6ZLQJBrXAhejSXOZFkWT1bhmlz77Jo6JJoAPPVIX6LmG4kJTWxWfwHUI23K8MLUDGB1Jv3fEhiWpORCHdzmdp8K7mmIOCU0mulzXodi4upBLBjLFhL34NdsydC/dRyxlNdj4KuA//DKWvnYbxlPY5kFh1OQBWks4cecBdSqozMnjwskLl3QuX7atQ8w/eYcDe4mXhi6dgih3/jQqIeT5AkmaDHH7ku40mJwfQyiO4FXQUYru4NCCbSw7imopxm4Dr84tJW/MKVTPC56m2Lg6mrUnbpWPErkgQh8xvGZ+pP2qd3zoeNFAczoi0XnTxX0S/dLiOA+eJU0qWK81Oh0hjDQdTVxbCU0qnqUpefeKgSbxQSZBE0eztl9OljVrdrIeRuupsaXJXlaD15yF0umvxcqBAYw3KrhaACX+JwVYg+AEy74VQi0I5EkyBKq6E5B70mAgWaRsbuNM0GSMwbomtMVOxtKFNU5Trrf32rqE1b5dbyZNVGSJ2kpDV5mHNInToHrxmKtcNPk24kAa9hIbAfV+SyUmN8k95ELqvDQUp+OFdK2xTXyO5dTY5pHwhyBzW/eQZpCbzjO88FmHmETvdoDCqMkDYlUhCVxelUbSHBTKheqwSHlVdsT6ZcI1JhzLC0cjgKHJ62E8KsmONXapd2CIBQyw3yiEeRdhRcMrGyPAEGJijV1XuWDDQGaaOE9NmPkDq3VeGMNEVD6VxOAxhQcleVqCMKuRXpNczN/ua6ZZYyDEjF3Lt5I6iPUakOWgiAwTk44wckGF4nTGbsPLberb1VgCb5S2AxRGTR5ge+LTufcoLwK10BjbZnAKtaVJOq7YCGBcqEmaVLwwhCT3ThBhLxvPBumKVttKTrC0gac1mGxDH7FxHWlS29qGGVS8uLowQRNgMABtvR7C6icfb6bQsLTXEY3hz4VNJDgFHzdKZWEvracmET40Gv7exm6o9+xSYVihjmhploY0iTkIdTSpdLnuI2zOU2HUvLJAugGR96AQpw3qZMy1ZU8qXIKrYVwdTUB8szYmHLp4JwT9RnQRbU18lFSFUHezcHwk7woivAjcCdaWF+JQHEUTsxFow16ChdUUMiPkQg2bpJJjvXRkHsfezWKtI5TOSw8NzPdQyeeuhyCJJ41KxM/NK5K1jijGLuXlphKFbZPaVbwtTTG6fPYRB4O2DaAwavIA3SlUevpNek10HgZtIqPuFCpMlPNJGnRJlEsl6BkS5SRhFV2Cskv4KfVgm+FEbrvoannh4AlI8orzPOnkooH3OZGz8kgkn0uTm611REizzjuhgiix1sKTFp3mmcRarVwQ8iiVKWoOnEK0suRz6xwh5XPIKxUoPsXo0siF9uV3Qi6sdYQpLKCSm03rBbU++uwjlKyK9xHDOtUGUBg1eQCVKMeFN6R3E6Q6ZiohJDFWr+TlHMNPtjQ7VYUk2uaZKGyZEOo1rhp+ImPzDnMrPYVq2wrmz+muDIG8cW1dEkKFXh4tuH4PR5dtiFaHd8q1ktDkMLd5JfFzciHxZnJy4eL1yGLNtg2HsjrC6GYbQGHU5AFei5SbEsQT5WwXOIdTqIRejmaffBxxVYgjL0yJcl6GiXtVCLmgExtQVA5uarugq0Is509695FT/oPECHAwDjOoLmQv+sxYR3yqC8m5Fx8aHA0Tl3HF6599Uq5Izn32Ec7wL8JPBUQQS5TzqApJQqMqxDdRzhTnNlWF+JxUpFUhtnkXXNJgFlUhjjR5nUJdkrWFi6PWUyPNtdJBLNfKgGM3cpfFnq8WMpaDSxKJdXkKDE2kgSc1TChjN2PvRMzYdU2sNeTg0cYuN7edZppZz6GjjrBPLPh4Z4l8RWmicNaeUMkN1sa26prdmgcrOSiMmjzA9m0NzWV1eiOAc78K2jKCHOpisNR7LdK3QlwW+2gz1uVOMPclULwIqEVK5YXQA0HlTsTG1ZQqR2/iyIwAiiazsUt5rXTymKApDGMvH6fbWsobQBulFE0KnjKUzeXgnjqiJpfrLpl0CmkSNLHhjURbKU3acR10nrxFV7PGsR48DS+Sb6t56Ahn7NLyqMuRpNch0ihlKx7NNAU57SOqsWutI20ChVGTB1Bue+5FWEnoir3C2jKUwMbXhScGLvvf5CFyyheQxrJ1eSTE6UnqfqW8cKYqF6qKjKtAk7qTibbk3ToUTQ28aVzOLW9bFZLJPSgm759jKIFLMo71S3h5sqRJ2DY0rTWkjhB6reoIKatcdSHh5bGlCWAMS0UHqDlwLYE23n0kCe/60uS2jxjvPnLdR9oECqMmDxDctSB/9FBYSQSk49W6U5t6D4OuEoLaNGN3OBDVT1zb5ClHRxMgi5GzlVNC123yOnwVr30agDJYGZrYy9AaTalKCKZKItDIRVUnF4KYP6CXR2oOOoh552i2zjUQVhKp41KVYFpZZXREVKnCyQWxXujuT1HHzU1HNDTpKmS08iZNPmeqhVI0MeuFVkc0NFFttRV1XGKtpV53CNc/Hc3SdddJR6h9RKcjehJaBYVRkweIHzHTtfWIzbsmOnIZ71FeDHPS10EW9044xbIdx+V4kQVNLomoPpUQlKeGktUOjheCZGyn5HNhfpEHLzLXkaxyk2xoAoRyYTKI3JLPs9ERQ7img7hROM8nTFzbZuHlzkXn/cP+dN5ZEX56RYEoaZBLDsvJFe2UHCYx0pxoynOxF5yuvBapBVgC3VQDTxgezFpH8jIOW1YCLZxblw3IWUdkhyCfwoLMD4Q2Cb0mmrx0JKcDbjvtI20ChVGTB1AZ4mK3oSahLbNLr+I0sQ/jeSxS8ofx7BJRxSXQthsQWwkhiZHnWAKtS+4TGnjW1U82D+NZV3ZIDDzOOBS6+7V0EeFDtgTa0Qhw0VthvpRTFZmrcSgNVztsqGSFU17eWY4uyfcAfA6lUUccDiMe+0h2lycW1U+vHLB231nGlD1OofISaJf8FJ+H8XQ0JcaVxu1Vuqi4sIYmp/t+kjFyq02EuIFXN66XESBd7Gs0NeVhPEuaYngv74QdHzMrgdZApvf9aG/nFcqU+BZdzvDX6YigXx1NamPxNQE+RoD0EJTICwTs12wnXiRpymMfKTw1BSRB+jAedQ8Kl/2vHbchrETFD/swHvMirOm0Yeo7pgS2r9QSNElPsC4GntTta7zvJ2TczQ5yITxVp947UvD6nBqFF6QngKkiM9LrUMGk8smWJkre1HEtXz62Gjd1Z4+6AblWIXEbuYuOCPNxEm297vtRvQC6O2585IK674ebA6mHKMmLmJfbcv6YCiax18pyHxHfau+yj7QBFEZNHqDdFKm3hRRIuv64l1e1rxdLT87c21D1X0lOXipe+i4RdSqL4YXfo62ckobTiHFt34GJtVXHtQzxUffysNUZuu9xWKRS8mhZtRPrm5tbx7eFpPKmHTdNU2jUEcuwF/UWmfRdIh2frHWeqUAjQhShdK2x1RG1BNqSJvH6J32DDtCENLn3jnR0UToiDS1SNCn4LPYR7r0+l32kDaAwavIAUca7T0JoDm5D0bjSsl9hv+1eFWKbj8NWCwlPoVqaBHzkkgZ9XOCZJ9b6uMdzqgrxqbqS5AhxeJ98nCaG4vhxifnTPpio4s03CudGE9fWx7MrmVsg+/tiPPKavLzcbQCFUZMHZGGYNL182jbvooFrQlWIS4xcQnOuSYOOfHSRC+FGrr8tVeqK5kJMhn5NbUVz66MjGVeCceNKNy/XfJw8cq1anlirCxN5rH95yZRXdaHAADe2zeAw4lOB5iIXbQCFUZMHkPF1oSsztxOsfTzaOuQibUtVz3A0NeMUmrWBl8XpyWeBm/PII2FpJm6dtf1eaVWIKaHX1K86rmXuBDeuvFrI8YqBrGkC/DxepGeKMAKkBp6Pd8LFMBHJo6+OGHAubaXe2WbvI20AhVGTB9i+iRN7ckXi5bG/74Jf7BNttfkcRL8czS5VIan3Z4Rv06jjavJ8As5bRvWtxSXepjG19XkTx/Y9MQ1efPme7iZqW6+j+O4jSz750CRtK9YRS5pcaOaMC5GOCEMUOp03tSXfhtJtmjId0eqm9G016ZME1m3pNbt9dMS2rce9SYWn5hUGEm8LVwLNJZOSr2k73E1AnYJiVn92lQPyXBDLO1LUcTmXvuvpl6sKYe/lMeH0ckFWLMRoSqPJO27EHiJh4mbUb2l+07e9qJAzdq2TLzXjau+Bkrr0uVBq0msl9OC5hGglOV46j7FKl0vYS1rVk7qnS+GFlmbiRmHKM+hDE9eWG1eydnJv7nnpiO1js4yOkPLI7CNtAIVRkwdQ2eUx96vAFc29u0HdJ8O98WNbscBmy2tOi9ocB01ViK4SwocmUWye805IK2R0dGnmQJKfEnsYz44m47s2SZp1NHFvC0nngJRHpm3ybSEXD0MHpyOUt4Xjo+CwonkTJ27s6nSE8P4xskp7dpnvoYxd6TtmllcXxEqgNfNH31+kk1UdL6Q6QuiXymOtTGno0nnLNOMGyVfH1bbkOqTgmZzClFzo1kapzut4YeJjG0Bh1OQBeSWTtippMK+3eLhLvCRGQC7VT5JYtkNbqVzYPkfhk/iXgUyxFzrajhsZCIb7fkT9uuRd+MuFNsQX69uWF8pGTnqeLPVHSlMeyaSi5HPOw6rpV5IX0yId0YbapBd9+uSdNXsfaQMojJo8QJo0aLuYtKoE2vaeBinOhxfiDchyUxSP67N5Ma5o5wRKem6dqp/yfhiPNfBsdUS4eVE05WoEOIYZAJlnN2uaPHjhUgJNPqsSK4G2DLFLjV2XtUYg514XfXKewaplOXie+0gbQGHU5AFUvJO9p4HwirCKKWjL3oNiuXByeInSAvIwUYQT5gj5GCZOHi9B5UfWnhrp97hUP0nkgjs5W3ueGF6QNEkNE0bOybyLfIwAUs4BWN8yLi2B9jF2fUqg21FHvCqNPPJXXL2zAGPgWfYLyPeR9ix+KoyaXEDqnYg2GU0MltqAuEVX21a42DfamvJTyERUy02TC8WlaNLgKoanAaQLnLVLX8U18oAImlmaNPlFtvMXO926PZMQT3yWyqPwxGdrTPnoSGxuM6QJmF/sdTrCylRj/nQ0J/qN4TivlYAmk1yk5NGQz+GlI42uLdc4Jx3JwDARrxcaXpFrNpOPY6vzmewjzNy67CNtAIVRkwfoTgVRbphSFSJyOTKJs1Q5OJd0q0saFLuTqevJ023j4xKPwXFtSRyXfNnAEQunSpc0mZSkS0iTlhe2SeDc/BA0Satc2GcSKLo4eUw8k5DEm2jiHvKjSq9dro+n5o99qiKdiKqnOdFObRsbV0KTSUcswwyUbkr5pKVLTfYlbhTWyQVJE4FjaVLHlXoziXGlOiJds63v+xHqng9NbQKFUZMHiF3RlmEiZQMivRNermjLvJhYWzvXevxhPEsXKpMol66EkNEU61tHcxauaLYqxDLkwnhMGnkKYZV7AFIDxPyIH8azndtYVQhVVmofoqXlQpprQPAi6/ATVw6eSUjMIeTsM64o/MTl4NnqiMf3iA8y2cqUFy8kNOUhF20AhVGTB3CJclSlRKMt53IkXNxkWyZRjg5R0Iqp/V5hVYgzL5iqEG1bYSjOiRfURXcUTQpdzrwwxu0dq004moWLve33qFUh1rzw+R6hEZC9jhC80JVAZ0RT4MULs6ySZeYMzRRNqpebukjSZf0LSD7KDBPqe1x4QePmt+7sdYRfw4rw0ysNsngYj0uUcy0ZziPj3SdBTzIu64rOqSokC1e0zbiuNMeqQlwvIvSoCml2FRnJ40YJdKhPoGyZjgjyIziPF2Wgc14cyti19R7F+qZocmmbl454FBb4VI6yOuKWrB33cjvuI6w300Eu2gAKoyYPiCXWetx2a+oXsHf7qgujK015uKJdecEZeJJERqfKDuGiS52uXJIVfV6BbpwkucRaHZDyqC6cGSSTSsf10pEM7j5y8VpJwod56kjWdx+50sTg2TCe6/xFnl2Pu4+cdESd95zuGXLVEe+7j9qz/KkwavIA6RslDaHRJTIyLkeyisLFhUrdeKq21SlBwz2uoyngNnIBzRRNprbJcVUeRziXC9rUqpCoQ4Jmpl81yY7ilVguLOde7TfUVXM50pR1W1uaAIOOCOdWQpMJl7mOCDxTLjpCrUOByifLkKZ2vfDRERNdDjT5tK1U5llhu2Y3RUcc9hGdTEnWoSKn5hUG2oXTsuqAyqRX+7Z+VsBAE+Xq1LkrqevWucoOclzL71HxMZokbnmDceHlTibmz/rSK8snB2J4zdMO2vmxpMnnWQGbii1JpRjVL0AbRGJ5s61+MmzGKR1BGseN66rzKp7VzQZOx39LvZWGXKQ0aemyrNrh8CRNwnCaS2Wiy5ptE+q22UesQ+jtVf5UGDU5APtWiCi7nKmEIPMFXCp+HCtvVLpsaeLw5AVSWeTUeIQKcr3E0LH6iW3rwAufCx0lsuwkFwIeAw464jG3rI4Qa4JwDsKs5SI3HXFvK35U1Hb+pKH7dpJzr7UzLx1haGoDKIyaPKAZyWGuSYNOyWHC07w2Ri4Me9nygqsKkcSFTVUhXgmu5r7F+QK24+oextPQ5FQh45qwy+Dlm5clzex7YoLvySPx2SvHy5UXGu9EZjR5JBn7JJ+7JhlzvPBZ/3LSkWzmgFj/gMz3kXaAwqjJA6SnHJ/kMNdrwp2Sw3xOT8JFyiVxU5Tcx9BkmzQtXaRckkkdE/8W9MN4GcsFe9+PhCaTgeeV+GymiXyZGmC8qGYdiF30aZtDpAs5S2n2OgQ1Lt/LVkf4nJqc1yGf5HMnHREWFmSsI+0AhVGTB2RhmDgZREJXtI8RkCVNXN9ebl+BaxZgrrSnFppmu5M9ZEpS/WQydkXhQYNx6FUV5xH2EpVAMy57sgQ6a70VhijI6rWsdcTdMPGaW2mo21KmxBd9enkzOZnSNCZDcfnsI6yxKwq/G3S+DaAwavIAsSta86ZREmfqm6zscHhDJkWTKZmNqoSwpEltS1Z2aGjS0RzDJWkyJZNaljnreKF9OkBCU6It9T3SfBzbttqr53XPKFhWEgEyuRDriJCmGF0Z60iyrU3CtStNKl5EUwJP8Uoib6YSaGsdIWhWINDRJF0fc9MRdZPXfI+EJtabmYWOCGmSjuvyPW0AhVGTB+hODOJyu2Tb5EaePG0Q/eo2TBNecutszDtBVcEw/dpWTonDG8IKjBLDC5ImYfmtLU2AZv6E/ero0lRdhTqcjhfQ4MUnOmnFDyMXVmEGg7GrlUfB3Bpptg19SHWEWS+oqrhm3gNF6YGUTzq8j86Tumk6BFnqvLWOMMaueK3h9hEfHaH2EQFNatvkYbPFUBg1eYCXS1joirZ1G0rjqCRNOYafnMI1jm5scVWISwWTh2s9twoMynvEyEVe1Vx5hU1UfFN1RHqqdtGRxmadU0jTpVoop7kNWq0jXtWFGa8XPuuf1z4ioInqu8VQGDV5gPgRM12inHSjsIz9ih/Gc4mRO9IkHdelrYQml3HzoonDe8hUlFPDJg06ylS7yYVrVQgrFx5VIa40qX3b0sS1zU1HfGgiXukGPPlocfeRrl+ftk7PkPjrfNMr6toACqMmDxDHMy1dwmrflsZHvBzc9qZVWkFIQ0yaNKiLr4vj0cTC6lMVQvLJoyrEIeFam2ugpYuSKeIWarYts3DaJlyz35NBVQi14bIVMtnqiDNNsXF9cjYsdVNaWOCVa0XJuc/3OOiIJC/G1NZLR3L6npz2kbhcGOhqMRRGTR7AWeeSGDl3UiFd0dyCYFntwGXw+9BEnXCTNKWSIC1OG8m4ryj5UsgnXS6I+PRkeI7ChiZhW+OL2NQtu3k/jOegI/zdR0TfBE2xcnDt3FN6S9OkvZVbQFNsXE3b1IEilUxv4dnQPSXCtZV+j9K3/d1Hlrfdej3OSoRyYm1tafLwmOSyjzjKY8zAK8JPrxywfmdJiANkp262lFKDp95UEV7u5kYToUAUTSpdTnwUtKVoMlaFZDG3Dq7oVrw/o94kTZVAt+x7stYRIU0kL5pIkw3NyWbcRZ+iqkXuEJStrAZZyJTR2JV4du3lPMhJR8h+Y23t5DHql6KrxVAYNXmA1n3nWP2U8jAQ3otUv0z2v+6kSblBTXSlTnxI48QZ/B6VRtLKAHZcol/pbZxONBFtKZp0eJKPBq+VrTxylRDWc+tY2ZH04FEeCNYtT/BKUv0EKLzwoInQocCmWihLHaGq13Q0OVz0GV2+lwlN3Lg5VZFpS/0N5eA261ASn9s+QtBkpKt9oDBq8oCSotS2ePX0qwOyqsdnXAFNJryk0shIE4Fn+Ui0ZRN6HcdVNy/bttIkYyde8G3DrMfleJHz97SXjnC8ICpKWJp8dISSR2Ei6itIR8wylcV6QeSdtZWOcGs203eLoTBq8gCPhNBMKo1cwgwSmkx4H5p8eCE5ObMxZUt3MseLVtDEtvU4hUq9E7bj5ikXotwkF154yEVuOuKx1njJo4AmI57y1HA0eSSQ5xUalj5J0FQd8Qj7++hIG0Bh1OQBzSiBFiUyWtAlTQ6zLWfNYpHyKYEWJ9lpcGyYwZaPDE2SBEpugSNLul1udab6FYbifKriMtcRoXeiqToiTXx20RGP5HPX+WN1RNJva3Qk8zVbegjyqYpr+j7C0NViKIyaPIARqCApUNoYuX2VC1lhwdHVkdhwY9UKaqIcddGT/cIpeh08amuK7Qou8TLlXVhn/6sbuYYuql+dy1ebE+BxiRfVlj3xaXgloclElyuPfWji+vbSEQFNpr7zoinV1pQvJZBHU4VgJhUyMh3RvqMkXR/Fnqcs5NGSJlPfraAJ8DN2ObpaDIVRkwdEcV/Dg18+MXKqby4WSrUVj0vRnDFNueYaCPJxNDTFqkKoPAUHmgIKz8kUwauoBNooj5K51W1AirFrO38MTSQvPHQk4GSKoosaV60QtNURcT6Hg47klYNC8kmYg6fFdeZDEzsut2Y78lE1dq11JMd9hJRHjzW7DaAwavIA1TpvgOm9nSR0JKbE5GHQ/SCJM528bGmCshno2iZxFE0c3oImsm2KpgRQ32M7bl40kXzk5tZCLii6KJo4umxokuBNNJkqO3RgrSMymoIgoOlK6bWhimz+B/P/ylRHNPf9mMBRHmP3/VjTZLkOWemIeW1N89hw348OKJyOD7o39zQ0cftIimYVqH4Bcm7JtV5HV/H20ysArBcLBbLcUFN4SjG5tp350EQuFrZGgMW4NoalzbgLjSZuXJ/5szFMsuqXw+c5B67jsjzOSS7yXC9cjQBWVnNaL3KaW62xK+23XXWemoM2gPambqFCnouujyB7LHDkCcprkWpDIyCv+WvVBuTV1mOxb5Vh0jIjwFE38zKWgPyMgNx0pEXrRcsM5RbRlNc+0gZQGDV5QJ7eCa/NqwWLfdtuQB58JDfrvIwLnwWuDU98ecpFy4wAx/lr04NMbjJFeTba0dDixm2ZjrThPtIGQMQUzLBnzx7s3r0b5XIZmzdvxlVXXYXTTz/d+PsnnngCd999N4aHh9Hf3493vetdOPfccyN8GIbYtWsXHn74YUxMTOCss87C1VdfjYGBAQDA0NAQvvzlL2Pv3r0ol8tYs2YNfv7nfx5vf/vb0dnp9An5gvXpVphLADTPCPDKu7CgySo/xSN3gs0FUcB2kWLzI6gxHfNIbPDWi25W8pilXDSBJg5vm48TwzVpA2Jz8IT9cm2tdUTYVhvmlupInsZ7ToegVnm589pH2gCsPTWPP/447rjjDlx22WXYsWMHNm/ejBtvvBGjo6Pa3z/77LO45ZZbcOGFF2LHjh3Yvn07br75Zhw4cCD6zf3334+HHnoI11xzDW666SYsWbIEN954I2ZmZgAAhw4dQhiGeO9734tPfepTeM973oOvfvWr+NKXvuT42flCrCok+iORHKZCChfvJ0gmnpkeoUs3TfedLNsmIJ2URo1L4JJ0kYmZ3KJL8Mq2LZm8Z2NMZUmTEGdNk0dbVqYoHMMLqi01ZpIoKx3h2gpp4uiiNhk2mZQy8HKiSde2CTrCJi9nShPFR4/vsRmXkketjmSzj2SrIws8UfiBBx7ARRddhDe96U3YtGkTrrnmGnR3d+ORRx7R/v7BBx/EOeecg7e85S3YtGkTrrzySmzZsgV79uwBUPPSPPjgg3j729+O7du3Y/Pmzbj22msxMjKCb33rWwCAc845B+9///tx9tlnY+PGjfiZn/kZ/Of//J/xzW9+0+PTc4ZW5F3ketpwGzdWAp0xTWT2v8eJnOwXcJ5brbErpMmLFx5zy4YDXMelqmN8+gUWXvK5l9HZhiFnDp+Xh6Fdw7styanJcR9ZTJ6aubk57N+/H9u2bZvvoFTCtm3bsG/fPm2bffv2xX4PAGeffTaee+45ALXQUrlcxutf//oIv3z5cpx++unGPgFgcnISPT09Rvzs7CwmJyej/01NTUW4IAgy/1+y3+SCEUDFpV2zUduOtPs13m9HAidvm8ar/XYmMMm2yXHNuBTNJXNbnYJEeE1okeaFB02Yn8NUv1bjJmhOygnFC82JXPw9VjQx41L9IvE9STkXzkGpVCJpJr/HIBfR75KybCMXmlJXow6wNBO6qbQrdXSkjDySJrXfTvp70npLrRe0rMZkmVkvrORCoUm3GQcxPDEH7PpnsV4kdURTrm+SR04uYmNTNPvoPLN22smFhY6oepjh/2zAKiFlbGwM1WoVfX19sb/39fXh0KFD2jblchm9vb2xv/X29qJcLkf4xt9Mv0nC4OAgHnroIfz6r/+6kdb77rsP99xzT/Tfr371q7Fjxw6sX7/e2MYX+vv7o3+/3NGFEDPRf2/s34iOlbVvHO3tw5jSbmVPD3rr+UPViZU4qOCCIIhyiwDgeE8PJhR8b28veur4mekJHFFwnZ2dsbZDy5ZhWsGvWbMay+r4qYPrcFTBdXcvwQal7eHEYrJu/Xp01/Hjq9dgRMEtW74ca5W2L3d2Ipybjf67f2M/Sj0r67zoTfBi5TwvelaQvDjWsxKTJl6cHGd4sTzGi9WrV9do6+/H1Lr1cV4sSfCiuxvqgwPr1q2b58Wa1TFeLF+2DGtivOiIPVfQ39+P0ooePS9WzstFZcUyqBoWBKUEL3rMvJg6keBFFyMXa+bl4qU4L5YsWYL1MV4sifFi/fr16DLIxfLly+O86OhEqNxy3L+xH6XlKwAA5d5enFDa9qi8WJ7gRSmIdK+/vx/HVqww82JyNMaLrs5O9Ku8WBrnxdq1a7C0wYt16xK8WJrgRXeCFxsiXpxYvRpllRfL4rx4qaMDmJu/7Ky/vx+lpctqvFiV4IWiI5VlS2i5WBGXi77ePqxo8GK8HOdFV4IXCblYu2aeF5Pr1uFYjBdxuTjUtQTqfbOqXKR4kZALdHTEbr/u7x9AaenSOi9WxXixcuVKrGrwYklXjBelEsOLvl6FFyMJueiK8eLIsmXKag6sXbt2nhdrE7xYujTBi24k795tyGuaFyviclHqiN3cO9Dfj6B7SZ0Xvc68OLpiBaYUfF/fvFxMjx3DkMqLrgQvli5N8GJeLpLf1wpowyxbGo4fP44bb7wR5513Hn7xF3/R+Lu3ve1tuPTSS6P/blh7w8PDmJszvIHjCEFQW1QHBwcR1m9ZDBPW5ZHBIwjGaypVnZyM4U6Mj2Py8OFau+mTMVwYhjhcxwFAZXomhh8dHcWJRtvjx2O4ubm5eNvZ+HcfPz6CUh1fLcdzomZmpqO2Osv+6PAwgiW1Dag6Ph7DTU1OxsYNE6fQwSODCE7U2lQnp2K4E+Mn5nlxMs6nNC+mY3grXiRkYGRkBMtQM5grifywmenpWNu5xE2aR48enefFiTgvJqemME3xYnAQQX0jryR5cUKRi8mJGC4Mqx5yMUvy4vjx4/NykeDFdJIX1TgvhoeHEXTVNuOkXExOTtK8ODKIYOnyGk0JHRmP8SLeb1gNMTg4GOngHMmLkRhuNiUX8e3n2DGVF2Mx3PT0SVIuhoeHEHTWNqAUL6bivEh6KAYHBxEsqW3klam4XIyrOjJ+IoZLycVMXEfKo2WMNdqOJHgxS+vIMQu5qEAjFx0NXsRluSEXjXUUpbhRMzh42MiLEydOYKLxPSfiNFWrSV7E5aJcHp3nRUoukjqSlItjZrk4GZeLiubi3cZekdaRibRcKEbN4cFBBF3dtX59eDE9G8OXy5RcxHkxR+iIbi/MAjo7O8UOCSujZtWqVSiVSikPSrlcTnlvGtDX15dKIh4dHY1+3/j/0dHR6LTc+O/TTjst1u748eP46Ec/ijPPPBPvfe97SVq7urrQ1dWlxWXJ7GS/Ud8JIyBEGF0rHWpcrPPGkBkHILX4haHSlugXAMKU21ChN9lvom3SqAlD5Xs4mjuSNLvyImB4EZrbBvG2yXHD+iIchiHLi6QLPAxh/h5VJnRtow7S36PODze3JC9SNCXkQhObj/BBmk/xuSXkgqOZkAsdnyQ6Eoahlia5jpjnIC0XSXkk5CJJc1IuCD6m5ILUeZ4ms44wchHK1ws7HWHaRh3occ3REaJvjhea3KXG2LyOdACKl5uSC6mOaNta7CPp8FNClpHYC5sMVjk1nZ2d2LJlC/bu3Rv9rVqtYu/evdi6dau2zdatW/HMM8/E/vb000/jjDPOAABs2LABfX19sd9MTk7i+eefj/XZMGhe/epX4/3vf38tJt/OQFUs2JTTJeOJVKkkW8bsUQmhyQmYxxE0cXT5lO66llZr8UKaOLoyLY0naGLlwqKtDU2pcanqDdtER2FlBytvLaBJ11ali+Wj49xb64hKU56JwhZJucn5cy2Bti1vJ3UkOW6Wa7bPWiOce44mSod89pE2AGvL4NJLL8XDDz+MRx99FC+//DJuu+02TE9P44ILLgAA7Ny5M1Zqfckll+Cpp57C7t27cfDgQezatQsvvPACLr74YgA1d9Ull1yCe++9F08++SQOHDiAnTt3YvXq1di+fTuAmkFzww03YN26dXj3u9+NsbExlMtlY85NW4DrRVw5VoX4VAuRVTCtqgppBU3cuHlVC7VrVVwrKjDasVKFw/sYAV53ivjMbU5VZHldntiONPm2da3ky5MmDt9isM6pOf/88zE2NoZdu3ahXC7jtNNOw/XXXx+FkY4ePRrLVj7zzDNx3XXX4a677sKdd96JgYEBfPCDH8Spp54a/eatb30rpqenceutt2JychJnnXUWrr/+enR312KHTz/9NAYHBzE4OIj3ve99MXp27drl8t35g2vZbyO7vGJ6br5F5apk2W8JpKPR9XbRUqlm5HEvzWrHpBepoKPDTLPHYh+UiH45umwfxhPSJDF2zbxwN0xIHnN0taxctQ2NgLw2oFaVt+d0fUTLNvK8vsdn3FZde9AG4JQofPHFF0eeliTccMMNqb+dd955OO+884z9BUGAK664AldccYUWf8EFF0SeoAUDPqfuRKKcuN8cvRNe96D4Xj0/ZzBq2nEDyvthPBe5aEfPE+A8B1Hpb9XB2PUx8DzvPnI28Ij5i+4+MuUveN59tKiM3VYZAa24ZyhPXrS5p6bNE1MWMOQWGmmRKzqvy5ryOiHluhm3YduFdqrm8Hkt9u1IU57j+h6ujG1z3Mgd1ynTPTeifrnwe046wt6g7OvldulX4OVuZyiMmrzAKtExmcTluCBkuSkmaNJdyJTJuDYnPjYB2YKmLDcRMsmOGZdKJrWhK8tTqBVNTTqF2iRuNsubaZNw3ay8CzYBmcJltA7p8F7JzWoBQE4eCGu9bQJNQJvuI+1tNrQ3dQsZqDdxrPIjbCqYuCz8rGhixuVotqk6oOjK8r2jQIizHZebA+e2Njy2rM6g+s1VHhV82+iIShOz2GuuIBDRpB2XwKXGdZw/ax3x4KO030zbevA42a/rOqTBByQfk22J9dErzy6nfaQNoDBq8oJWhEZyTBpsSfUT0JqkwXatNHKNobdr9dOC05E8Q0ieeWcuuFYlGefp2XCdg3YMDwKeet0CHWkDKIyavGCxlUB75fK0Yb5As+7gsMHnZQQ0Kzxog+PweS32C7EEulV5Z+2Yp1XkncnHbUXeWRtAYdTkBVQSV6uSw9rQCGCTztq0BNqpX4CtCiHB+eSVp+epVd6J9vNmLjjDslUGeJ5z4KrXPl7Sdsk7S+ELT00BWcIic0UHJSJRuA1PfKbXcUX9ernW2+/Ex1aFeNBEGmKeJdAkOC72UQm0A02vOGPXS0da5LVy9dLlWg7ehl7uvGhqAyiMmrzAyjBpgyqKdqQpibdJcOXaZlr9ZPPcBMHntjnxtTlNQHuGNFvhYUi2tagQTBm7Ns8V2Bp4NhWCRBIr6+V2PQRZl0DLq5/yuuMrbexmVP3k4+VuA2hv6hYyUAJntRkncUxmutViL6cppUDUImXzXpXP5sWVNFotJsQiZVMqmeXmRZUxsxtQRkZAU+fWVUcsaErR5SMzVL8JujiaKD6zpbsWhpiPjuRFU15vq7E0UXqdIU2ZHibz0hHFcLT2crdX+VNh1OQFrXL75pUo1zIXak7VQnlVheTJx7zi4G2Ya5VriNb1exdkPo6PPLaGJq+QZkt0xGMdapaX24Ymti0zbouhMGrygpYtcPkkCvuVdOcUXy8expO3XWgP47WjEdCOlSrcuHkZAe1IE8Csfz40L0DDpFV63WIojJq8oB1LQ72eSVhYicLsuO24kef1PT7j5mUscfh2NALadgNqwd1H7UgT0KY60oZykVfBSRtAe1O3gIFMDmtVVQibKGeOjfoku72iqkLyNAK8NgpXY7dVpbtteAptV8OkFaHudqSJHTcnI6BdnkmwwReemgKswUagbN40apbb1+atnaZVhdhWP6mVHfK29g/jyauF0lUhPpVTSjdZLqxq0mCrHsaj+ASQ89O0u4+srvf3MPytNq/km202Ou/zfAZFU4Iu283Y5201KY7DU2t2O9Kkw8dwOe4jLYbCqMkLqOoAG0FOZfATYSC2LbNRUJtmZ2Lc2PcQOG2/Nm0twl65tW0RTeS4TL9k5ZSHTLE4am6ZtrHKKY4mBc8a4MT3ptqa5bFWFULNH/G9nbZykVWlkeUcCGkKSiUPzwZHkwWPLar80hWclFxY9Jsy/C3WXYam1EPCGe0jqfXcoi0rjy2GwqjJCzq6zDhOoDqJtjaLvQ1NAE2XD01kvwuwLfW9edJktdhnOC7Ji5xkNS+auHHz4qOX7nmsF7m2dfxeHzn30hEPuchrvciVphbtIy2GwqjJC7oopc7JuODwHkoQeH3PImvbKl5Q41I433FdZYqjKTc++mzkOfGxbeU8p7Wmy6NfH7kgdYSRi1boda40tWgfaTEURk1e4GFhBzmdnsh+fdpyblDXUww07leLtvT35OQVydNj4jVuTidyH7nIzeOVp1fEsW1eJ30On6dcuJ7m8/TgtaOnJi+vYqu8cJx+tRgKoyYvWIiCTI5LeHGCoDWLfRtuXtZ5FxY0kYaYx9yShiPTluRFqYPOu8jL2PUy/HMyILg3pzzk3EsufHjhutbkeAhqSZgVcNdrL1606T7SYiiMmryATBpMCkVisYvhk0laCWGzSnYjaALigpxMLEu5QRm6KJzadyoBOUlTpwynpcliQUjyqpPgow3N5Nwm8D5zy/AisKIpAYRcsIneFB9t5CJFE6UjDE2kjmQoF2pibcrYtdBNG15YyoVX8rmVjiR4YTN/IOQi0TToIGQ1UwOPkovETwmZCkoluyR+Si7IZODs9pF08jLaCgqjJi/I68TAxWDzOs1zbckYuoe3xceNnVfehdcJyYMmKr7eKtc6lzfjmjDqMz9sflFe4Q2P+fPSEY/TfG46wvHRNWfNYx1qVeI6t2a7jusjq3nuIy2GwqjJC7yqhTw2r5wSUYOubrotuXnltJF7bV45GgGt2Ly8aPKQKa9wQE5JkF7Gbo7J2q0wdr02r5y+ByDpokOLPkZ0i5K181qzfWQ1z32kxVAYNXkBmWuQcDkmoQ3zLvLKXwlKHaAvd3PfvFqWcO06fznmXeSV3Jxf3oXHST+vpE7A0yviOK5PwnWupeQ+fMzpEJRbTk2LvDwLjaY2gMKoyQu8DIi8DJMWbeR5JfflGSpowf0dbJJxbhtQ+1WF5MmLICeDyC/JmJALn4Rrlhf5GES8sZvTIciHF7kdgvIyatq06rTFUBg1eUFeJyQvt7zHIuV1n4JPuK0FYSAO37ITUqvyi9rPQ5SfjuTIx9x0JKd+2zHvDPDQkVZVC+WTE+VVdZpn6L7FUBg1eQGbLU9l6au4RDdstYnNIkW1TeCSOTUUXSxNFtVeXQRNyX69aIIcz1WCdRHzx9EsNVyS7ZI0kVU7HE0WsmpTUcfNn7OOcJUdOdGUBI6P1PzZ6EiyX5uL7qyquXz01sLAs6icStFlU9mWpCmFF8qbFk+M26o1O7d9hJGLFkNh1OQEPhfd5eZy5O67IKufcnRFL7aL7lriqeHkokWemlbkbLQspyZHr0heF921Kr+oFXrdrve6tMLrWOTUFGANecXXW3TRXW5Gmkfbtr3ojqK5VRfd+SRcv6IuumuRXOS1abJJxi0yiFqRN7gYL7rLLeHao22LoTBq8oJ2vEMl10Uqr/JBD5pzK2n04EWubzS1wrORT74APy5j7LrmH7Xq4cmcdJM9BLneL4X8EmvzyqlpXdWpu46whyBXucjxENRqKIyavCBXN3ZOmxflIfK6g8Pje/JyY7cjTdy4PvOX12bctqGevHSkDU/zPuXveZXGt2u58QKrOl2QOtJiKIyavCDXS7xycjn6XL6X12neg+aW0dSqW1pdv7e46C4jmlrk8fK46M6PJh+ZyqntK+2iu5bsI0y/LYbCqMkLkouFRSVE3OWoyQ3oJPAqLvlmR9LlaPPeB5PxHsd7vJVEjavLk6B4RfEpmXfhUxVCvbmS4VtJ5FsuSbpsKsE4mqxO8zZVV4xckDTbGGI2lSqJ8EQraNL17fo9HF02J3KrijoPOecq6lx1nhmXlousaGLwtpVgVEVdTvsIS1OLoTBq8oI8XY4tiM3n6opuQWyev9ytBXk+QEvu7/C76I6rqMuHj16JteRFd0zehYeOkDkoHqd5v8vdKF5wF925zy2dj+NTabm4qk5bdtt3kVNTQAoW4vXkXiXdbRibz6001IemHOWiFXH9BWns5jR/rZqfdszxattqITeaW1V12o5XJrCHoBZDYdTkBV6K2aLTvNeDlj40ecSFW/FQoxdNeXrhWpDjkGe+QMt0xDFvJidPGtu2HW/7zpWmVhnKrjrShnoL5KcjLYbCqMkLFpvLsV0z+NvxortW0MS2bUNPTZ5ykZvnsAWhX67vXL0iLfDUtGG4mm27ENfOvPS6xVAYNXmBlVBkeIV/q1yOucXXlba6fDRpfF3X1vF70gnXcpq4K8bpi+5yrFKikgrJezTyfGCQ0pE8DX8pzUyScaqtu0Ek1xEuIZTol03ET/ZrYfhzyaYqcM+Q5OVhsArd+zx1kCFNee0jPgZRi6EwavKClFBYKLXNIpXKeCcUT0uXkCaOLjFNTFtrmohF2WoxsawoEZ9SPTZjm+oMILuqEBuaODxpLDGblxdNhFz4GHic4e86f8l+LSoEM5Mptq28X6+L7lgDT0oT17fFgSLZN4VLVrNyhyCLA0XAVqWaaKL1Non2MnZbDIVRkxdYWf1JXItCLsVFdzIc4J4wuiCTY3Mat21DLj666Zh/lGceyUJLXM/1DqlXko60aB9pMRRGTV7ATbrPpUl5JcfmRXOu4bQWJdm5Lsq5JpNSZb8eMpXX0w55XnTnJY8ec5DXRXe5yXkb6pfPOpQnHxecjrRoH2kxFEZNThCUPJJJOfdrThY2u/G50ky9McLQxCZc55TstvgeGPSZ25xOzj4J121bMpzTRt6q5NiWeCfakCbfcZ29Vm1IkwTfQiiMmlaBVJCrFbu2LasWIgwE7sZJKU3VMDOa2HFzWhxZXkj7DavpvnMyiLwSa7mL7lz7jfEiLRckzR4GUXZJxol+fQ5BDE15PTyZ20V3eR6CFlrVaasqmHz2kRZDYdTkCZQgdy+Z/3dyo1NxszPptl1L0n/TtbXFUzRxbSmaAHqhktI0p+GFlCbb7+lm7uWJtSXmj4MkWVKaZufS+CUZyYUNTQBA3WHE8ZECKU0VDS9ImrLSEcu23F1PFEhp0vGCmgOVpjbRkZThT62PFE0VzYGQakvpT7KtDU2AszyyhyApTbpDUF77SIuhMGryBFXBkrJJCbmKm9MtUtRiQYyZwid+wC260k1Gp4hdUpoJmmZ0Bh7xvaxizrdNkeyzSFE0JcalcUleqMbuNDOuBY+TfVP96j7Ix1CmKjSkNOnkQrrxaTdySh4t5ILaZHTjxkrjLfRapUmrIx6bsdgw0ckUpSMMLyig5JE7EFI6z9Ek1PmA0xFSN81DcOOmcYxcSGnSASfLLYTCqMkTpIt9AqyscwpX1VjnjjRxbcl+AWYjJ/rlXNEt4gVJc1aegGS/ag6K5nvE/epObT4nWAKfGy9UY2hu1r1fXehqgclFzDCc0Ri7C5IXxHpByaPqHffhhQa8eOGjI1SYiPQM5scLLw9szlAYNXmCj9CQ/QoNhGYKspUr2nJc5349TvPcidzne3zcvhRIaZrVGAE+oavcQj0+4RohTT4GkS2e1ZEmhCioMbVeYaFH0nrcfHQkxgsfwz/r0FUzwqEJiB0ItXIupMn2QNhiKIyaPIEQ5CCrXANqTM4VneqXoSmvGCxnQJD9ChddbW6Sz0KTk4HnlXchpEln7ErDadq+Pb7HZw4oEOuIJS9yMkxYfG5ywYRrcsvf85EpD7mQzq32QCgM9Wj79tGRvAx/IY+1OlIYNa9MaIF3InZS0eVdeIRr/E5elIGXEy/Uk0o7ueVbwgsldOXlwbPkhU3ypU2/HEhDV9q8ixaFKFogF7EqlnYK1+TFi4UYuiIMl9zkQg1d2epIi6EwavIEdeJtE/SoOCpXsdAAbchFmMjICjKVZMwlylnQBIB8W4iiSQXOwKMS5ThekHPLJKJSiY62+XfShcZ2bm3kwjY5lpQLpi0FXjoipMk2id/mNG8rU9J+k08D2ByCKB3hqq7IRHxaR1LhNR+5INrGeeFu7Ia6azio9XGJhY5QOK8kY80zFw2wTTJuMRRGTZ6QU2w+4E6/DdDFUam2nFs+t1yDJpzmrXNq2pUXGYRrrBdsCwPPliaftq4JlCro5jY3HckxB0Vs+BNgXUXGhChaJReu/argI+e6tlldt2Db1rVfFWxDcS2GwqjJE3xi83klkxL9xk4qtoKsKq3tXRk+G3lO+Ths6EpaSqmpNMpr4wukeRfaJEiKF0roato210DB6UKaOemIl+FPhT64SiPqe9hQXD65PGK5sNV5LnQlviZAc6mml45QvMhpI/epQLO5JsAGB9AXSUp1xPZ7WgyFUZMj5BVHze0SLxU0gkx+D5OUm1dpaG6Jcip4nWA1vPBJjm1F5ZQKtptXLFzTxEojL2M3p8U+ds9QG3m8HPvlD0FSucj48sQsdMSy+inGC53hL/2ePC8MtKFJBVu9bTEURk2e0IaLVDPCNaGPEWCLbwYvbBOufZLs2nDzioFP6Eqz2Odm+LfKwBOHrppp+LejXKgGXtZysYB5sdjCeC2AwqjJE0iBWjr/b1tLeMlSM44Dqctxespq3PhJRdNWetpgky/lNLEgbXvypFVbNXQVntTxUfg9lmEiH14E3cK20zpeECdYJXQV2spFN8cLjxMsAYGUjzpeEHxUQ1fWvFBpsq1M9JELKR+1OkLIhRqu0bWVzq1teNdHLrx0hGgb40VaLmgDT+lXE8XLbb3w4GPeUBg1ecKKFcp/JDLEly6b//fECU3blUS/PfP/1pQbY9mK9N8asFxpS6XL6zZjdVwq431inG6b4sVypa2OFz3pv0U4gk8ArbhSXkymvyegaGLaYrlCc5KPyxRe6NpK5UID5OIonltGVq3lgtCR5QrOmheMXFAP8knlgpNzqiiEaZuq+FF0OpyaoMcl+9XgqTlbzvCxAZPcGmYeI7SdW3V+piY1baXrhWXVjljO6TUsObfqf4fs2kngtHIhXC9sC5ikOtICKIyaPGHFKiMqlog6nlaCoIcSRqVfnVJTbSkcB8T3xECr1EJeaNoGlGKq33NSt8BRbYXfowNu02yAZm6pOYgl5erkglyklO/ReAJKK3vNbaVyobtdVMoLW7nooHlB0qzSpDs5Z6EjmoR4Um9VsOWFepofH9O0lX1PqAtvSPWLAl04Tawjmu+hdES9T0YnF0JjN9SFlZcTBoSUFzpvi9faSfBCDWla68g8TaHulnH1gGXTb4vB6f3wPXv2YPfu3SiXy9i8eTOuuuoqnH766cbfP/HEE7j77rsxPDyM/v5+vOtd78K5554b4cMwxK5du/Dwww9jYmICZ511Fq6++moMDAxEvxkfH8dnP/tZfPvb30YQBHjDG96A3/iN38DSpR7hh7xBOPGhtSArSj1hWOCGB/WNVeXShb0IEC/YtsqlQGhrBEiU+viwvrG62OsqtiiQGkSm+RGAVi4oXqguYQ0vSqt6UTk2pG+syoU1L/LZvFTQ8SJYsVLrcQcQ99CZ9Gt0RN9WnR/dnSMUSDcvDx3RGv49FC8Ur7BpXN3cJGnShXoo8DLwcuKFulGbeKHzHCVp0nnIKWiCjpgOhCJemOZAd2gGEnJuyYucwdpT8/jjj+OOO+7AZZddhh07dmDz5s248cYbMTo6qv39s88+i1tuuQUXXnghduzYge3bt+Pmm2/GgQMHot/cf//9eOihh3DNNdfgpptuwpIlS3DjjTdiRrH+//zP/xwvvfQS/uAP/gD/63/9L/zwhz/Erbfe6vDJzQP5ScVjgbNtqwhyqFMgqXVOKbXGDZrXaZ5vS/FCCW/o2lKhK4+5lRuHOoPInRekp0YNA+lc4FSlhEoTJRe2XjgVbI1d1cXv4+XRbXCqB4nql1rrNcauFy8oL4/KC1sdUcNPk5rwBhWGaYKOaNewvHjRw/CCAq/1z+dA6B4tEPNCF/ZqIVgbNQ888AAuuugivOlNb8KmTZtwzTXXoLu7G4888oj29w8++CDOOeccvOUtb8GmTZtw5ZVXYsuWLdizZw+AmpfmwQcfxNvf/nZs374dmzdvxrXXXouRkRF861vfAgC8/PLL+N73vof3ve99OOOMM3DWWWfhqquuwuOPP47jx497fH7OkJMrmmtLLY5cqId23TJhLwqacVLRLo5CpT7h7tK35kWLjF3KqIkl9Np6l1SadKGeBugSXFukI6QnVDFatBuFNJdHF+ppgC4hPidvZgxsNz41udk67KX0S/HCNiSmQsbrBc2LeQ+5VkeEoSttCLAButBVizxeUg+5Vh5bCFZGzdzcHPbv349t27bNd1AqYdu2bdi3b5+2zb59+2K/B4Czzz4bzz33HABgaGgI5XIZr3/96yP88uXLcfrpp0d97tu3DytWrMBrXvOa6Dfbtm1DEAR4/vnntePOzs5icnIy+t/U1LywBEGQ+f+0/a7dMD9mqZRus74WXgt+6pwUrnTaGSS9Ee60M9L4nzpH1nbDQBr/736uhlzZm2pTUq8uX9GTbrvtZ2rIrf82jVu3kebFug1mXrx6q5AXp3vwoj/9mwYvVvWlaVK8OMHyFem+X/fvarizXp/Gre9XeKGhqS43wWvTbcW82PyaFG7puf9e1na9Ri5+ps6LvjVpnMqLZcvT+H/70/XvOVvDi/kQs1YuVq8z8rG05czY9yS/K8KdmuZF6XXnxtoaebFuo4YXb6whV69L45QCgGDpsjT+tWfXcBo5DzaeQtPUuwYAUDprWxq/5SzZ95y6xZkXJR0vtv98Dbl2QxqneH21vDirtuYHrzs3NlYQBAj6X0XTtKrPKBfBa5rAi7UaXvxsnRc6PikGT7Bkabrfra+r4bb9TLpt/yaaprohFpz5ujT+9NfKePFvXp3mRWM953ixZn3q77rf+/zPCkILOHbsWHj55ZeHzz77bOzvX/jCF8Lf+73f07a58sorw8ceeyz2tz179oRXX311GIZh+KMf/Si8/PLLw+PHj8d+88lPfjL81Kc+FYZhGH75y18Or7vuulTfv/mbvxl+5Stf0Y579913h5dffnn0vw996EOyj8wYJr/5WDj55ONa3Nyx4XBs991hZWJci594dE948odPa3EzBw+EJx68J6zOzKRw1Wo1PPHQveH0j5/Ttp1+4dnwxD/cH1ar1XTb6ZPh2O5d4ezgQW3bk898J5x47KtaXOXEWDi2++5wbuSYFj/5jcfCqW8/ocXNHR0ieTH+yENuvKhUwhMP3RvOvPi8tu308z/ieXHkkLatiBfl41r85DceC6e+48OLZ7S4mZd/UuPF7GwKF/HiJy9o204//8Nw/B/+TsuLyskpkhdTz3w7nPjnf9TiKmOjDC/+KZz6zr9qcbPDR8Kx3bvCyuSEFj/+tQfDkz8y8eJFmhcPfjmc+cl+bdvp534Yjn91N8OLw9q2U08/GU7888Na3NxYuc6LES1+8l+/7s6Lh/8+PPnsXi2uxosvZ8+LqTovhgy8eOpb4cS/fE2Lmxsrh2N/d1c4NzqixU8+8Wg49d1vaHGzw4M1XkxNavHjDz9g5sVLP67xYk7Di7m58MSD94QzB36sbTu97wfh+D/u1uJqvLib5sXjj2hxc6MjNC8efySc+t43tbjZocMsL6b3fV+Lmzkg4MVLP9a2nd73/XD84Qe0uFZCEIbyjKfjx4/jfe97Hz7+8Y9j69b5E+MXv/hF/OAHP8BNN92UavNf/st/wW/91m/hjW98Y/S3r3zlK7jnnnvwmc98Bs8++yw+/OEP49Zbb8Xq1auj33zqU59CEAT4//6//w/33nsvvv71r+OWW26J9X311VfjHe94B37pl34pNe7s7CxmlYzuIAiwbNkyDA8PY07n+vWAIAjQ39+PwcFBWLBzwcBi/z5g8X9j8X0LHxb7Ny727wMW/zfm9X2dnZ1Yv3697Lc2Ha9atQqlUgnlcjn293K5jL6+Pm2bvr6+VBLx6Oho9PvG/4+OjsaMmtHRUZx22mnRb8bG4jHMSqWC8fFx47hdXV3oUquEFMhLmMIwXJSC2oDF/n3A4v/G4vsWPiz2b1zs3wcs/m9s5fdZ5dR0dnZiy5Yt2Lt3b/S3arWKvXv3xjw3KmzduhXPPPNM7G9PP/00zjijljOyYcMG9PX1xX4zOTmJ559/Pupz69atmJiYwP79+6Pf7N27F2EYkqXkBRRQQAEFFFDAKwesq58uvfRSPPzww3j00Ufx8ssv47bbbsP09DQuuOACAMDOnTvxpS99Kfr9JZdcgqeeegq7d+/GwYMHsWvXLrzwwgu4+OKLAdTcVZdccgnuvfdePPnkkzhw4AB27tyJ1atXY/v27QCATZs24ZxzzsGtt96K559/Hj/60Y/w2c9+Fueffz7WrFmTARsKKKCAAgoooICFDtaX751//vkYGxvDrl27UC6Xcdppp+H666+PwkBHjx6NZSufeeaZuO6663DXXXfhzjvvxMDAAD74wQ/i1FNPjX7z1re+FdPT07j11lsxOTmJs846C9dffz26lWqb6667Drfffjs+9rGPIQhql+9dddVVHp9eQAEFFFBAAQUsJrBKFF4MMDw8HEsgzgKCIMDAwAAOHz68KOOki/37gMX/jcX3LXxY7N+42L8PWPzfmNf3dXV1iROFi7efCiiggAIKKKCARQGFUVNAAQUUUEABBSwKKIyaAgoooIACCihgUUBh1BRQQAEFFFBAAYsCCqOmgAIKKKCAAgpYFFAYNQUUUEABBRRQwKKAwqgpoIACCiiggAIWBRRGTQEFFFBAAQUUsCjA+kbhhQ6dnfl9cp59twMs9u8DFv83Ft+38GGxf+Ni/z5g8X9j1t9n098r7kbhAgoooIACCihgcUIRfsoApqam8D//5//E1NRUq0nJBRb79wGL/xuL71v4sNi/cbF/H7D4v7Edvq8wajKAMAzx4x//eFG+5QEs/u8DFv83Ft+38GGxf+Ni/z5g8X9jO3xfYdQUUEABBRRQQAGLAgqjpoACCiiggAIKWBRQGDUZQFdXFy677DJ0dXW1mpRcYLF/H7D4v7H4voUPi/0bF/v3AYv/G9vh+4rqpwIKKKCAAgooYFFA4akpoIACCiiggAIWBRRGTQEFFFBAAQUUsCigMGoKKKCAAgoooIBFAYVRU0ABBRRQQAEFLApY3A9QNAH27NmD3bt3o1wuY/Pmzbjqqqtw+umnt5osFu677z5885vfxMGDB9Hd3Y2tW7fi137t13DKKadEv7nhhhvwgx/8INbuF3/xF/He9743+u+jR4/iM5/5DL7//e9j6dKl+IVf+AW8853vREdHR9O+RQe7du3CPffcE/vbKaecgj/7sz8DAMzMzOCOO+7A448/jtnZWZx99tm4+uqr0dfXF/2+Xb+tAb/1W7+F4eHh1N9/6Zd+CVdfffWCm78f/OAH+Lu/+zv8+Mc/xsjICH73d38XP/uzPxvhwzDErl278PDDD2NiYgJnnXUWrr76agwMDES/GR8fx2c/+1l8+9vfRhAEeMMb3oDf+I3fwNKlS6Pf/OQnP8Htt9+OF154AatWrcLFF1+Mt771rS3/xrm5Odx111347ne/i6GhISxfvhzbtm3DO9/5TqxZsybqQzfv73znO/Erv/IrLf9Gbg7/4i/+Al//+tdjbc4++2z8/u//fvTfC3kOAeAd73iHtt2v/dqv4S1veQuA9p1Dyb6Q1dr5/e9/H3fccQdeeuklrF27Fr/6q7+KCy64wPsbCqPGAx5//HHccccduOaaa3DGGWfg7//+73HjjTfiz/7sz9Db29tq8kj4wQ9+gF/+5V/Ga17zGlQqFdx55534+Mc/jk996lOxxeOiiy7CFVdcEf13d3d39O9qtYo//uM/Rl9fHz7+8Y9jZGQEO3fuREdHB975znc29Xt08G/+zb/Bhz/84ei/S6V5x+TnP/95fOc738Hv/M7vYPny5bj99tvxyU9+En/0R38EoP2/DQD++I//GNVqNfrvAwcO4OMf/zjOO++86G8Laf6mp6dx2mmn4cILL8T//t//O4W///778dBDD+G3fuu3sGHDBtx999248cYb8alPfSr6rj//8z/HyMgI/uAP/gCVSgWf/vSnceutt+IDH/gAAGBychIf//jHsW3bNlxzzTU4cOAA/u///b9YsWIFfvEXf7Gl3zgzM4Mf//jH+NVf/VWcdtppGB8fx+c+9zl84hOfwJ/8yZ/EfvuOd7wjRq+qs638Rm4OAeCcc87B+9///ui/k48VLuQ5BID/9//+X+y/v/vd7+Iv//Iv8YY3vCH293acQ8m+kMXaOTQ0hD/5kz/Bm9/8Zvz2b/829u7di7/8y79EX18fzjnnHL+PCAtwht/7vd8Lb7vttui/K5VK+N73vje87777WkeUI4yOjoaXX355+P3vfz/62x/+4R+Gf/VXf2Vs853vfCd8xzveEY6MjER/+8pXvhK++93vDmdnZ3Okloe77747/N3f/V0tbmJiIrzyyivDJ554Ivrbyy+/HF5++eXhs88+G4Zhe3+bCf7qr/4qvPbaa8NqtRqG4cKev8svvzz8xje+Ef13tVoNr7nmmvD++++P/jYxMRG+853vDP/5n/85DMMwfOmll8LLL788fP7556PffPe73w3f8Y53hMeOHQvDsPZ9//W//tfY933xi18MP/CBD+T8RWlIfqMOnnvuufDyyy8Ph4eHo7+9//3vDx944AFjm3b5Rt337dy5M9yxY4exzWKcwx07doQf/ehHY39bKHOY3BeyWju/8IUvhL/zO78TG+tP//RPw49//OPeNBc5NY4wNzeH/fv3Y9u2bdHfSqUStm3bhn379rWQMjeYnJwEAPT09MT+/thjj+E3f/M38T/+x//Al770JUxPT0e4ffv24dRTT425Hc855xxMTU3hpZdeagrdFAwODuK//bf/hmuvvRZ//ud/jqNHjwIA9u/fj0qlEpu7V73qVVi3bl00d+3+bUmYm5vDY489hje96U0IgiD6+0KePxWGhoZQLpfx+te/Pvrb8uXLcfrpp8fmbMWKFXjNa14T/Wbbtm0IggDPP/989JvXvva1Me/A2WefjUOHDmF8fLxJXyOHyclJBEGA5cuXx/7+t3/7t7jqqqvwoQ99CH/3d3+HSqUS4dr9G3/wgx/g6quvxgc+8AF85jOfwYkTJyLcYpvDcrmM7373u7jwwgtTuIUwh8l9Iau187nnnov1AdS+L4u9swg/OcLY2Biq1Wps4gCgr68Phw4dag1RjlCtVvG5z30OZ555Jk499dTo72984xuxbt06rFmzBj/5yU/w13/91zh06BB+93d/F0BNYZPf3wi7lcvlZpGvhTPOOAPvf//7ccopp2BkZAT33HMPPvKRj+CTn/wkyuUyOjs7sWLFilib3t7eiO52/jYdfPOb38TExEQsJr2Q5y8JDXqSYd3knK1atSqG7+joQE9PT+w3GzZsiP2mwYNyuZwy6lsJMzMz+Ou//mv83M/9XMyo+Y//8T/i1a9+NXp6evDss8/izjvvxMjICN7znvcAaO9vPOecc/CGN7wBGzZswODgIO68807cdNNNuPHGG1EqlRbdHH7961/H0qVLYzk3wMKYQ92+kNXaWS6Xtbo8NTWFmZmZWJjcFgqjpgDcfvvteOmll/Cxj30s9nc1dnvqqadi9erV+NjHPobBwUH09/c3m0wr+Omf/uno35s3b46MnCeeeMJLYdoVHnnkEZxzzjmxhNKFPH+vdJibm8Of/umfAgCuvvrqGO7SSy+N/r1582Z0dnbiM5/5DN75zne2/fX7P/dzPxf9+9RTT8XmzZvx27/92/j+97+fOrkvBnjkkUfw8z//86k1ZyHMoWlfaHcowk+OsGrVquhkoYLOSm1nuP322/Gd73wHf/iHf4i1a9eSv21UdQ0ODgKonRyS3z86Ohrh2glWrFiBU045BYODg+jr68Pc3BwmJiZivxkdHY3oXkjfNjw8jKeffhoXXXQR+buFPH8Nehr0NSA5Z2NjYzF8pVLB+Pg4Oa+N/26Xb24YNEePHsUf/MEfpEJPSTjjjDNQqVSiapqF8I0N2LhxI1auXBmTycUwhwDwwx/+EIcOHdKGnpLQbnNo2heyWjv7+vq0urxs2TLvQ2dh1DhCZ2cntmzZgr1790Z/q1ar2Lt3L7Zu3dpCymQQhiFuv/12fPOb38RHPvKRlKtTBy+++CIAYPXq1QCArVu34sCBAzHhfPrpp7Fs2TJs2rQpF7pd4eTJk5FBs2XLFnR0dOCZZ56J8IcOHcLRo0ejuVtI3/bII4+gt7cX5557Lvm7hTx/GzZsQF9fX2zOJicn8fzzz8fmbGJiAvv3749+s3fvXoRhGBl0W7duxQ9/+EPMzc1Fv3n66adxyimntEXYomHQDA4O4sMf/jBWrlzJtnnxxRcRBEEUtmn3b1Th2LFjGB8fj8nkQp/DBnzta1/Dli1bcNppp7G/bZc55PaFrNbOM844I9ZH4zdZ7J2FUeMBl156KR5++GE8+uijePnll3Hbbbdheno6k1r7vOH222/HY489hg984ANYtmwZyuUyyuUyZmZmANRO8/fccw/279+PoaEhPPnkk/iLv/gLvPa1r8XmzZsB1BK7Nm3ahJ07d+LFF1/E9773Pdx111345V/+5Za7UO+44w784Ac/wNDQEJ599lncfPPNKJVKeOMb34jly5fjwgsvxB133IG9e/di//79+PSnP42tW7dGStXO36ZCtVrFo48+il/4hV+I3QGxEOfv5MmTePHFFyPja2hoCC+++CKOHj2KIAhwySWX4N5778WTTz6JAwcOYOfOnVi9ejW2b98OANi0aRPOOecc3HrrrXj++efxox/9CJ/97Gdx/vnnR2G5N77xjejs7MRf/uVf4qWXXsLjjz+Ohx56KBYOaNU3zs3N4VOf+hT279+P3/7t30a1Wo30srG57du3D3//93+PF198EUeOHMFjjz2Gz3/+8/j5n//5aLNr5TdS33fy5El84QtfwL59+zA0NIRnnnkGn/jEJ9Df34+zzz77/2/vjl3Mj+M4jr82E4Mrs0UpZbwRk9SZlCgTsuB/kNmibKJc3V9goqw6q+GbjQELBsUpMfym0+nc3XB+HZ+ej1Ep7975fl99v99eX0n3v8N3u91Og8Hg4lWaW97hT+eFax07w+GwFouFXl5eNJ/P1e129fr6qqenp1/PwFu6f6nT6ajdbmu9XsvtdiudTsvj8fz1z/rRVwVR+XxeoVBIq9VKtVpN0+lU+/1eDw8Penx8VCwWO7scvlwu1Wg0ZFmWbDabgsGgUqnUnxfUVatVjUYjbTYbORwOeb1eJZPJ07Mk7wVS/X5fx+PxYoHUrc720XA4PHUjfSzIusf9WZalcrn86fNgMKhCoXAq3+v1etrtdvJ6vcpms2dzb7dbNZvNs+K2TCbzZXGb3W5XJBI5Kz37n76bMR6Pq1gsXvxeqVSSz+fTeDxWs9nUfD7X4XCQy+VSIBBQNBo9C6J/NeN38+VyOVUqFU0mE729vcnpdMrv9yuRSJz97+55h4VCQZLU6/XUarVUr9c/3T685R3+dF6QrnfstCxLz8/Pms1mVy3fI9QAAAAjcPsJAAAYgVADAACMQKgBAABGINQAAAAjEGoAAIARCDUAAMAIhBoAAGAEQg0AADACoQYAABiBUAMAAIxAqAEAAEYg1AAAACP8A3KqbJp+M8BBAAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 200\n",
    "num_cycles = 6\n",
    "total_steps = len(data_loader)*epochs\n",
    "print(total_steps)\n",
    "print(len(data_loader))\n",
    "schedule_fn = make_cyclical_lr_fn(lr_0, epochs, num_cycles)\n",
    "lrs = []\n",
    "cycle_ends = []\n",
    "for i in range(total_steps):\n",
    "    curr_lr = schedule_fn(i)\n",
    "    lrs.append(curr_lr)\n",
    "    if curr_lr == 0.0:\n",
    "        cycle_ends.append(i)\n",
    "\n",
    "plt.plot(lrs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "outputs": [
    {
     "data": {
      "text/plain": "DeviceArray(0.01, dtype=float32, weak_type=True)"
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrs[33]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### ResNet Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(seed)\n",
    "epochs = 1000\n",
    "num_cycles = 7\n",
    "batch_size = 80\n",
    "beta = 0.7\n",
    "lr_0 = 1e-3\n",
    "num_blocks = 1\n",
    "hidden_dims = [1000, 500]\n",
    "temp, sigma = 1e-2, 1.0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "data_loader = NumpyLoader(NumpyData(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_data = NumpyData(X_val, y_val)\n",
    "\n",
    "bnn_model, state, val_losses = train_resnet_model(rng_key, data_loader, val_data, epochs, num_cycles, beta, lr_0, num_blocks,\n",
    "                                                  hidden_dims, dropout_rate=0.0, act_fn=jax.nn.swish, patience=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rmse_val, r2_val = eval_bnn_model(rng_key, bnn_model, X_val, y_val, state)\n",
    "rmse_test, r2_test = eval_bnn_model(rng_key, bnn_model, X_test, y_test, state)\n",
    "print(f\"Val RMSE: {rmse_val}, r2_score: {r2_val}\")\n",
    "print(f\"Test RMSE: {rmse_test}, r2_score: {r2_test}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_deep_ens_model(rng_key, train_loader, val_data, epochs, num_cycles, beta, lr_0,\n",
    "                         hidden_sizes, num_ensembles=4, dropout_rate=0.0, act_fn=jax.nn.relu, patience=20):\n",
    "\n",
    "    \n",
    "    subkeys = jax.random.split(rng_key, num_ensembles)\n",
    "    ens_models = []\n",
    "    ens_states = []\n",
    "\n",
    "    val_losses = []\n",
    "    for ens_key in subkeys:\n",
    "        model, state, val_loss = train_nn_model(ens_key, train_loader, val_data, epochs, num_cycles, beta, lr_0,\n",
    "                                            hidden_sizes, dropout_rate=dropout_rate, act_fn=act_fn, patience=patience)\n",
    "\n",
    "        ens_models.append(model)\n",
    "        ens_states.append(state)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "    return ens_models, ens_states, val_losses\n",
    "\n",
    "\n",
    "def train_resnet_deep_ens_model(rng_key, train_loader, val_data, epochs, num_cycles, beta, lr_0,\n",
    "                                num_blocks, hidden_dim, output_dim, num_ensembles=4, dropout_rate=0.3, act_fn=jax.nn.relu, patience=20):\n",
    "\n",
    "    \n",
    "    subkeys = jax.random.split(rng_key, num_ensembles)\n",
    "    ens_models = []\n",
    "    ens_states = []\n",
    "\n",
    "    val_losses = []\n",
    "    for ens_key in subkeys:\n",
    "        model, state, val_loss = train_resnet_model(rng_key, train_loader, val_data, epochs, num_cycles, beta, lr_0, num_blocks,\n",
    "                                                    hidden_dim, output_dim, dropout_rate=dropout_rate, act_fn=act_fn, patience=patience)\n",
    "\n",
    "        ens_models.append(model)\n",
    "        ens_states.append(state)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "    return ens_models, ens_states, val_losses\n",
    "\n",
    "def eval_deep_ens_model(rng_key, models, X, y, ens_states):\n",
    "    y_preds = np.zeros((len(models), len(y)))\n",
    "\n",
    "    for i, (model, state) in enumerate(zip(models, ens_states)):\n",
    "        preds, _ = model.apply(state.avg_params, state.net_state, X, rng_key, is_training=False)\n",
    "        preds_mean, preds_std = jnp.split(preds, [1], axis=-1)\n",
    "        preds_mean = preds_mean.squeeze()\n",
    "        y_preds[i] = preds_mean\n",
    "\n",
    "    y_preds = np.mean(y_preds, axis=0)\n",
    "\n",
    "    rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "    r2 = r2_score(y, y_preds)\n",
    "\n",
    "    return rmse, r2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(seed)\n",
    "epochs = 1000\n",
    "num_cycles = 3\n",
    "batch_size = 80\n",
    "beta = 0.80\n",
    "lr_0 = 1e-3\n",
    "hidden_sizes = [1000, 500, 300, 100]\n",
    "temp, sigma = 1e-2, 1.0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "data_loader = NumpyLoader(NumpyData(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_data = NumpyData(X_val, y_val)\n",
    "models, states, val_losses = train_deep_ens_model(rng_key, data_loader, val_data, epochs, num_cycles, beta, lr_0,\n",
    "                                        hidden_sizes, dropout_rate=0.0, act_fn=jax.nn.swish, num_ensembles=5, patience=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for loss in val_losses:\n",
    "    plt.plot(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rmse_val, r2_val = eval_deep_ens_model(rng_key, models, X_val, y_val, states)\n",
    "rmse_test, r2_test = eval_deep_ens_model(rng_key, models, X_test, y_test, states)\n",
    "print(f\"Val RMSE: {rmse_val}, r2_score: {r2_val}\")\n",
    "print(f\"Test RMSE: {rmse_test}, r2_score: {r2_test}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model, state in zip(models, states):\n",
    "    rmse_val, r2_val = eval_bnn_model(rng_key, model, X_val, y_val, state)\n",
    "    rmse_test, r2_test = eval_bnn_model(rng_key, model, X_test, y_test, state)\n",
    "    print(f\"Val RMSE: {rmse_val}, r2_score: {r2_val}\")\n",
    "    print(f\"Test RMSE: {rmse_test}, r2_score: {r2_test}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "J = np.load(f\"{data_dir}/cell_line/cancer_genes_net.npy\")\n",
    "np.count_nonzero(J)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def disc_sgd_gradient_update(step_size_fn,\n",
    "                         momentum_decay=0.,\n",
    "                         preconditioner=None):\n",
    "  \"\"\"Optax implementation of the SGD optimizer.\n",
    "  \"\"\"\n",
    "\n",
    "  if preconditioner is None:\n",
    "    preconditioner = get_identity_preconditioner()\n",
    "\n",
    "  def init_fn(gamma):\n",
    "    return OptaxSGLDState(\n",
    "        count=jnp.zeros([], jnp.int32),\n",
    "        momentum=jax.tree_map(jnp.zeros_like, gamma),\n",
    "        preconditioner_state=preconditioner.init(gamma))\n",
    "\n",
    "  def update_fn(key, gamma, gradient, state):\n",
    "    lr = step_size_fn(state.count)\n",
    "    lr_sqrt = jnp.sqrt(lr)\n",
    "\n",
    "    preconditioner_state = preconditioner.update_preconditioner(\n",
    "        gradient, state.preconditioner_state)\n",
    "\n",
    "    def update_momentum(m, g):\n",
    "      return momentum_decay * m + g * lr_sqrt\n",
    "      \n",
    "    def proposal(theta, g, step_size):\n",
    "        diff = (0.5*g*-(2*theta - 1)) - (1./(2*step_size))\n",
    "        prob = jax.nn.sigmoid(diff)\n",
    "        prob_inv = 1 - prob\n",
    "        prob = prob[...,None]\n",
    "        prob_inv = prob_inv[...,None]\n",
    "        delta = jnp.argmax(jnp.concatenate([prob, prob_inv], axis=1), axis=-1)  \n",
    "\n",
    "        theta_delta = (1 - theta)*delta + theta*(1 - delta)\n",
    "        return theta_delta*1.\n",
    "\n",
    "    momentum = jax.tree_map(update_momentum, state.momentum, gradient)\n",
    "    # updates = preconditioner.multiply_by_m_inv(momentum, preconditioner_state)\n",
    "    # updates = jax.tree_map(lambda m: m * lr_sqrt, updates)\n",
    "    gamma = proposal(gamma, gradient, lr)\n",
    "\n",
    "    return gamma, OptaxSGLDState(\n",
    "        count=state.count + 1,\n",
    "        momentum=momentum,\n",
    "        preconditioner_state=preconditioner_state)\n",
    "\n",
    "  return GradientTransformation(init_fn, update_fn)\n",
    "\n",
    "\n",
    "def disc_sgld_gradient_update(step_size_fn, \n",
    "                         momentum_decay=0.,\n",
    "                         preconditioner=None):\n",
    "  \"\"\"Optax implementation of the SGLD optimizer.\n",
    "\n",
    "  If momentum_decay is set to zero, we get the SGLD method [1]. Otherwise,\n",
    "  we get the underdamped SGLD (SGHMC) method [2].\n",
    "\n",
    "  Args:\n",
    "    step_size_fn: a function taking training step as input and prodng the\n",
    "      step size as output.\n",
    "    seed: int, random seed.\n",
    "    momentum_decay: float, momentum decay parameter (default: 0).\n",
    "    preconditioner: Preconditioner, an object representing the preconditioner\n",
    "      or None; if None, identity preconditioner is used (default: None).  [1]\n",
    "        \"Bayesian Learning via Stochastic Gradient Langevin Dynamics\" Max\n",
    "        Welling, Yee Whye Teh; ICML 2011  [2] \"Stochastic Gradient Hamiltonian\n",
    "        Monte Carlo\" Tianqi Chen, Emily B. Fox, Carlos Guestrin; ICML 2014\n",
    "  \"\"\"\n",
    "\n",
    "  if preconditioner is None:\n",
    "    preconditioner = get_identity_preconditioner()\n",
    "\n",
    "  def init_fn(gamma):\n",
    "    return OptaxSGLDState(\n",
    "        count=jnp.zeros([], jnp.int32),\n",
    "        momentum=jax.tree_map(jnp.zeros_like, gamma),\n",
    "        preconditioner_state=preconditioner.init(gamma))\n",
    "\n",
    "  def update_fn(key, gamma, gradient, state):\n",
    "    lr = step_size_fn(state.count)\n",
    "    lr_sqrt = jnp.sqrt(lr)\n",
    "\n",
    "    preconditioner_state = preconditioner.update_preconditioner(\n",
    "        gradient, state.preconditioner_state)\n",
    "\n",
    "    def update_momentum(m, g):\n",
    "      return momentum_decay * m + g * lr_sqrt\n",
    "      \n",
    "    def proposal(key, theta, g, step_size):\n",
    "        diff = (-0.5*g*(2*theta - 1)) - (1./(2*step_size))\n",
    "        delta = jax.random.bernoulli(key, jax.nn.sigmoid(diff))\n",
    "        theta_delta = (1 - theta)*delta + theta*(1 - delta)\n",
    "        return theta_delta*1.\n",
    "\n",
    "\n",
    "\n",
    "    momentum = jax.tree_map(update_momentum, state.momentum, gradient)\n",
    "    # updates = preconditioner.multiply_by_m_inv(momentum, preconditioner_state)\n",
    "    # updates = jax.tree_map(lambda m: m * lr_sqrt, updates)\n",
    "    gamma = proposal(key, gamma, gradient, lr)\n",
    "\n",
    "\n",
    "    return gamma, OptaxSGLDState(\n",
    "        count=state.count + 1,\n",
    "        momentum=momentum,\n",
    "        preconditioner_state=preconditioner_state)\n",
    "\n",
    "  return GradientTransformation(init_fn, update_fn)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### BNN with BG"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "Pytree = Any\n",
    "\n",
    "class BgTrainingState(NamedTuple):\n",
    "  params: hk.Params\n",
    "  avg_params: hk.Params\n",
    "  opt_state: optax.OptState\n",
    "  net_state: hk.State\n",
    "  disc_state: Pytree\n",
    "  avg_disc_state: Pytree\n",
    "  disc_opt_state: OptaxSGLDState\n",
    "\n",
    "class BgBayesNN():\n",
    "    def __init__(self, sgd_optim, sgld_optim, disc_sgd_optim, disc_sgld_optim, \n",
    "                        weight_prior, temp,\n",
    "                        hidden_sizes, J, eta, mu, act_fn=jax.nn.relu):\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.act_fn = act_fn\n",
    "        self.sgd_optim = sgd_optim\n",
    "        self.sgld_optim = sgld_optim\n",
    "        self.optimiser = sgd_optim\n",
    "\n",
    "        self.disc_optimiser = disc_sgd_optim\n",
    "        self.disc_sgd_optim = disc_sgd_optim\n",
    "        self.disc_sgld_optim = disc_sgld_optim\n",
    "\n",
    "        self._forward = hk.transform_with_state(self._forward_fn)\n",
    "        self.loss = jax.jit(self.loss)\n",
    "        self.update = jax.jit(self.update)\n",
    "\n",
    "        self.add_noise = False\n",
    "        self.J = J\n",
    "        self.eta = eta\n",
    "        self.mu = mu\n",
    "\n",
    "        self.temperature = temp\n",
    "        self.weight_prior = weight_prior\n",
    "        # weight_decay = self.sigma*self.temperature\n",
    "        # self.weight_prior = tfd.Normal(0, self.sigma)\n",
    "        # self.weight_prior = tfd.StudentT(df=2, loc=0, scale=self.sigma)\n",
    "        # self.weight_prior = tfd.Horseshoe(scale=self.sigma)\n",
    "        # self.weight_prior = tfd.Laplace(0, self.sigma)\n",
    "\n",
    "    def init(self, rng, x):\n",
    "        gamma = tfd.Bernoulli(0.5*jnp.ones(x.shape[-1])).sample(seed=rng)*1.\n",
    "        params, net_state = self._forward.init(rng, x, gamma, is_training=True)\n",
    "        opt_state = self.optimiser.init(params)\n",
    "        disc_opt_state = self.disc_optimiser.init(gamma)\n",
    "        return BgTrainingState(params, params, opt_state, net_state, gamma, gamma, disc_opt_state)\n",
    "\n",
    "    def apply(self, params, net_state, key, gamma, x, is_training):\n",
    "        return self._forward.apply(params, net_state, key, x, gamma, is_training)\n",
    "\n",
    "\n",
    "    def update(self, key, train_state, x, y):\n",
    "        if self.add_noise:\n",
    "            self.optimiser = self.sgld_optim\n",
    "            self.disc_optimiser = self.disc_sgld_optim\n",
    "        else:\n",
    "            self.optimiser = self.sgd_optim\n",
    "            self.disc_optimiser = self.disc_sgd_optim\n",
    "        \n",
    "        params, avg_params, opt_state, net_state, gamma, avg_gamma, disc_opt_state = train_state\n",
    "        grads, net_state = jax.grad(self.loss, has_aux=True)(params, net_state, key, gamma, x, y)\n",
    "        updates, opt_state = self.optimiser.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        avg_params = optax.incremental_update(params, avg_params, step_size=0.001)\n",
    "\n",
    "        disc_grads, net_state = jax.grad(self.disc_loss, has_aux=True)(gamma, params, net_state, key, x, y)\n",
    "        gamma, disc_opt_state = self.disc_optimiser.update(key, gamma, disc_grads, disc_opt_state)\n",
    "        avg_gamma = optax.incremental_update(gamma, avg_gamma, step_size=0.01)\n",
    "        return BgTrainingState(params, avg_params, opt_state, net_state, gamma, avg_gamma, disc_opt_state)\n",
    "\n",
    "    def _forward_fn(self, x, gamma, is_training=True):\n",
    "        x = x @ jnp.diag(gamma)\n",
    "        init_fn = hk.initializers.VarianceScaling()\n",
    "        for hd in self.hidden_sizes:\n",
    "            x = hk.Linear(hd, w_init = init_fn)(x)\n",
    "            \n",
    "            # x = hk.BatchNorm(False, False, 0.9)(x, is_training)\n",
    "            x = self.act_fn(x)\n",
    "\n",
    "        x = hk.Linear(2)(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, params, net_state, key, gamma, x, y, is_training=True):\n",
    "        preds, state = self.apply(params, net_state, key, gamma, x, is_training)\n",
    "        preds_mean, preds_std = jnp.split(preds, [1], axis=-1)\n",
    "        preds_std = jax.nn.softplus(preds_std.squeeze())\n",
    "        tempered_var = (preds_std * jnp.sqrt(self.temperature))**2\n",
    "        preds_mean = preds_mean.squeeze()\n",
    "        # se = (y - preds_mean)**2\n",
    "        # log_likelihood = (-0.5*se / (preds_std**2) - \n",
    "        #                  0.5*jnp.log((preds_std**2)*2*jnp.pi))\n",
    "        # log_likelihood = jnp.sum(log_likelihood)\n",
    "        # preds = preds.squeeze()\n",
    "        # l2_loss = jnp.mean(optax.l2_loss(y, preds))\n",
    "        # l2_reg =  l2_regulariser = 0.5 * sum(\n",
    "        #         jnp.sum(jnp.square(p)) for p in jax.tree_util.tree_leaves(params))\n",
    "        \n",
    "        log_likelihood = jnp.sum(tfd.Normal(preds_mean, tempered_var).log_prob(y))\n",
    "        # batch_size = x.shape[0]\n",
    "        # log_likelihood = (self.data_size / batch_size)*log_likelihood\n",
    "        log_prior = self.log_prior(params)\n",
    "\n",
    "        return log_likelihood + log_prior, state\n",
    "\n",
    "    def log_prior(self, params):\n",
    "        \"\"\"Computes the Gaussian prior log-density.\"\"\"\n",
    "        logprob_tree = jax.tree_util.tree_leaves(jax.tree_util.tree_map(lambda x: jnp.sum(self.weight_prior.log_prob(x.reshape(-1))/self.temperature), \n",
    "                                                                            params))\n",
    "        \n",
    "        return sum(logprob_tree)\n",
    "\n",
    "    def disc_loss(self, gamma, params, net_state, key, x, y):\n",
    "        log_ll, state = self.loss(params, net_state, key, gamma, x, y)\n",
    "        log_prior = self.ising_prior(gamma)\n",
    "\n",
    "        return (log_prior + log_ll) / self.temperature, state\n",
    "\n",
    "    def ising_prior(self, gamma):\n",
    "        \"\"\"Log probability of the Ising model - prior over the discrete variables\"\"\"\n",
    "        return (self.eta*(gamma.T @ self.J @ gamma) + mu*jnp.sum(gamma))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BGResNet:\n",
    "    def __init__(self, sgd_optim, sgld_optim, disc_sgd_optim, disc_sgld_optim,\n",
    "                 num_blocks, hidden_dim, skip_dim, J, eta, mu, act_fn=jax.nn.relu, dropout_rate=0.0):\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.act_fn = act_fn\n",
    "        self.sgd_optim = sgd_optim\n",
    "        self.sgld_optim = sgld_optim\n",
    "        self.optimiser = sgd_optim\n",
    "\n",
    "        self.num_blocks = num_blocks\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.skip_dim = skip_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.disc_optimiser = disc_sgd_optim\n",
    "        self.disc_sgd_optim = disc_sgd_optim\n",
    "        self.disc_sgld_optim = disc_sgld_optim\n",
    "\n",
    "        self._forward = hk.transform_with_state(self._forward_fn)\n",
    "        self.loss = jax.jit(self.loss)\n",
    "        self.update = jax.jit(self.update)\n",
    "\n",
    "        self.add_noise = False\n",
    "        self.J = J\n",
    "        self.eta = eta\n",
    "        self.mu = mu\n",
    "\n",
    "        # weight_decay = self.sigma*self.temperature\n",
    "        # self.weight_prior = tfd.Normal(0, self.sigma)\n",
    "        # self.weight_prior = tfd.StudentT(df=2, loc=0, scale=self.sigma)\n",
    "        # self.weight_prior = tfd.Horseshoe(scale=self.sigma)\n",
    "        # self.weight_prior = tfd.Laplace(0, self.sigma)\n",
    "\n",
    "    def init(self, rng, x):\n",
    "        gamma = tfd.Bernoulli(0.5*jnp.ones(x.shape[-1])).sample(seed=rng)*1.\n",
    "        params, net_state = self._forward.init(rng, x, gamma, is_training=True)\n",
    "        opt_state = self.optimiser.init(params)\n",
    "        disc_opt_state = self.disc_optimiser.init(gamma)\n",
    "        return BgTrainingState(params, params, opt_state, net_state, gamma, gamma, disc_opt_state)\n",
    "\n",
    "    def apply(self, params, net_state, key, gamma, x, is_training):\n",
    "        return self._forward.apply(params, net_state, key, x, gamma, is_training)\n",
    "\n",
    "\n",
    "    def update(self, key, train_state, x, y):\n",
    "        if self.add_noise:\n",
    "            self.optimiser = self.sgld_optim\n",
    "            self.disc_optimiser = self.disc_sgld_optim\n",
    "        else:\n",
    "            self.optimiser = self.sgd_optim\n",
    "            self.disc_optimiser = self.disc_sgd_optim\n",
    "\n",
    "        params, avg_params, opt_state, net_state, gamma, avg_gamma, disc_opt_state = train_state\n",
    "        grads, net_state = jax.grad(self.loss, has_aux=True)(params, net_state, key, gamma, x, y)\n",
    "        updates, opt_state = self.optimiser.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        avg_params = optax.incremental_update(params, avg_params, step_size=0.001)\n",
    "\n",
    "        disc_grads, net_state = jax.grad(self.disc_loss, has_aux=True)(gamma, params, net_state, key, x, y)\n",
    "        gamma, disc_opt_state = self.disc_optimiser.update(key, gamma, disc_grads, disc_opt_state)\n",
    "        avg_gamma = optax.incremental_update(gamma, avg_gamma, step_size=0.01)\n",
    "        return BgTrainingState(params, avg_params, opt_state, net_state, gamma, avg_gamma, disc_opt_state)\n",
    "\n",
    "    def _forward_fn(self, x, gamma, is_training=True):\n",
    "        x = x @ jnp.diag(gamma)\n",
    "        init_fn = hk.initializers.VarianceScaling()\n",
    "        x = hk.Linear(self.skip_dim, w_init=init_fn)(x)\n",
    "        for _ in range(self.num_blocks):\n",
    "            z = x\n",
    "            z = ResNetBlock(self.act_fn, self.hidden_dim, self.skip_dim, self.dropout_rate)(z, is_training)\n",
    "            x = x + z\n",
    "\n",
    "        x = hk.BatchNorm(False, False, 0.9)(x, is_training)\n",
    "        x = self.act_fn(x)\n",
    "        x = hk.Linear(2)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, params, net_state, key, gamma, x, y, is_training=True):\n",
    "        preds, state = self.apply(params, net_state, key, gamma, x, is_training)\n",
    "        preds_mean, preds_std = jnp.split(preds, [1], axis=-1)\n",
    "        preds_std = jax.nn.softplus(preds_std)\n",
    "        preds_mean = preds_mean.squeeze()\n",
    "        se = (y - preds_mean)**2\n",
    "        log_likelihood = (-0.5*se / (preds_std**2) -\n",
    "                          0.5*jnp.log((preds_std**2)*2*jnp.pi))\n",
    "        log_likelihood = jnp.sum(log_likelihood)\n",
    "        # preds = preds.squeeze()\n",
    "        # l2_loss = jnp.mean(optax.l2_loss(y, preds))\n",
    "        # l2_reg =  l2_regulariser = 0.5 * sum(\n",
    "        #         jnp.sum(jnp.square(p)) for p in jax.tree_util.tree_leaves(params))\n",
    "\n",
    "        return -log_likelihood, state\n",
    "\n",
    "    def log_prior(self, params):\n",
    "        \"\"\"Computes the Gaussian prior log-density.\"\"\"\n",
    "        logprob_tree = jax.tree_util.tree_leaves(jax.tree_util.tree_map(lambda x: jnp.sum(self.weight_prior.log_prob(x.reshape(-1))/self.temperature),\n",
    "                                                                        params))\n",
    "\n",
    "        return sum(logprob_tree)\n",
    "\n",
    "    def disc_loss(self, gamma, params, net_state, key, x, y):\n",
    "        log_ll, state = self.loss(params, net_state, key, gamma, x, y)\n",
    "        log_prior = self.ising_prior(gamma)\n",
    "\n",
    "        return log_prior - log_ll, state\n",
    "\n",
    "    def ising_prior(self, gamma):\n",
    "        \"\"\"Log probability of the Ising model - prior over the discrete variables\"\"\"\n",
    "        return (self.eta*(gamma.T @ self.J @ gamma) + mu*jnp.sum(gamma))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_bg_bnn_model(rng_key, train_loader, val_data, epochs, num_cycles, num_models, lr_0,\n",
    "                        disc_lr_0, weight_prior, temp, hidden_sizes, eta, mu, J, act_fn=jax.nn.relu, patience=20):\n",
    "\n",
    "    num_batches = len(train_loader)\n",
    "    total_steps = num_batches*epochs\n",
    "    cycle_len = epochs // num_cycles\n",
    "    print(cycle_len)\n",
    "    step_size_fn = make_cyclical_lr_fn(lr_0, total_steps, num_cycles)\n",
    "    disc_step_size_fn = make_cyclical_lr_fn(disc_lr_0, total_steps, num_cycles)\n",
    "\n",
    "    # step_size_fn = lambda c: lr_0\n",
    "    # disc_step_size_fn = lambda c: disc_lr_0\n",
    "\n",
    "    sgld_optim = optax.chain(optax.scale_by_schedule(step_size_fn),  optax.adam(lr_0), optax.scale(-1.0))\n",
    "    sgd_optim = optax.chain(optax.scale_by_schedule(step_size_fn),  optax.adam(lr_0), optax.scale(-1.0))\n",
    "\n",
    "    # sgd_optim = sgd_gradient_update(step_size_fn, momentum_decay=0, preconditioner=get_rmsprop_preconditioner())\n",
    "    # sgld_optim = sgld_gradient_update(step_size_fn, momentum_decay=0, preconditioner=get_rmsprop_preconditioner())\n",
    "    \n",
    "    disc_sgd_optim = disc_sgd_gradient_update(disc_step_size_fn, momentum_decay=0, preconditioner=get_identity_preconditioner())\n",
    "    disc_sgld_optim = disc_sgld_gradient_update(disc_step_size_fn, momentum_decay=0, preconditioner=get_identity_preconditioner())\n",
    "\n",
    "\n",
    "    model = BgBayesNN(sgd_optim, sgld_optim, disc_sgd_optim, disc_sgld_optim,\n",
    "                        weight_prior, temp,\n",
    "                        hidden_sizes, J, eta, mu, act_fn=act_fn)\n",
    "\n",
    "    train_state = model.init(rng_key, next(iter(train_loader))[0])\n",
    "\n",
    "    val_losses = []\n",
    "    lrs = []\n",
    "    key = rng_key\n",
    "    early_stopping = 0\n",
    "    step = 0\n",
    "    states = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            _, key = jax.random.split(key, 2)\n",
    "            train_state = model.update(key, train_state, batch_x, batch_y)\n",
    "            step += 1\n",
    "        val_loss, _  = eval_bg_bnn_model(key, model, val_data.data, val_data.target, train_state)\n",
    "        c_lr = step_size_fn(step)\n",
    "        lrs.append(c_lr)\n",
    "        if (epoch % cycle_len) + 1 > (cycle_len - num_models):\n",
    "            states.append(train_state)\n",
    "\n",
    "        if epoch != 0 and (val_loss > val_losses[-1]):\n",
    "            early_stopping += 1\n",
    "            if early_stopping > patience:\n",
    "                print(f\"Stopping at epoch: {epoch}, Total train steps: {step}\")\n",
    "                break\n",
    "        else:\n",
    "            early_stopping = 0 #reset\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Total number of steps trained: {step}\")\n",
    "\n",
    "    return model, states, val_losses, lrs\n",
    "\n",
    "def train_bg_bnn_model_v2(rng_key, train_loader, val_data, epochs, lr_0,\n",
    "                       disc_lr_0, weight_prior, temp, hidden_sizes, eta, mu, J, act_fn=jax.nn.relu, patience=20):\n",
    "\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    step_size_fn = lambda c: lr_0\n",
    "    disc_step_size_fn = lambda c: disc_lr_0\n",
    "\n",
    "    sgld_optim = optax.chain(optax.scale_by_schedule(step_size_fn),  optax.adam(lr_0), optax.scale(-1.0))\n",
    "    sgd_optim = optax.chain(optax.scale_by_schedule(step_size_fn),  optax.adam(lr_0), optax.scale(-1.0))\n",
    "\n",
    "\n",
    "\n",
    "    disc_sgd_optim = disc_sgd_gradient_update(disc_step_size_fn, momentum_decay=0, preconditioner=get_identity_preconditioner())\n",
    "    disc_sgld_optim = disc_sgld_gradient_update(disc_step_size_fn, momentum_decay=0, preconditioner=get_identity_preconditioner())\n",
    "\n",
    "\n",
    "    model = BgBayesNN(sgd_optim, sgld_optim, disc_sgd_optim, disc_sgld_optim,\n",
    "                      weight_prior, temp,\n",
    "                      hidden_sizes, J, eta, mu, act_fn=act_fn)\n",
    "\n",
    "    train_state = model.init(rng_key, next(iter(train_loader))[0])\n",
    "\n",
    "    val_losses = []\n",
    "    key = rng_key\n",
    "    early_stopping = 0\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            _, key = jax.random.split(key, 2)\n",
    "            train_state = model.update(key, train_state, batch_x, batch_y)\n",
    "            step += 1\n",
    "        val_loss, _  = eval_bg_bnn_model(key, model, val_data.data, val_data.target, train_state)\n",
    "        if epoch != 0 and (val_loss > val_losses[-1]):\n",
    "            early_stopping += 1\n",
    "            if early_stopping > patience:\n",
    "                # print(f\"Stopping at epoch: {epoch}, Total train steps: {step}\")\n",
    "                break\n",
    "        else:\n",
    "            early_stopping = 0 #reset\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    # print(f\"Total number of steps trained: {step}\")\n",
    "\n",
    "    return model, train_state, val_losses\n",
    "\n",
    "\n",
    "def eval_bg_bnn_model(rng_key, model, X, y, state):\n",
    "    # disc_state = jax.random.bernoulli(rng_key, state.avg_disc_state)\n",
    "    preds, _ = model.apply(state.params, state.net_state, rng_key, state.disc_state, X, is_training=False)\n",
    "    preds_mean, preds_std = jnp.split(preds, [1], axis=-1)\n",
    "    preds_mean = preds_mean.squeeze()\n",
    "\n",
    "    rmse = jnp.sqrt(jnp.mean((y - preds_mean)**2))\n",
    "    r2 = r2_score(y, preds_mean)\n",
    "\n",
    "    return rmse, r2\n",
    "\n",
    "def eval_per_model_score(rng_key, model, X, y, ens_states):\n",
    "    y_preds = np.zeros(len(y))\n",
    "\n",
    "    for i, state in enumerate(ens_states):\n",
    "        # disc_state = jax.random.bernoulli(rng_key, state.avg_disc_state)\n",
    "        preds, _ = model.apply(state.params, state.net_state, rng_key, state.disc_state, X, is_training=False)\n",
    "        preds_mean, preds_std = jnp.split(preds, [1], axis=-1)\n",
    "        preds_mean = preds_mean.squeeze()\n",
    "        y_preds += preds_mean\n",
    "\n",
    "    y_preds = y_preds / len(ens_states)\n",
    "\n",
    "    rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "    r2 = r2_score(y, y_preds)\n",
    "\n",
    "    return rmse, r2\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### BG + NN model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(seed)\n",
    "epochs = 1000\n",
    "num_cycles = 10\n",
    "batch_size = 80\n",
    "num_models = 79\n",
    "lr_0, disc_lr_0 = 1e-3, 0.8\n",
    "hidden_sizes = [1000, 500, 300, 100]\n",
    "eta, mu = 10.0, 1.0\n",
    "\n",
    "temp, sigma = 1.0, 1.0\n",
    "weight_prior = tfd.StudentT(df=2, loc=0, scale=sigma)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "data_loader = NumpyLoader(NumpyData(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_data = NumpyData(X_val, y_val)\n",
    "bg_bnn_model, states, val_losses, lrs = train_bg_bnn_model(rng_key, data_loader, val_data, epochs, num_cycles, num_models, lr_0, disc_lr_0,\n",
    "                                                        weight_prior, sigma,\n",
    "                                                        hidden_sizes, eta, mu, J, act_fn=jax.nn.swish)\n",
    "\n",
    "len(states)                                            "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rmse_val, r2_val = eval_per_model_score(rng_key, bg_bnn_model, X_val, y_val, states)\n",
    "rmse_test, r2_test = eval_per_model_score(rng_key, bg_bnn_model, X_test, y_test, states)\n",
    "print(f\"Val  RMSE: {rmse_val}, r2_score: {r2_val}\")\n",
    "print(f\"Test RMSE: {rmse_test}, r2_score: {r2_test}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for state in states:\n",
    "    rmse_val, r2_val = eval_bg_bnn_model(rng_key, bg_bnn_model, X_val, y_val, state)\n",
    "    rmse_test, r2_test = eval_bg_bnn_model(rng_key, bg_bnn_model, X_test, y_test, state)\n",
    "    print(f\"Val  RMSE: {rmse_val}, r2_score: {r2_val}\")\n",
    "    print(f\"Test RMSE: {rmse_test}, r2_score: {r2_test}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def train_bg_deep_ens_model(rng_key, train_loader, val_data, epochs, lr_0, disc_lr_0,\n",
    "                            weight_prior, temp, hidden_sizes, eta, mu, J, act_fn=jax.nn.relu, patience=20, num_ensembles=5):\n",
    "\n",
    "    \n",
    "    subkeys = jax.random.split(rng_key, num_ensembles)\n",
    "    ens_models = []\n",
    "    ens_states = []\n",
    "\n",
    "    val_losses = []\n",
    "\n",
    "    for ens_key in subkeys:\n",
    "        model, state, val_loss = train_bg_bnn_model_v2(ens_key, train_loader, val_data, epochs, lr_0, disc_lr_0,\n",
    "                                                    weight_prior, temp, hidden_sizes, eta, mu, J, act_fn=act_fn, patience=patience)\n",
    "\n",
    "        ens_models.append(model)\n",
    "        ens_states.append(state)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    return ens_models, ens_states, val_losses\n",
    "\n",
    "def eval_bg_deep_ens_model(rng_key, models, X, y, ens_states):\n",
    "    y_preds = np.zeros((len(models), len(y)))\n",
    "\n",
    "    for i, (model, state) in enumerate(zip(models, ens_states)):\n",
    "        # disc_state = jax.random.bernoulli(rng_key, state.avg_disc_state)\n",
    "        preds, _ = model.apply(state.params, state.net_state, rng_key, state.disc_state, X, is_training=False)\n",
    "        preds_mean, preds_std = jnp.split(preds, [1], axis=-1)\n",
    "        preds_mean = preds_mean.squeeze()\n",
    "        y_preds[i] = preds_mean\n",
    "\n",
    "    y_preds = np.mean(y_preds, axis=0)\n",
    "\n",
    "    rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "    r2 = r2_score(y, y_preds)\n",
    "\n",
    "    return rmse, r2\n",
    "\n",
    "\n",
    "def get_feats_dropout_loss(models, states, X, y):\n",
    "\n",
    "    def eval_fn(models, X, y, ens_states, p, on=True):\n",
    "        y_preds = np.zeros((len(models), len(y)))\n",
    "        for i, (models, state) in enumerate(zip(models, ens_states)):\n",
    "            disc_state = state.disc_state\n",
    "            if not on:\n",
    "                disc_state = disc_state.at[p].set(0)\n",
    "            # disc_state = jax.random.bernoulli(rng_key, state.avg_disc_state)\n",
    "            preds, _ = model.apply(state.params, state.net_state, rng_key,  disc_state, X, is_training=False)\n",
    "            preds_mean, preds_std = jnp.split(preds, [1], axis=-1)\n",
    "            preds_mean = preds_mean.squeeze()\n",
    "            y_preds[i] = preds_mean\n",
    "\n",
    "        y_preds = np.mean(y_preds, axis=0)\n",
    "\n",
    "        rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "        return rmse\n",
    "\n",
    "\n",
    "    var_loss_dict = {\"feats_idx\": [], \"num_models\": [] , \"loss_on\": [], \"loss_off\": [], \"loss_diff\": []}\n",
    "    disc_states  = np.array([state.disc_state for state in states])\n",
    "    # mean_gammas = np.mean(disc_states, axis=0)\n",
    "    # idxs = np.argwhere(mean_gammas == 1.0).ravel()\n",
    "    p = X.shape[-1]\n",
    "\n",
    "    for idx in range(p):\n",
    "        loss_diff = 0.0\n",
    "        idx_on = np.argwhere(disc_states[:,idx] == 1.).ravel()\n",
    "        loss_on, loss_off = 0., 0.\n",
    "        if idx_on.size == 0: ## irrelevant feature\n",
    "            loss_diff = 1e9\n",
    "        else:\n",
    "            models_on = itemgetter(*idx_on)(models)\n",
    "            states_on = itemgetter(*idx_on)(states)\n",
    "            if idx_on.size == 1:\n",
    "                models_on = [models_on]\n",
    "                states_on = [states_on]\n",
    "            loss_on += eval_fn(models_on, X, y, states_on, idx)\n",
    "            loss_off += eval_fn(models_on, X, y, states_on, idx, on=False)\n",
    "            loss_diff += (loss_on - loss_off)\n",
    "\n",
    "        var_loss_dict[\"feats_idx\"].append(idx)\n",
    "        var_loss_dict[\"num_models\"].append(idx_on.size)\n",
    "        var_loss_dict[\"loss_on\"].append(loss_on)\n",
    "        var_loss_dict[\"loss_off\"].append(loss_off)\n",
    "        var_loss_dict[\"loss_diff\"].append(loss_diff)\n",
    "\n",
    "\n",
    "    var_loss_df = pd.DataFrame(var_loss_dict).sort_values(by=\"loss_diff\")\n",
    "\n",
    "    return var_loss_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Deep Ensembles"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "rng_key = jax.random.PRNGKey(seed)\n",
    "epochs = 500\n",
    "num_cycles = 4\n",
    "batch_size = 80\n",
    "num_models = 5\n",
    "lr_0, disc_lr_0 = 1e-3, 0.8\n",
    "hidden_sizes = [1000, 500, 300, 100]\n",
    "eta, mu = 10.0, 1.0\n",
    "\n",
    "temp, sigma = 1.0, 1.0\n",
    "weight_prior = tfd.StudentT(df=2, loc=0, scale=sigma)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "data_loader = NumpyLoader(NumpyData(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_data = NumpyData(X_val, y_val)\n",
    "bg_bnn_models, bg_states, val_losses = train_bg_deep_ens_model(rng_key, data_loader, val_data, epochs, lr_0, disc_lr_0,\n",
    "                                                               weight_prior, temp, hidden_sizes, eta, mu, J, num_ensembles=20, act_fn=jax.nn.swish, patience=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, loss in enumerate(val_losses):\n",
    "    plt.plot(loss, label=f\"Ens - {i}\")\n",
    "\n",
    "# plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rmse_val, r2_val = eval_bg_deep_ens_model(rng_key, bg_bnn_models, X_val, y_val, bg_states)\n",
    "rmse_test, r2_test = eval_bg_deep_ens_model(rng_key, bg_bnn_models, X_test, y_test, bg_states)\n",
    "print(f\"Val  RMSE: {rmse_val}, r2_score: {r2_val}\")\n",
    "print(f\"Test RMSE: {rmse_test}, r2_score: {r2_test}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model, state in zip(bg_bnn_models, bg_states):\n",
    "    rmse_val, r2_val = eval_bg_bnn_model(rng_key, model, X_val, y_val, state)\n",
    "    rmse_test, r2_test = eval_bg_bnn_model(rng_key, model, X_test, y_test, state)\n",
    "    print(f\"Val  RMSE: {rmse_val}, r2_score: {r2_val}\")\n",
    "    print(f\"Test RMSE: {rmse_test}, r2_score: {r2_test}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def train_rf_model(seed, X, y):\n",
    "\n",
    "    cv = KFold(n_splits=5, random_state=seed, shuffle=True)\n",
    "\n",
    "    param_grid = {\n",
    "        'bootstrap': [True],\n",
    "        'max_depth': [80, 90, 100, 110],\n",
    "        'max_features': [2, 3],\n",
    "        'min_samples_leaf': [3, 4, 5],\n",
    "        'min_samples_split': [8, 10, 12],\n",
    "        'n_estimators': [100, 200, 300, 1000]\n",
    "    }\n",
    "\n",
    "    rf_reg = RandomForestRegressor(random_state=seed, max_samples=1.0)\n",
    "    grid_cv = GridSearchCV(estimator = rf_reg, param_grid = param_grid,\n",
    "                           cv = cv, n_jobs = -1, verbose = 0, scoring=\"r2\").fit(X, y)\n",
    "\n",
    "    rf_reg = RandomForestRegressor(random_state=seed, max_samples=1.0,**grid_cv.best_params_)\n",
    "    rf_reg.fit(X, y)\n",
    "\n",
    "    return rf_reg\n",
    "\n",
    "def eval_rf_model(model, X, y):\n",
    "    y_preds = model.predict(X)\n",
    "    rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "    r2 = r2_score(y, y_preds)\n",
    "    return rmse, r2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_model = train_rf_model(seed, X_train, y_train)\n",
    "\n",
    "rmse_val, r2_val = eval_rf_model(rf_model, X_val, y_val)\n",
    "rmse_test, r2_test = eval_rf_model(rf_model, X_test, y_test)\n",
    "\n",
    "print(f\"Val RMSE: {rmse_val}, r2_score:  {r2_val}\")\n",
    "print(f\"Test RMSE: {rmse_test}, test_score:  {r2_test}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.quantile(mean_gammas, q=[0., 0.25, 0.5, 0.95, 1.])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_feat_idx = np.argsort(mean_gammas)[::-1][:50]\n",
    "# bnn_feat_idx = dropout_loss_df[\"feats_idx\"][:10].to_list()\n",
    "# X_train_df.iloc[:, bnn_feat_idx].columns.to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_feat_idx = np.argsort(rf_model.feature_importances_)[::-1][:50]\n",
    "# X_train_df.iloc[:, rf_feat_idx].columns.to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_2, y_train_2 = jax.random.choice(rng_key, X_train, shape=(X_val.shape[0],), replace=False), jax.random.choice(rng_key, y_train, shape=(X_val.shape[0],), replace=False)\n",
    "X_gp_train_rf, X_gp_train_2_rf, X_gp_val_rf, X_gp_test_rf = X_val[:,rf_feat_idx], X_train_2[:,rf_feat_idx], X_train[:,rf_feat_idx], X_test[:,rf_feat_idx]\n",
    "rf_val_rmse_score, rf_val_r2_score, rf_val_pcc, rf_val_pval, \\\n",
    "    rf_test_rmse_score, rf_test_r2_score, rf_test_pcc, rf_test_pval = train_linear_model(seed, X_gp_train_2_rf, X_gp_train_rf, X_gp_test_rf, y_train_2, y_val, y_test)\n",
    "\n",
    "print(f\"val_r2_score: {rf_val_r2_score}, test_r2_score: {rf_test_r2_score}\")\n",
    "print(f\"val_pcc: {rf_val_pcc}, pval: {rf_val_pval}, test_pcc: {rf_test_pcc}, pval: {rf_test_pval}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_gp_train_bnn, X_gp_train_2_bnn, X_gp_val_bnn, X_gp_test_bnn = X_val[:,bnn_feat_idx], X_train_2[:,bnn_feat_idx], X_train[:,bnn_feat_idx], X_test[:,bnn_feat_idx]\n",
    "bnn_val_rmse_score, bnn_val_r2_score, bnn_val_pcc, bnn_val_pval, \\\n",
    "         bnn_test_rmse_score, bnn_test_r2_score, bnn_test_pcc, bnn_test_pval = train_linear_model(seed, X_gp_train_2_bnn, X_gp_train_bnn, X_gp_test_bnn, y_train_2, y_val, y_test)\n",
    "\n",
    "print(f\"val_r2_score: {bnn_val_r2_score}, test_r2_score: {bnn_test_r2_score}\")\n",
    "print(f\"val_pcc: {bnn_val_pcc}, pval: {bnn_val_pval}, test_pcc: {bnn_test_pcc}, pval: {bnn_test_pval}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_gp_val_rmse_score, rf_gp_val_r2_score, rf_gp_val_pcc, rf_gp_val_pval, \\\n",
    "         rf_gp_test_rmse_score, rf_gp_test_r2_score, rf_gp_test_pcc, rf_gp_test_pval, gp_df = train_gp(seed, X_gp_train_rf, X_gp_train_2_rf, X_gp_val_rf, X_gp_test_rf, \n",
    "                                                                                                      y_val, y_train_2, y_train, y_test, num_models=5, verbose=1, \n",
    "                                                                                                      p_cxvr=0.8, p_subt_mut=0.1, p_hmut=0.05, p_pmut=0.05, complexity_coef=0.01)\n",
    "\n",
    "print(f\"val_r2_score: {rf_gp_val_r2_score}, test_r2_score: {rf_gp_test_r2_score}\")\n",
    "print(f\"val_pcc: {rf_gp_val_pcc}, test_pcc: {rf_gp_test_pcc}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_gp_val_rmse_score, bnn_gp_val_r2_score, bnn_gp_val_pcc, bnn_gp_val_pval, \\\n",
    "         bnn_gp_test_rmse_score, bnn_gp_test_r2_score, bnn_gp_test_pcc, bnn_gp_test_pval, gp_df = train_gp(seed, X_gp_train_bnn, X_gp_train_2_bnn, X_gp_val_bnn, X_gp_test_bnn, \n",
    "                                                                                                      y_val, y_train_2, y_train, y_test, num_models=5, verbose=1, p_cxvr=0.8, p_subt_mut=0.1, \n",
    "                                                                                                      p_hmut=0.05, p_pmut=0.05, complexity_coef=0.01)\n",
    "\n",
    "print(f\"val_r2_score: {bnn_gp_val_r2_score}, test_r2_score: {bnn_gp_test_r2_score}\")\n",
    "print(f\"val_pcc: {bnn_gp_val_pcc}, test_pcc: {bnn_gp_test_pcc}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"val_r2_score: {bnn_gp_val_r2_score}, test_r2_score: {bnn_gp_test_r2_score}\")\n",
    "print(f\"val_pcc: {bnn_gp_val_pcc}, test_pcc: {bnn_gp_test_pcc}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gp_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_df.iloc[:,np.array(bnn_feat_idx)[[53, 5, 44]]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_feat_idx[5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def evaluate_bnn_bg_models(model, X, y, params, gammas):\n",
    "    eval_fn = lambda p, g: model.apply(p, g, X).ravel()\n",
    "    preds = jax.vmap(eval_fn)(params, gammas)\n",
    "    preds = preds.reshape(-1, preds.shape[-1])\n",
    "    losses = jax.vmap(optax.l2_loss, in_axes=(0, None))(preds, y)\n",
    "    mean_loss = jnp.sqrt(jnp.mean(losses, axis=-1))\n",
    "    return jnp.sum(mean_loss)\n",
    "\n",
    "def get_feats_dropout_loss(models, states, X, y):\n",
    "\n",
    "    def eval_fn(model, X, y, ens_states, p, on=True):\n",
    "        y_preds = np.zeros(len(y))\n",
    "\n",
    "        for i, state in enumerate(ens_states):\n",
    "            # disc_state = jax.random.bernoulli(rng_key, state.avg_disc_state)\n",
    "            disc_state = state.disc_state\n",
    "            if not on:\n",
    "                disc_state = disc_state.at[:,p].set(0)\n",
    "            preds, _ = model.apply(state.params, state.net_state, rng_key, disc_state, X, is_training=False)\n",
    "            preds_mean, preds_std = jnp.split(preds, [1], axis=-1)\n",
    "            preds_mean = preds_mean.squeeze()\n",
    "            y_preds += preds_mean\n",
    "\n",
    "        y_preds = y_preds / len(ens_states)\n",
    "\n",
    "        rmse = jnp.sqrt(jnp.mean((y - y_preds)**2))\n",
    "        return rmse\n",
    "\n",
    "\n",
    "    var_loss_dict = {\"feats_idx\": [], \"num_models\": [] , \"loss_on\": [], \"loss_off\": [], \"loss_diff\": []}\n",
    "    disc_states  = np.array([state.disc_state for state in states])\n",
    "    p = X.shape[1]\n",
    "\n",
    "    for idx in range(p):\n",
    "        loss_diff = 0.0\n",
    "        idx_on = np.argwhere(disc_states[:,idx] == 1.).ravel()\n",
    "        loss_on, loss_off = 0., 0.\n",
    "        if idx_on.size == 0: ## irrelevant feature\n",
    "            loss_diff = 1e9\n",
    "        else:\n",
    "            models_on = itemgetter(*idx_on)(models)\n",
    "            states_on = itemgetter(*idx_on)(states)\n",
    "            for (model, state) in zip(models_on, states_on):\n",
    "                loss_on += eval_fn(model, X, y, state, idx)\n",
    "                loss_off += eval_fn(model, X, y, state, idx, on=False)\n",
    "                loss_diff += (loss_on - loss_off)\n",
    "\n",
    "\n",
    "        var_loss_dict[\"feats_idx\"].append(idx)\n",
    "        var_loss_dict[\"num_models\"].append(idx_on.size)\n",
    "        var_loss_dict[\"loss_on\"].append(loss_on)\n",
    "        var_loss_dict[\"loss_off\"].append(loss_off)\n",
    "        var_loss_dict[\"loss_diff\"].append(loss_diff)\n",
    "\n",
    "\n",
    "    var_loss_df = pd.DataFrame(var_loss_dict).sort_values(by=\"loss_diff\")\n",
    "\n",
    "    return var_loss_df\n",
    "\n",
    "def get_gene_names(gene_cols):\n",
    "    return [gene.split(\"(\")[0].strip() for gene in gene_cols]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from smac.facade.smac_hpo_facade import SMAC4HPO\n",
    "from smac.scenario.scenario import Scenario\n",
    "from smac.configspace import ConfigurationSpace\n",
    "from ConfigSpace.hyperparameters import (\n",
    "    CategoricalHyperparameter,\n",
    "    UniformFloatHyperparameter,\n",
    "    UniformIntegerHyperparameter,\n",
    ")\n",
    "\n",
    "from ConfigSpace import InCondition, Configuration\n",
    "\n",
    "import math\n",
    "\n",
    "def get_configspace_bnn_bg()-> ConfigurationSpace:\n",
    "    \n",
    "    # Build Configuration Space which defines all parameters and their ranges.\n",
    "    cs  = ConfigurationSpace()    \n",
    "    mu = CategoricalHyperparameter(\"mu\", [1., 5., 10.], default_value=1.)\n",
    "    temp = CategoricalHyperparameter(\"temp\", [1e-4, 1e-3, 1e-2, 1e-1, 1.0], default_value=1e-3)\n",
    "    cs.add_hyperparameters([temp, mu])\n",
    "    return cs\n",
    "\n",
    "def get_configspace_bnn()-> ConfigurationSpace:\n",
    "    \n",
    "    # Build Configuration Space which defines all parameters and their ranges.\n",
    "    cs  = ConfigurationSpace()    \n",
    "    temp = CategoricalHyperparameter(\"temp\", [1e-4, 1e-3, 1e-2, 1e-1, 1.], default_value=1e-3)\n",
    "    cs.add_hyperparameters([temp])\n",
    "    return cs\n",
    "\n",
    "def generate_train_bnn_bg_cs(seed, X, y, J):\n",
    "    cv = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "    def train_cs(config: Configuration)-> float:\n",
    "        epochs = 2000\n",
    "        num_cycles = 10\n",
    "        bathc_size = 80\n",
    "        beta = 0.99\n",
    "        lr_0, disc_lr_0 = 1e-3, 0.8\n",
    "        hidden_sizes = [1000, 500, 300, 100]\n",
    "        sigma = 1.0\n",
    "        eta = 1.0\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        for train_idx, test_idx in cv.split(X, y):\n",
    "            x_train_cv, y_train_cv = X[train_idx], y[train_idx]\n",
    "            x_test_cv, y_test_cv = X[test_idx], y[test_idx]\n",
    "            bg_bnn_model, states, disc_states = train_bg_bnn_model(seed, x_train_cv, y_train_cv, epochs,\n",
    "                                                                     num_cycles, beta, lr_0, disc_lr_0,\n",
    "                                                                     batch_size, hidden_sizes, config[\"temp\"], sigma, eta, config[\"mu\"], J)\n",
    "            \n",
    "            _, r2 = eval_bg_bnn_model(bg_bnn_model, x_test_cv, y_test_cv, states, disc_states)\n",
    "            scores.append(r2)\n",
    "\n",
    "        mean_score = np.mean(np.array(scores))\n",
    "        if math.isnan(mean_score) or mean_score < 0:\n",
    "            return 1e9\n",
    "        \n",
    "        return 1 - mean_score\n",
    "\n",
    "\n",
    "    return train_cs\n",
    "\n",
    "def generate_train_bnn_cs(seed, X, y):\n",
    "    cv = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "    def train_cs(config: Configuration)-> float:\n",
    "        epochs = 2000\n",
    "        num_cycles = 10\n",
    "        bathc_size = 80\n",
    "        beta = 0.99\n",
    "        lr_0 = 1e-3\n",
    "        hidden_sizes = [1000, 500, 300, 100]\n",
    "        sigma = 1.0\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        for train_idx, test_idx in cv.split(X, y):\n",
    "            x_train_cv, y_train_cv = X[train_idx], y[train_idx]\n",
    "            x_test_cv, y_test_cv = X[test_idx], y[test_idx]\n",
    "            bnn_model, states = train_bnn_model(seed, x_train_cv, y_train_cv, epochs, num_cycles, beta, lr_0,\n",
    "                                                    batch_size, hidden_sizes, config[\"temp\"], sigma)\n",
    "            \n",
    "            _, r2 = eval_bnn_model(bnn_model, x_test_cv, y_test_cv, states)\n",
    "            scores.append(r2)\n",
    "\n",
    "        mean_score = np.mean(np.array(scores))\n",
    "        if math.isnan(mean_score) or mean_score < 0:\n",
    "            return 1e9\n",
    "        \n",
    "        return 1 - mean_score\n",
    "\n",
    "\n",
    "    return train_cs\n",
    "\n",
    "\n",
    "def optimize_hyper_parameters(seed, X, y, J, total_time=60, bg=True):\n",
    "    if bg:\n",
    "        cs = get_configspace_bnn_bg()\n",
    "        train_cs = generate_train_bnn_bg_cs(seed, X, y, J)\n",
    "    else:\n",
    "        cs = get_configspace_bnn()\n",
    "        train_cs = generate_train_bnn_cs(seed, X, y)\n",
    "\n",
    "    scenario = Scenario({\n",
    "        \"run_obj\": \"quality\",\n",
    "        \"wallclock-limit\": total_time,\n",
    "        \"cs\": cs,\n",
    "        \"deterministic\": True,\n",
    "        \"cutoff\": 10,  # runtime limit for the target algorithm\n",
    "        \"seed\": seed\n",
    "    })\n",
    "\n",
    "    smac = SMAC4HPO(scenario=scenario, rng=np.random.RandomState(seed), tae_runner=train_cs)\n",
    "\n",
    "    tae = smac.get_tae_runner()\n",
    "\n",
    "    try:\n",
    "        incumbent = smac.optimize()\n",
    "\n",
    "    finally:\n",
    "        incumbent = smac.solver.incumbent\n",
    "\n",
    "\n",
    "    inc_val = tae.run(config=incumbent, seed=seed)\n",
    "\n",
    "    return incumbent, 1 - inc_val[1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# config, score = optimize_hyper_parameters(seed, X_train, y_train, J, total_time=180)\n",
    "# score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "# epochs = 2000\n",
    "# num_cycles = 10\n",
    "# batch_size = 80\n",
    "# beta = 0.99\n",
    "# lr_0, disc_lr_0 = 1e-3, 0.5\n",
    "# hidden_sizes = [1000, 500, 300, 10]\n",
    "# sigma = 1.0\n",
    "# eta, mu = 1.0, 1.0\n",
    "# temp = 1e-3\n",
    "epochs = 1000\n",
    "num_cycles = 10\n",
    "batch_size = 80\n",
    "beta = 0.80\n",
    "lr_0, disc_lr_0 = 1e-3, 0.5\n",
    "hidden_sizes = [1000, 500, 300, 100]\n",
    "temp, sigma = 1e-2, 1.0\n",
    "eta, mu = 1.0, 1.0\n",
    "\n",
    "\n",
    "def cross_val_run(seeds, X, y):\n",
    "\n",
    "    bnn_rf_bg_dict = {\"seed\":[], \"model\": [], \"val_rmse\": [], \"test_rmse\": [], \"val_r2_score\": [], \"test_r2_score\": [], \"temp\": [], \"mu\": []}\n",
    "\n",
    "    for seed in tqdm(seeds):\n",
    "        X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X, y, random_state=seed, shuffle=True, test_size=0.2)\n",
    "        X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X_train_df, y_train_df, shuffle=True, \n",
    "                                                    random_state=seed, test_size=0.2)\n",
    "        # train_transformer = PowerTransformer().fit(X_train_df)\n",
    "        train_transformer = QuantileTransformer(random_state=seed, output_distribution=\"normal\").fit(X_train_df)\n",
    "        train_transformed = train_transformer.transform(X_train_df)\n",
    "        val_transformed = train_transformer.transform(X_val_df)\n",
    "        test_transformed = train_transformer.transform(X_test_df)\n",
    "\n",
    "        X_train_df = pd.DataFrame(train_transformed, columns=X_train_df.columns)\n",
    "        X_val_df = pd.DataFrame(val_transformed, columns=X_val_df.columns)\n",
    "        X_test_df = pd.DataFrame(test_transformed, columns=X_test_df.columns)\n",
    "        X_train, y_train = X_train_df.values, y_train_df.values\n",
    "        X_val, y_val = X_val_df.values, y_val_df.values\n",
    "        X_test, y_test = X_test_df.values, y_test_df.values\n",
    "\n",
    "\n",
    "        # config_bnn_bg, score = optimize_hyper_parameters(seed, X_train, y_train, J, total_time=180, bg=True)\n",
    "        bg_bnn_model, states, disc_states = train_bg_bnn_model(seed, X_train, y_train, epochs, num_cycles, beta, lr_0, disc_lr_0,\n",
    "                                                                        batch_size, hidden_sizes, temp, sigma, eta, mu, J)\n",
    "\n",
    "        rmse_val_bnn_bg, r2_val_bnn_bg = eval_bg_bnn_model(bg_bnn_model, X_val, y_val, states, disc_states)\n",
    "        rmse_test_bnn_bg, r2_test_bnn_bg = eval_bg_bnn_model(bg_bnn_model, X_test, y_test, states, disc_states)\n",
    "\n",
    "        # config_bnn, score = optimize_hyper_parameters(seed, X_train, y_train, J, total_time=180, bg=False)\n",
    "        bnn_model, states = train_bnn_model(seed, X_train, y_train, epochs, num_cycles, beta, lr_0,\n",
    "                                                batch_size, hidden_sizes, temp, sigma)\n",
    "\n",
    "        rmse_val_bnn, r2_val_bnn = eval_bnn_model(bnn_model, X_val, y_val, states)\n",
    "        rmse_test_bnn, r2_test_bnn = eval_bnn_model(bnn_model, X_test, y_test, states)\n",
    "\n",
    "        rf_model = train_rf_model(seed, X_train, y_train)\n",
    "        rmse_val_rf, r2_val_rf = eval_rf_model(rf_model, X_val, y_val)\n",
    "        rmse_test_rf, r2_test_rf = eval_rf_model(rf_model, X_test, y_test)\n",
    "\n",
    "        bnn_rf_bg_dict[\"seed\"].append(seed)\n",
    "        bnn_rf_bg_dict[\"model\"].append(\"RF\")\n",
    "        bnn_rf_bg_dict[\"val_rmse\"].append(rmse_val_rf)\n",
    "        bnn_rf_bg_dict[\"test_rmse\"].append(rmse_test_rf)\n",
    "        bnn_rf_bg_dict[\"val_r2_score\"].append(r2_val_rf)\n",
    "        bnn_rf_bg_dict[\"test_r2_score\"].append(r2_test_rf)\n",
    "        bnn_rf_bg_dict[\"temp\"].append(0.0)\n",
    "        bnn_rf_bg_dict[\"mu\"].append(0.0)\n",
    "\n",
    "        bnn_rf_bg_dict[\"seed\"].append(seed)\n",
    "        bnn_rf_bg_dict[\"model\"].append(\"BNN\")\n",
    "        bnn_rf_bg_dict[\"val_rmse\"].append(rmse_val_bnn)\n",
    "        bnn_rf_bg_dict[\"test_rmse\"].append(rmse_test_bnn)\n",
    "        bnn_rf_bg_dict[\"val_r2_score\"].append(r2_val_bnn)\n",
    "        bnn_rf_bg_dict[\"test_r2_score\"].append(r2_test_bnn)\n",
    "        bnn_rf_bg_dict[\"temp\"].append(temp)\n",
    "        bnn_rf_bg_dict[\"mu\"].append(0.0)\n",
    "\n",
    "        bnn_rf_bg_dict[\"seed\"].append(seed)\n",
    "        bnn_rf_bg_dict[\"model\"].append(\"BNN + BG\")\n",
    "        bnn_rf_bg_dict[\"val_rmse\"].append(rmse_val_bnn_bg)\n",
    "        bnn_rf_bg_dict[\"test_rmse\"].append(rmse_test_bnn_bg)\n",
    "        bnn_rf_bg_dict[\"val_r2_score\"].append(r2_val_bnn_bg)\n",
    "        bnn_rf_bg_dict[\"test_r2_score\"].append(r2_test_bnn_bg)\n",
    "        bnn_rf_bg_dict[\"temp\"].append(temp)\n",
    "        bnn_rf_bg_dict[\"mu\"].append(mu)\n",
    "\n",
    "        print(f\"RF scores - Val: {r2_val_rf}, Test: {r2_test_rf}\")\n",
    "        print(f\"BNN scores - Val: {r2_val_bnn}, Test: {r2_test_bnn}\")\n",
    "        print(f\"BNN + BG scores - Val: {r2_val_bnn_bg}, Test: {r2_test_bnn_bg}\")\n",
    "\n",
    "        \n",
    "    return pd.DataFrame(bnn_rf_bg_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# seeds = [422,261,968,282,739,573,220,413,745,775,482,442,210,423,760,57,769,920,226,196]\n",
    "# bnn_rf_bg_df = cross_val_run(seeds, X_selected, target)\n",
    "save_dir = f\"{data_dir}/exp_data_5/cancer/gdsc\"\n",
    "# bnn_rf_bg_df.to_csv(f\"{save_dir}/bnn_rf_bg_df_v5.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_rf_bg_df = pd.read_csv(f\"{save_dir}/bnn_rf_bg_df_v5.csv\")\n",
    "bnn_rf_bg_df.groupby(\"model\").mean().iloc[:,1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_rf_bg_df.groupby([\"model\"])[\"val_r2_score\" ,\"test_r2_score\"].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = [\"RF\", \"BNN\", \"BNN + BG\"]\n",
    "\n",
    "rf_val_r2_scores, rf_test_r2_scores = bnn_rf_bg_df[bnn_rf_bg_df[\"model\"] == \"RF\"][\"val_r2_score\"], \\\n",
    "                                               bnn_rf_bg_df[bnn_rf_bg_df[\"model\"] == \"RF\"][\"test_r2_score\"]\n",
    "\n",
    "bnn_val_r2_scores, bnn_test_r2_scores = bnn_rf_bg_df[bnn_rf_bg_df[\"model\"] == \"BNN\"][\"val_r2_score\"], \\\n",
    "                                                  bnn_rf_bg_df[bnn_rf_bg_df[\"model\"] == \"BNN\"][\"test_r2_score\"]\n",
    "\n",
    "bnn_bg_val_r2_scores, bnn_bg_test_r2_scores = bnn_rf_bg_df[bnn_rf_bg_df[\"model\"] == \"BNN + BG\"][\"val_r2_score\"], \\\n",
    "                                                  bnn_rf_bg_df[bnn_rf_bg_df[\"model\"] == \"BNN + BG\"][\"test_r2_score\"]                                              \n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1.grid(color='grey', axis='y', linestyle='-', linewidth=0.25, alpha=0.5)\n",
    "ax2.grid(color='grey', axis='y', linestyle='-', linewidth=0.25, alpha=0.5)\n",
    "\n",
    "bplot1 = ax1.boxplot([rf_val_r2_scores, bnn_val_r2_scores, bnn_bg_val_r2_scores], showmeans=True, patch_artist=True, labels=labels)\n",
    "bplot2 = ax2.boxplot([rf_test_r2_scores, bnn_test_r2_scores, bnn_bg_test_r2_scores], showmeans=True, patch_artist=True, labels=labels)\n",
    "\n",
    "# fill with colors\n",
    "colors = ['lightyellow' ,'lightblue', 'lightgreen']\n",
    "for bplot in (bplot1, bplot2):\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "fig.suptitle(\"Model Comparison\")\n",
    "plt.legend([bplot1['medians'][0], bplot1['means'][0]], ['median', 'mean'])\n",
    "\n",
    "ax1.set_ylabel(\"$R^{2}$\")\n",
    "ax1.set_title(\"Validation Scores\")\n",
    "\n",
    "ax2.set_ylabel(\"$R^{2}$\")\n",
    "ax2.set_title(\"Test Scores\")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "stats.ttest_rel(bnn_bg_test_r2_scores, rf_test_r2_scores, alternative=\"greater\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### GP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gplearn.genetic import SymbolicTransformer, SymbolicClassifier, SymbolicRegressor\n",
    "from gplearn.functions import make_function\n",
    "import operator\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_best_programs(gp, num_models, classifier=True, ascending=True, sort_fit=\"OOB_fitness\"):\n",
    "    gp_dict = {'Gen': [], \"Ind\": [], \"Fitness\": [], 'OOB_fitness': [], \"Equation\": []}\n",
    "\n",
    "    if classifier:\n",
    "        for idGen in range(len(gp._programs)):\n",
    "            for idPopulation in range(gp.population_size):\n",
    "                gp_dict[\"Gen\"].append(idGen)\n",
    "                gp_dict[\"Ind\"].append(idPopulation)\n",
    "                gp_dict[\"Fitness\"].append(gp._programs[idGen][idPopulation].fitness_)\n",
    "                gp_dict[\"OOB_fitness\"].append(gp._programs[idGen][idPopulation].oob_fitness_)\n",
    "                gp_dict[\"Equation\"].append(str(gp._programs[idGen][idPopulation]))\n",
    "    else:\n",
    "        for idx, prog in enumerate(gp._programs[-1]):\n",
    "                gp_dict[\"Gen\"].append(-1)\n",
    "                gp_dict[\"Ind\"].append(idx)\n",
    "                gp_dict[\"Fitness\"].append(prog.fitness_)\n",
    "                gp_dict[\"OOB_fitness\"].append(prog.oob_fitness_)\n",
    "                gp_dict[\"Equation\"].append(str(prog))\n",
    "\n",
    "    gp_df = pd.DataFrame(gp_dict).sort_values(sort_fit, ascending=ascending)[:num_models]\n",
    "    programs = []\n",
    "    for i in range(num_models):\n",
    "        gen, ind = int(gp_df.iloc[i][\"Gen\"]), int(gp_df.iloc[i][\"Ind\"])\n",
    "        programs.append(gp._programs[gen][ind])\n",
    "\n",
    "    return programs, gp_df\n",
    "\n",
    "\n",
    "def gp_transform(est, X, classifier=False, num_models=100, sort_fit=\"Fitness\"):\n",
    "    if classifier or (sort_fit == \"OOB_fitness\"):\n",
    "        programs, gp_df = get_best_programs(est, num_models, classifier, sort_fit=sort_fit, ascending=classifier)\n",
    "        out = np.zeros((X.shape[0], len(programs)))\n",
    "        for i, prog in enumerate(programs):\n",
    "            out[:, i] = prog.execute(X)\n",
    "\n",
    "        return out, gp_df\n",
    "    else:\n",
    "        return est.transform(X), None\n",
    "\n",
    "function_set = ['add', 'sub', 'mul', 'div', 'sqrt', 'log',\n",
    "                'abs', 'neg', 'inv', 'max', 'min']\n",
    "\n",
    "def train_linear_model(seed, X_train, X_test, y_train, y_test):\n",
    "    cv = KFold(n_splits=3, random_state=seed, shuffle=True)\n",
    "    param_grid = {\"alpha\": np.logspace(-2, 2, 20)}\n",
    "    grid_cv = GridSearchCV(estimator=Ridge(max_iter=10000), param_grid=param_grid, \n",
    "                                            verbose=0, scoring=\"r2\", cv=cv).fit(X_train, y_train)\n",
    "    lin_model = Ridge(max_iter=10000, **grid_cv.best_params_)\n",
    "    lin_model.fit(X_train, y_train)\n",
    "    y_test_pred = lin_model.predict(X_test)\n",
    "\n",
    "    test_rmse_score = np.sqrt(np.mean((y_test - y_test_pred)**2))\n",
    "    test_r2_score = r2_score(y_test, y_test_pred)\n",
    "    test_pearson, test_pval = stats.pearsonr(y_test, y_test_pred)\n",
    "\n",
    "    return test_rmse_score, test_r2_score, test_pearson, test_pval\n",
    "\n",
    "def train_gp(seed, X_train, X_train_2, X_test, y_train, y_train_2, y_test, num_models=5, sort_fit=\"OOB_fitness\", verbose=0, num_gen=100, \n",
    "                        p_cxvr=0.8, p_subt_mut=0.1, p_hmut=0.05, p_pmut=0.1, subsample=0.8, complexity_coef=0.05):\n",
    "    gp_est = SymbolicTransformer(population_size=1000, hall_of_fame=200, n_components=50, generations=num_gen,\n",
    "                           p_crossover=p_cxvr, p_subtree_mutation=p_subt_mut,\n",
    "                           p_hoist_mutation=p_hmut, p_point_mutation=p_pmut,\n",
    "                           max_samples=subsample, verbose=verbose,\n",
    "                           parsimony_coefficient=complexity_coef, random_state=seed)\n",
    "\n",
    "    gp_est.fit(X_train, y_train)\n",
    "\n",
    "    gp_features_train, gp_train_df = gp_transform(gp_est, X_train_2, classifier=False, sort_fit=sort_fit, num_models=num_models)\n",
    "    gp_features_test, gp_test_df = gp_transform(gp_est, X_test, classifier=False, sort_fit=sort_fit, num_models=num_models)\n",
    "\n",
    "    X_train_comb = np.concatenate([X_train_2, gp_features_train], axis=1)\n",
    "    X_test_comb = np.concatenate([X_test, gp_features_test], axis=1)\n",
    "\n",
    "\n",
    "    test_rmse_score, test_r2_score, test_pearson, test_pval = train_linear_model(seed, X_train_comb, X_test_comb, y_train_2, y_test)\n",
    "\n",
    "    return test_rmse_score, test_r2_score, test_pearson, test_pval, gp_test_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from scipy.stats import pearsonr\n",
    "epochs = 500\n",
    "batch_size = 80\n",
    "num_models = 5\n",
    "lr_0, disc_lr_0 = 1e-3, 0.8\n",
    "hidden_sizes = [1000, 500, 300, 100]\n",
    "eta, mu = 10.0, 1.0\n",
    "\n",
    "temp, sigma = 1.0, 1.0\n",
    "weight_prior = tfd.StudentT(df=2, loc=0, scale=sigma)\n",
    "num_feats = [20, 40, 60]\n",
    "num_ens = 10\n",
    "\n",
    "save_dir = f\"{data_dir}/exp_data_5/cancer/gdsc\"\n",
    "\n",
    "def cross_val_gp_train(seeds, X, y):\n",
    "   \n",
    "   bnn_rf_gp_dict = {\"seed\": [], \"model\": [], \"num_feats\": [], \"test_rmse_score\": [], \n",
    "                    \"test_r2_score\": [], \"test_pcc\": [], \"test_pcc_pval\": []}\n",
    "\n",
    "   bnn_bg_rf_dict = {\"seed\":[], \"model\": [], \"val_rmse\": [], \"test_rmse\": [], \"val_r2_score\": [], \"test_r2_score\": []}\n",
    "\n",
    "   for seed in tqdm(seeds):\n",
    "      rng_key = jax.random.PRNGKey(seed)\n",
    "      X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X, y, random_state=seed, shuffle=True, test_size=0.2)\n",
    "      X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X_train_df, y_train_df, shuffle=True, \n",
    "                                                   random_state=seed, test_size=0.2)\n",
    "      train_transformer = QuantileTransformer(random_state=seed, output_distribution=\"normal\").fit(X_train_df)\n",
    "      train_transformed = train_transformer.transform(X_train_df)\n",
    "      val_transformed = train_transformer.transform(X_val_df)\n",
    "      test_transformed = train_transformer.transform(X_test_df)\n",
    "\n",
    "      X_train_df = pd.DataFrame(train_transformed, columns=X_train_df.columns)\n",
    "      X_val_df = pd.DataFrame(val_transformed, columns=X_val_df.columns)\n",
    "      X_test_df = pd.DataFrame(test_transformed, columns=X_test_df.columns)\n",
    "      X_train, y_train = X_train_df.values, y_train_df.values\n",
    "      X_val, y_val = X_val_df.values, y_val_df.values\n",
    "      X_test, y_test = X_test_df.values, y_test_df.values\n",
    "\n",
    "      # Train BNN and RF models and select features\n",
    "      torch.manual_seed(seed)\n",
    "      data_loader = NumpyLoader(NumpyData(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "      val_data = NumpyData(X_val, y_val)\n",
    "\n",
    "      bg_bnn_models, bg_states, val_losses = train_bg_deep_ens_model(rng_key, data_loader, val_data, epochs, lr_0, disc_lr_0,\n",
    "                                                                     weight_prior, temp, hidden_sizes, eta, mu, J, num_ensembles=num_ens, act_fn=jax.nn.swish, patience=10)\n",
    "      \n",
    "      bnn_bg_rmse_val, bnn_bg_r2_val = eval_bg_deep_ens_model(rng_key, bg_bnn_models, X_val, y_val, bg_states)\n",
    "      bnn_bg_rmse_test, bnn_bg_r2_test = eval_bg_deep_ens_model(rng_key, bg_bnn_models, X_test, y_test, bg_states)\n",
    "\n",
    "      gammas = np.array([state.avg_disc_state for state in bg_states])\n",
    "      mean_gammas = np.mean(gammas, axis=0)\n",
    "      \n",
    "\n",
    "      rf_model = train_rf_model(seed, X_train, y_train)\n",
    "      rf_rmse_val, rf_r2_val = eval_rf_model(rf_model, X_val, y_val)\n",
    "      rf_rmse_test, rf_r2_test = eval_rf_model(rf_model, X_test, y_test)\n",
    "\n",
    "      X_train_2, y_train_2 = jax.random.choice(rng_key, X_train, shape=(X_val.shape[0],), replace=False), jax.random.choice(rng_key, y_train, shape=(X_val.shape[0],), replace=False)\n",
    "\n",
    "      for num_feat in num_feats:\n",
    "         bnn_feat_idx = np.argsort(mean_gammas)[::-1][:num_feat]\n",
    "\n",
    "         rf_feat_idx = np.argsort(rf_model.feature_importances_)[::-1][:num_feat]\n",
    "\n",
    "         # Create new validation for training Linear Regression models same size as outer validation\n",
    "         X_gp_train_rf, X_gp_train_2_rf, X_gp_val_rf, X_gp_test_rf = X_val[:,rf_feat_idx], X_train_2[:,rf_feat_idx], X_train[:,rf_feat_idx], X_test[:,rf_feat_idx] \n",
    "         X_gp_train_bnn, X_gp_train_2_bnn, X_gp_val_bnn, X_gp_test_bnn = X_val[:,bnn_feat_idx], X_train_2[:,bnn_feat_idx], X_train[:,bnn_feat_idx], X_test[:,bnn_feat_idx]\n",
    "         \n",
    "         y_train_gp, y_train_gp_2, y_val_gp, y_test_gp = y_val, y_train_2, y_train, y_test\n",
    "\n",
    "         ## Run GP\n",
    "\n",
    "   \n",
    "         bnn_test_rmse_score, bnn_test_r2_score, bnn_test_pcc, bnn_test_pval = train_linear_model(seed, X_gp_train_2_bnn, X_gp_test_bnn, y_train_gp_2, y_test_gp)\n",
    "         rf_test_rmse_score, rf_test_r2_score, rf_test_pcc, rf_test_pval = train_linear_model(seed, X_gp_train_2_rf, X_gp_test_rf, y_train_gp_2, y_test_gp)\n",
    "\n",
    "\n",
    "         bnn_gp_test_rmse_score, bnn_gp_test_r2_score, bnn_gp_test_pcc, bnn_gp_test_pval, _ = train_gp(seed, X_gp_train_bnn, X_gp_train_2_bnn, X_gp_test_bnn, \n",
    "                                                                                                               y_train_gp, y_train_gp_2, y_test_gp, num_models=num_models, verbose=0, p_cxvr=0.8, p_subt_mut=0.1, \n",
    "                                                                                                               p_hmut=0.05, p_pmut=0.05, complexity_coef=0.01)\n",
    "\n",
    "\n",
    "         rf_gp_test_rmse_score, rf_gp_test_r2_score, rf_gp_test_pcc, rf_gp_test_pval, _ = train_gp(seed, X_gp_train_rf, X_gp_train_2_rf, X_gp_test_rf, \n",
    "                                                                                                               y_train_gp, y_train_gp_2, y_test_gp, num_models=num_models, verbose=0, p_cxvr=0.8, p_subt_mut=0.1, \n",
    "                                                                                                               p_hmut=0.05, p_pmut=0.05, complexity_coef=0.01)\n",
    "\n",
    "\n",
    "         # ======================== Save scores for compressed models =========================\n",
    "\n",
    "         bnn_rf_gp_dict[\"seed\"].append(seed)\n",
    "         bnn_rf_gp_dict[\"model\"].append(\"RF + LR\")\n",
    "         bnn_rf_gp_dict[\"num_feats\"].append(num_feat)\n",
    "         bnn_rf_gp_dict[\"test_rmse_score\"].append(rf_test_rmse_score)\n",
    "         bnn_rf_gp_dict[\"test_r2_score\"].append(rf_test_r2_score)\n",
    "         bnn_rf_gp_dict[\"test_pcc\"].append(rf_test_pcc)\n",
    "         bnn_rf_gp_dict[\"test_pcc_pval\"].append(rf_test_pval)\n",
    "\n",
    "         bnn_rf_gp_dict[\"seed\"].append(seed)\n",
    "         bnn_rf_gp_dict[\"model\"].append(\"RF + LR + GP\")\n",
    "         bnn_rf_gp_dict[\"num_feats\"].append(num_feat)\n",
    "         bnn_rf_gp_dict[\"test_rmse_score\"].append(rf_gp_test_rmse_score)\n",
    "         bnn_rf_gp_dict[\"test_r2_score\"].append(rf_gp_test_r2_score)\n",
    "         bnn_rf_gp_dict[\"test_pcc\"].append(rf_gp_test_pcc)\n",
    "         bnn_rf_gp_dict[\"test_pcc_pval\"].append(rf_gp_test_pval)\n",
    "\n",
    "         bnn_rf_gp_dict[\"seed\"].append(seed)\n",
    "         bnn_rf_gp_dict[\"model\"].append(\"BNN + LR\")\n",
    "         bnn_rf_gp_dict[\"num_feats\"].append(num_feat)\n",
    "         bnn_rf_gp_dict[\"test_rmse_score\"].append(bnn_test_rmse_score)\n",
    "         bnn_rf_gp_dict[\"test_r2_score\"].append(bnn_test_r2_score)\n",
    "         bnn_rf_gp_dict[\"test_pcc\"].append(bnn_test_pcc)\n",
    "         bnn_rf_gp_dict[\"test_pcc_pval\"].append(bnn_test_pval)\n",
    "\n",
    "\n",
    "         bnn_rf_gp_dict[\"seed\"].append(seed)\n",
    "         bnn_rf_gp_dict[\"model\"].append(\"BNN + LR + GP\")\n",
    "         bnn_rf_gp_dict[\"num_feats\"].append(num_feat)\n",
    "         bnn_rf_gp_dict[\"test_rmse_score\"].append(bnn_gp_test_rmse_score)\n",
    "         bnn_rf_gp_dict[\"test_r2_score\"].append(bnn_gp_test_r2_score)\n",
    "         bnn_rf_gp_dict[\"test_pcc\"].append(bnn_gp_test_pcc)\n",
    "         bnn_rf_gp_dict[\"test_pcc_pval\"].append(bnn_gp_test_pval)\n",
    "\n",
    "\n",
    "         print(f\"RF scores - LR: {rf_test_r2_score} GP: {rf_gp_test_r2_score}\")\n",
    "         print(f\"BNN scores - LR: {bnn_test_r2_score} GP: {bnn_gp_test_r2_score}\")\n",
    "\n",
    "\n",
    "      # ======================== Save scores for non-compressed models =========================\n",
    "      bnn_rf_deep_ens_bg_dict[\"seed\"].append(seed)\n",
    "      bnn_rf_deep_ens_bg_dict[\"model\"].append(\"RF\")\n",
    "      bnn_rf_deep_ens_bg_dict[\"val_rmse\"].append(rf_rmse_val)\n",
    "      bnn_rf_deep_ens_bg_dict[\"val_r2_score\"].append(rf_r2_val)\n",
    "      bnn_rf_deep_ens_bg_dict[\"test_rmse\"].append(rf_rmse_test)\n",
    "      bnn_rf_deep_ens_bg_dict[\"test_r2_score\"].append(rf_r2_test)\n",
    "\n",
    "      bnn_rf_deep_ens_bg_dict[\"seed\"].append(seed)\n",
    "      bnn_rf_deep_ens_bg_dict[\"model\"].append(\"Deep Ens\")\n",
    "      bnn_rf_deep_ens_bg_dict[\"val_rmse\"].append(bnn_bg_rmse_val)\n",
    "      bnn_rf_deep_ens_bg_dict[\"val_r2_score\"].append(bnn_bg_r2_val)\n",
    "      bnn_rf_deep_ens_bg_dict[\"test_rmse\"].append(bnn_bg_rmse_test)\n",
    "      bnn_rf_deep_ens_bg_dict[\"test_r2_score\"].append(bnn_bg_r2_test)\n",
    "\n",
    "\n",
    "\n",
    "   return bnn_rf_deep_ens_bg_dict, bnn_rf_gp_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# seeds = [422,261,968,282,739,573,220,413,745,775,482,442,210,423,760,57,769,920,226,196]\n",
    "seeds = [422,261,968,282,739]\n",
    "bnn_rf_deep_ens_bg_dict, bnn_rf_gp_dict = cross_val_gp_train(seeds, X_selected, target)\n",
    "save_dir = f\"{data_dir}/exp_data_5/cancer/gdsc\"\n",
    "bnn_rf_gp_df = pd.DataFrame(bnn_rf_gp_dict)\n",
    "bnn_rf_gp_df.to_csv(f\"{save_dir}/bnn_rf_gp_df_avg_state_v2.csv\", index=False)\n",
    "bnn_rf_deep_ens_bg_df = pd.DataFrame(bnn_rf_deep_ens_bg_dict)\n",
    "bnn_rf_deep_ens_bg_df.to_csv(f\"{save_dir}/bnn_deep_ens_rf.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_rf_deep_ens_bg_df = pd.read_csv(f\"{save_dir}/bnn_deep_ens_rf.csv\")\n",
    "bnn_rf_deep_ens_bg_df.drop(index=[36, 37]).groupby(\"model\").mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.read_csv(f\"{save_dir}/bnn_deep_ens_rf.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_dir = f\"{data_dir}/exp_data_5/cancer/gdsc\"\n",
    "bnn_rf_gp_df = pd.read_csv(f\"{save_dir}/bnn_rf_gp_df_avg_state_v2.csv\")\n",
    "bnn_rf_gp_df.groupby([\"model\", \"num_feats\"]).mean().iloc[:,1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_rf_gp_df.groupby([\"model\"]).mean().iloc[:,1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_rf_deep_ens_bg_df.iloc[37]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnn_rf_gp_df_2 = pd.read_csv(f\"{save_dir}/bnn_rf_gp_df_avg_state.csv\")\n",
    "bnn_rf_gp_df_2.groupby(\"model\").mean().iloc[:,1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def input_mapping(x, B):\n",
    "  if B is None:\n",
    "    return x\n",
    "  else:\n",
    "    def transform_fn(x):\n",
    "      x_proj = (2.*jnp.pi*x) * B\n",
    "      return jnp.concatenate([jnp.sin(x_proj), jnp.cos(x_proj)], axis=-1)\n",
    "\n",
    "    x_fft = jax.vmap(transform_fn)(x)\n",
    "    return x_fft\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(seed)\n",
    "epochs = 1000\n",
    "num_cycles = 3\n",
    "batch_size = 80\n",
    "beta = 0.8\n",
    "lr_0 = 1e-3\n",
    "hidden_sizes = [1000, 500, 300, 100]\n",
    "temp, sigma = 1e-2, 1.0\n",
    "n, p = X_train.shape\n",
    "\n",
    "# scales = [1.0, 10., 100.0]\n",
    "scales = [15, 20, 50]\n",
    "\n",
    "for scale in scales:\n",
    "    B_gauss = scale*jax.random.normal(rng_key, (p,))\n",
    "    X_train_fft = input_mapping(X_train, B_gauss)\n",
    "    X_val_fft = input_mapping(X_val, B_gauss)\n",
    "    X_test_fft = input_mapping(X_test, B_gauss)\n",
    "    X_train_fft.shape\n",
    "    torch.manual_seed(seed)\n",
    "    data_loader = NumpyLoader(NumpyData(X_train_fft, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_data = NumpyData(X_val_fft, y_val)\n",
    "\n",
    "    bnn_model, state, val_losses = train_nn_model(rng_key, data_loader, val_data, epochs, num_cycles, beta, lr_0,\n",
    "                                        hidden_sizes, dropout_rate=0.0, act_fn=jax.nn.swish, patience=20)\n",
    "\n",
    "    rmse_val, r2_val = eval_bnn_model(rng_key, bnn_model, X_val_fft, y_val, state)\n",
    "    rmse_test, r2_test = eval_bnn_model(rng_key, bnn_model, X_test_fft, y_test, state)\n",
    "    print(f\"Scale: {scale}\")\n",
    "    print(f\"Val RMSE: {rmse_val}, r2_score: {r2_val}\")\n",
    "    print(f\"Test RMSE: {rmse_test}, r2_score: {r2_test}\")                                    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(seed)\n",
    "n, p = X_train.shape\n",
    "B_gauss = 10.*jax.random.normal(rng_key, (p, ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_fft = input_mapping(X_train, B_gauss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_fft.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}